---
title: "Health Outcomes & Storms"
output: html_document
editor_options:
  chunk_output_type: console
---

## Summary of the procedures used in TC modeling and merging of birth records (initially written by ChatGPT using my code below then edited by me)

We built a comprehensive spatial-temporal model to assess the effects of tropical cyclones (TCs) on birth outcomes, focusing on whether pregnancies in Florida were exposed to hurricanes during critical trimesters and preconception. Here's a summary of our key components and logic:

**Storm Data Preparation**

We download and import North Atlantic tropical cyclone track data from the NOAA IBTrACS dataset. The data includes storm track geometries and attributes like wind speed and pressure. The tracks are filtered to include only significant storms (e.g., tropical cyclones with wind speeds â‰¥55 knots) that occurred between 1981 and 2022. Missing values for the radius to maximum winds (RMW) are imputed in stages based on pedigree (storm ID) and minimum central pressure.

Each storm's track is buffered using the RMW to create polygonal wind swaths. These swaths are then spatially intersected with a boundary defining the state of Florida to identify storms affecting the region. Overlapping swaths from the same storm are dissolved to form one geometry per storm, and each is assigned a Saffir-Simpson category based on peak wind speed. Swath geometries are transformed into a geographic coordinate system for compatibility with later spatial operations.

**Temporal Expansion and Classification**

The modeled storm footprints are expanded into a time series that includes a "threat" day before landfall, the "impact" day, and a "cleanup" day after. These dates are used to classify exposure windows. The final product contains spatial polygons for each day a storm might have affected Florida, along with categorical storm effects and metadata like storm name and category.

**Birth Record Preparation**

Next we load individual-level birth data containing information on the birth date and geographic coordinates, along with detailed trimester start and end dates. This dataset is transformed into a spatial features object, allowing spatial joins with storm data. A cleaned and trimmed set of relevant variables (including health, demographic, and environmental exposure indicators) is also loaded for future merging.

**Exposure Assessment**

The code then iterates through chunks of the birth dataset to assess exposure during a given trimester (e.g., first trimester weeks 1-13). For each chunk, the code filters storms temporally to those falling within the individual's first trimester. It then uses a spatial join to determine whether the birth location falls within the storm's effect zone on those dates. Only those records with confirmed overlap are kept. This procedure is memory-intensive and optimized through chunking and garbage collection.

Finally, births exposed to storms are recombined and linked with the larger birth dataset for downstream statistical analysis or modeling of birth outcomes.

## Storm effects model

Get and import the IBTraCS storm data

https://www.ncei.noaa.gov/data/international-best-track-archive-for-climate-stewardship-ibtracs/v04r00/access/shapefile/
https://www.ncei.noaa.gov/data/international-best-track-archive-for-climate-stewardship-ibtracs/v04r00/doc/IBTrACS_v04_Technical_Details.pdf
https://www.ncei.noaa.gov/sites/default/files/2021-07/IBTrACS_v04_column_documentation.pdf

```{r}
L <- "https://www.ncei.noaa.gov/data/international-best-track-archive-for-climate-stewardship-ibtracs/v04r00/access/shapefile/IBTrACS.NA.list.v04r00.lines.zip"
  
if(!"IBTrACS.NA.list.v04r00.lines.zip" %in% list.files(here::here("data"))) {
download.file(url = L,
              destfile = here::here("data",
                                    "IBTrACS.NA.list.v04r00.lines.zip"))
unzip(here::here("data", "IBTrACS.NA.list.v04r00.lines.zip"),
      exdir = here::here("data"))
}

Tracks.sf <- sf::st_read(dsn = here::here("data"), 
                         layer = "IBTrACS.NA.list.v04r00.lines") |>
  sf::st_transform(crs = 32616)
```

Geometry type is LINESTRING. Wind speed is in units of knots (nautical mile per hour)

Keep only storms having USA_WIND >= 55 occurring between 1981 and 2022
```{r}
Tracks2.sf <- Tracks.sf |>
  dplyr::filter(year >= 1981 & year <= 2022) |>
  dplyr::filter(USA_WIND >= 64) |> # change to 34 for tropical storms and hurricanes
  dplyr::select(SID, SEASON, year, month, day, hour, min,
                NAME, SUBBASIN, ISO_TIME,
                USA_WIND, USA_PRES, USA_RMW, USA_EYE, USA_ROCI)
```

Average radius to maximum wind 1981-2022. Values of USA_RMW (radius to maximum winds) and USA_EYE (eye diameter) are in nautical miles. ROCI: Radius of outer closed isobar. To convert to kilometers multiply by 1.852
```{r}
Tracks2.sf |>
  sf::st_drop_geometry() |>
#  dplyr::group_by(USA_PRES) |>
  dplyr::summarize(avgRMW = mean(USA_RMW, na.rm = TRUE) * 1.852,
                   avgEYE = mean(USA_EYE, na.rm = TRUE) * 1.852,
                   avgROCI = mean(USA_ROCI, na.rm = TRUE) * 1.852)
```

Fill in missing RMW values. Start with pedigree, then use minimum pressure, and finish again with pedigree
```{r}
Tracks2.sf <- Tracks2.sf |>
  dplyr::group_by(SID) |>  # pedigree
  dplyr::mutate(USA_RMW = ifelse(is.na(USA_RMW), mean(USA_RMW, na.rm = TRUE), USA_RMW))

Tracks2.sf <- Tracks2.sf |>
  dplyr::group_by(USA_PRES) |> # minimum pressure
  dplyr::mutate(USA_RMW = ifelse(is.na(USA_RMW), mean(USA_RMW, na.rm = TRUE), USA_RMW))

Tracks2.sf <- Tracks2.sf |>
  dplyr::group_by(SID) |> # pedigree
  dplyr::mutate(USA_RMW = ifelse(is.na(USA_RMW), mean(USA_RMW, na.rm = TRUE), USA_RMW))
```

Add a buffer to the tracks to make segmented wind swaths
```{r}
Swaths.sf <- Tracks2.sf |>
  sf::st_buffer(dist = Tracks2.sf$USA_RMW * 1852) # 1852 converts to meters

#tmap::tmap_mode("view")
#tmap::tm_shape(Swaths.sf) +
#  tmap::tm_borders()
```

Wind swaths that cross Florida. `USAboundaries` package no longer maintained on CRAN
```{r}
#devtools::install_github("ropensci/USAboundariesData")
devtools::install_github("ropensci/USAboundaries", force = TRUE)
install.packages("USAboundariesData", repos = "https://ropensci.r-universe.dev", type = "source")


Boundaries.sf <- USAboundaries::us_states(resolution = "low", states = "FL") |> 
  sf::st_transform(crs = 32616)

X <- Swaths.sf |>
  sf::st_intersects(Boundaries.sf, sparse = FALSE) #Does the swath intersect the state border?
Swaths.sf <- Swaths.sf[X, ]
Swaths.sf <- Swaths.sf |>
  dplyr::mutate(Date = lubridate::as_date(ISO_TIME)) #Add a date column

#tmap::tm_shape(Swaths.sf) +
#  tmap::tm_borders()
```

Extract the boundaries of storm impacts by storm ID. Dissolve the overlap borders of the individual swaths by storm to create a single storm swath. Add the storm category (Saffir-Simpson) based on wind speed
```{r}
Swaths2.sf <- Swaths.sf |>
  dplyr::group_by(SID) |>
  dplyr::summarize(Date0 = dplyr::first(Date),
                   NAME = dplyr::first(NAME),
                   Wind = max(USA_WIND),
                   geometry = sf::st_union(geometry)) |>
  dplyr::mutate(Storm_Category = dplyr::case_when(
    Wind >= 55 & Wind <= 63 ~ 0,
    Wind >= 64 & Wind <= 82 ~ 1,
    Wind >= 83 & Wind <= 95 ~ 2,
    Wind >= 96 & Wind <= 112 ~ 3,
    Wind >= 113 & Wind <= 136 ~ 4,
    Wind >= 137 ~ 5
  ))

tmap::tm_shape(Swaths2.sf) +
  tm_borders("Storm_Category") +
  tm_shape(Boundaries.sf) +
  tmap::tm_borders()
```

Each storm in `Swaths2.sf` is represented by a unique polygon (or multipolygon) with an impact date

What to do with storms having multiple landfalls (e.g., Peninsula then Panhandle like Hurricane Erin in 1995)? In those cases the geometry will be MULTIPOLYGON. The problem is that the date of second landfall maybe different from the first (e.g., a day or two later)

Check to see which storms these are
```{r}
#x <- sf::st_geometry_type(Swaths2.sf$geometry) == "MULTIPOLYGON"
#Swaths2.sf$NAME[x]
```

The situations are different for Erin, Fay, and Irma so it is not clear what to do. One approach is to simply remove these storms from further consideration. Here we keep them essentially ignoring the second landfall
```{r}
#Swaths2.sf <- Swaths2.sf[!x,]
```

Transform the geometry to a geographic CRS (4326) and unionize the swaths
```{r}
Swaths2.sf <- Swaths2.sf |>
    sf::st_transform(crs = 4326)
union_Swaths2.sfg <- Swaths2.sf |>
  sf::st_union()
```

Expand `Swaths2.sf` by adding rows based on the increment value of the attribute `Date0`. Dates prior to the impact date are threat dates and those after the impact date are cleanup dates. Here we used one day prior and one day after impact
```{r}
deltaT <- 1   # One day increment
n_new <- 1    # Number of new rows to create for each feature (row) times 2

TIC.sf <- Swaths2.sf |>
    dplyr::rowwise() |>
    dplyr::mutate(new_features = list(tidyr::tibble(
                  Date = Date0 + (-n_new:n_new) * deltaT))) |>
    tidyr::unnest(new_features) |>
    dplyr::ungroup() |>
    dplyr::select(Date, Storm_Name = NAME, Storm_Category)

TIC <- rep(rep(c("Threat", "Impact", "Cleanup"), times = c(n_new, 1, n_new)), 
           times = nrow(Swaths2.sf))
TIC.sf$Storm_Effect <- TIC
```

Add birth month and year indicators
```{r}
TIC.sf <- TIC.sf |>
  dplyr::mutate(Month = lubridate::month(Date),
                Year = lubridate::year(Date))
storm_months <- unique(TIC.sf$Month)
storm_year_range <- range(TIC.sf$Year)
```

```{r}
sf::st_geometry(TIC.sf) <- "geom" #Can't have matching column names when merging with birth/death records
```

## Merge with birth records

Email from Jihoon on December 11, 2024
1. ID 2. DATE_OF_BIRTH 3. final_lat  4. final_lon 5. trimester1_s_date: The first Trimester start date
6. trimester1_e_date : The first Trimester end date 7. trimester2_s_date : The second Trimester start date
8. trimester2_e_date : The second Trimester end date 9. trimester3_s_date: The third Trimester start date
The first Trimester: the weeks 1-13, The second Trimester: the weeks 14-28, The third Trimester: the weeks 29 ~
```{r}
library(data.table)
Trimesters.df <- data.table::fread(file = "data/all_births_hurricane.csv")
```

Add a "IDate" column that is 3 months prior to the pregnancy
```{r}
library(lubridate)
Trimesters.df[, three_mo_before_preg := as.IDate(as.Date(trimester1_s_date) %m-% months(3))]
```

Get corresponding birth and heat data by concatenating the `.Rdata` files in the folder `21_definition_final_re`. This takes 7.14 minutes
```{r}
start_time <- Sys.time()
files <- list.files(path = "data/21_definition_final_re", 
                    pattern = "\\.Rdata$", full.names = TRUE)

All.dt <- rbindlist(lapply(files, function(file) {
  df <- get(load(file))
  as.data.table(df)
}), use.names = TRUE, fill = TRUE)
Sys.time() - start_time
```

Select only the columns in the following list sent by Jessica Broach via email on April 14, 2025
```{r}
col_list <- c("ID", "final_lat", "final_lon", "ALCOHOL_DRINKS", "ALCOHOL_USE", "BIRTH_WEIGHT_GRAMS", 
"CalculatedGestationalAge", "DATE_OF_BIRTH", "GESTATION_WEEKS", "FATHER_AGE", "FATHER_EDCODE",
"Father_CalculatedHisp", "Father_CalculatedRace", "MENSES_DATE", "MOTHER_AGE", "Mother_CalculatedHisp",
"Mother_CalculatedRace", "MOTHER_EDCODE", "MOTHER_MARRIED", "MOTHER_RES_COUNTY", "MOTHER_WIC_YESNO", "MR_DIAB_GEST", "MR_HYPERT_ECLAMPSIA", "MR_HYPERT_PREG", "MR_PREV_PRETERM", "PLURALITY_CODE", 
"PrePregnancy_BMI", "PRINCIPAL_SOURCE_PAY", "PRINCIPAL_SRCPAY_CODE", "SEX", "m28_temperature",
"lat_pm25", "lon_pm25", "date_pm25", "loc_data_pm25", "m28_pm25", "m21_pm25", "m14_pm25", "m7_pm25",
"dday_pm25", "p7_pm25", "p14_pm25", "p21_pm25", "p28_pm25", "m20_pm25", "m19_pm25", "m18_pm25",
"m17_pm25", "m16_pm25", "m15_pm25", "m14_pm25.1", "m13_pm25", "m12_pm25", "m11_pm25", "m10_pm25",
"m9_pm25", "m8_pm25", "m7_pm25.1", "m6_pm25", "m5_pm25", "m4_pm25", "m3_pm25", "m2_pm25", "m1_pm25",
"m0_pm25", "year_pm25r", "d_1_13_wk_90_1", "d_14_28_wk_90_1", "d_from29_wk_90_1", "d_1_13_wk_95_1",
"d_14_28_wk_95_1", "d_from29_wk_95_1", "d_1_13_wk_98_1", "d_14_28_wk_98_1", "d_from29_wk_98_1",
"d_1_13_wk_90_2", "d_14_28_wk_90_2", "d_from29_wk_90_2", "d_1_13_wk_95_2", "d_14_28_wk_95_2",
"d_from29_wk_95_2", "d_1_13_wk_98_2", "d_14_28_wk_98_2", "d_from29_wk_98_2", "d_1_13_wk_90_3",
"d_14_28_wk_90_3", "d_from29_wk_90_3", "d_1_13_wk_95_3", "d_14_28_wk_95_3", "d_from29_wk_95_3",
"d_1_13_wk_98_3", "d_14_28_wk_98_3", "d_from29_wk_98_3", "trimester1_length", "trimester2_length",
"trimester3_length")

All.dt <- All.dt |>
  dplyr::select(dplyr::all_of(col_list))
```

Make the `Trimester.df` data frame a simple feature data frame with POINT geometry
```{r}
Trimesters.sf <- Trimesters.df |>
  dplyr::mutate(Date = lubridate::as_date(DATE_OF_BIRTH),
                DateT0s = three_mo_before_preg,
                DateT0e = lubridate::as_date(trimester1_s_date),
                DateT1s = lubridate::as_date(trimester1_s_date),
                DateT1e = lubridate::as_date(trimester1_e_date),
                DateT2s = lubridate::as_date(trimester2_s_date),
                DateT2e = lubridate::as_date(trimester2_e_date),
                DateT3s = lubridate::as_date(trimester3_s_date),
                DateT3e = Date,
                Year_of_Birth = lubridate::year(Date),
                Month_of_Birth = lubridate::month(Date),
                Month_of_T0s = lubridate::month(DateT1s),
                Month_of_T0e = lubridate::month(DateT1e),
                Month_of_T1s = lubridate::month(DateT1s),
                Month_of_T1e = lubridate::month(DateT1e),
                Month_of_T2s = lubridate::month(DateT2s),
                Month_of_T2e = lubridate::month(DateT2e),
                Month_of_T3s = lubridate::month(DateT3s),
                Month_of_T3e = lubridate::month(DateT3e)
                ) |>
  sf::st_as_sf(coords = c("final_lon", "final_lat"),
               crs = 4326) |>
  dplyr::select(ID = ID, Birth_Date = Date, 
                DateT0s, DateT0e, DateT1s, DateT1e,
                DateT2s, DateT2e, DateT3s, DateT3e,
                Month_of_Birth, Year_of_Birth, 
                Month_of_T0s, Month_of_T0e,
                Month_of_T1s, Month_of_T1e, 
                Month_of_T2s, Month_of_T2e, Month_of_T3s)
```

# Start of code that parses by trimester

Filter births temporally by trimester start and end dates and spatially by storm effect zones. It took 5.2 hours on my Apple M4 (1st trimester)
```{r}
start_time <- Sys.time()
#keep_rows <- sample.int(nrow(Trimesters.sf), size = 5000)

num_obs <- nrow(Trimesters.sf)
chunk_size <- 50000
num_chunks <- ceiling(num_obs / chunk_size)
print(paste("Number of chunks to process", num_chunks))

# Pre-allocate an empty list to store chunks
results_list <- vector("list", num_chunks)

for (i in seq(1, num_obs, by = chunk_size)) {
  chunk_index <- (i - 1) / chunk_size + 1
  chunk_indices <- i:min(i + chunk_size - 1, num_obs)
  
  print(paste("Processing chunk", chunk_index))

  chunk.sf <- Trimesters.sf[chunk_indices, ]

# 1 Filter births by trimester date range, change DateTXs & DateTXe for different trimesters
births_storm_filtered <- chunk.sf |>
  dplyr::rowwise() |>
  dplyr::mutate(
    matched_storms = list(
      TIC.sf |>
        dplyr::filter(Date >= DateT0s & Date <= DateT0e))) |>
  tidyr::unnest(matched_storms, keep_empty = TRUE)

# 2 Perform spatial join to check if birth locations fall within the storm effect zones
births_with_impacts <- sf::st_join(births_storm_filtered, TIC.sf, join = sf::st_within)

# 3 Keep only records where a birth was inside a storm effect zone during the specified trimester
births_with_impacts <- births_with_impacts |>
  dplyr::filter(!is.na(Storm_Name.x)) |>
  dplyr::select(ID, Birth_Date, DateT0s, DateT0e,  # change DateTXs, DateTXe
                Storm_Name = Storm_Name.x, 
                Storm_Category = Storm_Category.x,
                Storm_Effect = Storm_Effect.x, geometry) |>
  dplyr::distinct()

results_list[[chunk_index]] <- births_with_impacts

gc() #ensures memory from previous iterations is released
}

births_with_impacts <- do.call(rbind, results_list) #row bind the chunks
Sys.time() - start_time
```

Combine with all births and remove the geometry column. 15 sec
```{r}
start_time <- Sys.time()
# which birth IDs are unique
unique_Birth_IDs <- unique(births_with_impacts$ID)

# is the birth ID not in the births_with_impacts data
X <- !Trimesters.sf$ID %in% unique_Birth_IDs
births_without_impacts <- Trimesters.sf[X, ]

# drop geometry and add dummy columns to the births_without_impacts
births_without_impacts <- births_without_impacts |>
  sf::st_drop_geometry() |>
  dplyr::mutate(Storm_Name = NA, Storm_Category = NA, Storm_Effect = "None") |>
  dplyr::select(ID, Birth_Date, DateT0s, DateT0e, Storm_Name, Storm_Category, Storm_Effect) # change DateTXs & DateTXs

# combine
combined_data <- births_with_impacts |>
  sf::st_drop_geometry() |>
  rbind(births_without_impacts)
Sys.time() - start_time
```

Cast to a wider format. 24 sec
```{r}
start_time <- Sys.time()
# make the data frame a data table
combined_data.dt <- combined_data |>
  as.data.table()

# Create unique index per birth in the data table
combined_data.dt[, row_id := sequence(.N), by = ID]

# fast long to wide for data tables
combined_data_wide.dt <- dcast(
  data = combined_data.dt, 
  formula = ID + Birth_Date + DateT0s + DateT0e ~ row_id, 
  value.var = c("Storm_Name", "Storm_Category", "Storm_Effect") #change DateTXs and DateTXe
)
Sys.time() - start_time
```

Combine with all data. Start by ordering births according to the ID in `All.dt` then column bind to `All.dt`. 9 sec
```{r}
common_cols <- intersect(names(combined_data_wide.dt), names(All.dt))

start_time <- Sys.time()

if(length(common_cols) > 0){
  setnames(combined_data_wide.dt, old = common_cols, new = paste0(common_cols, "_new"))
}

combined_data_wide_ordered.dt <- combined_data_wide.dt[order(match(combined_data_wide.dt$ID_new, All.dt$ID))]

All_Pre_Conception_Storm_Effects.dt <- cbind(All.dt, combined_data_wide_ordered.dt)

Sys.time() - start_time
```

Write out the data table to a csv file. 23 sec
```{r}
data.table::fwrite(All_Pre_Conception_Storm_Effects.dt, 
                   file = "data/outputs/All_Pre_Conception_Storm_Effects.csv")

# read it back
start_time <- Sys.time()
All_2nd_Trimester_Storm_Effects.dt <- data.table::fread(file = "data/outputs/All_2nd_Trimester_Storm_Effects.csv")
Sys.time() - start_time
```

Filter for births during 1986. NOT RUN
```{r}
#All_1st_Trimester_Storm_Effects.dt$Birth_Year <- lubridate::year(All_1st_Trimester_Storm_Effects.dt$DATE_OF_BIRTH)

#All_1st_Tri_SE_1986.dt <- All_1st_Trimester_Storm_Effects.dt[All_1st_Trimester_Storm_Effects.dt$Birth_Year == 1986, ]
#data.table::fwrite(All_1st_Tri_SE_1986.dt, 
#                   file = "data/outputs/All_1st_Tri_SE_1986.csv")
```

## Merge with death records

File received from Jihoon on December 7, 2024 via email. From email:
1) Whether impacted or not when a hurricane passes on the date of death? What was impacted? 2) Whether impacted or not during a warning on the date of death
3) Whether impacted or not during a watch on the date of death. 4) Whether impacted or not during an advisory on the date of death
5) (If available) Whether impacted or not during an evacuation order on the date of death

```{r}
Deaths.df <- read.csv(file = "data/all_deaths_hurricane.csv")
```

6.2 million deaths. Time period is 1981-1-1 until 2022-12-31. Only dates are given not times. Note: Watch/warning data are only available starting in 2008. lat/lon ranges appear to include Texas through Florida

Create a simple feature data frame with point geometry from the original data frame
```{r}
Deaths.sf <- Deaths.df |>
  dplyr::mutate(Date = lubridate::as_date(DATE_OF_DEATH)) |>
  sf::st_as_sf(coords = c("final_lon", "final_lat"),
               crs = 4326) |>
  dplyr::select(Death_ID = ID, Death_Date = Date)
```

Keep only deaths occurring inside the union of all threat/impact/cleanup polygons. Note: when using tropical cyclones and Florida this will be almost all deaths
```{r}
withIn <- Deaths.sf |>
  sf::st_within(union_Swaths2.sfg, sparse = FALSE)
DeathsWithin.sf <- Deaths.sf[withIn, ]
```

Check with a map by plotting locations from a sample of 10,000 death records. Note: this is only useful for hurricanes
```{r}
nrows <- nrow(DeathsWithin.sf)
keep_rows <- sample.int(nrows, size = 10000)

tmap::tm_shape(Swaths2.sf) +
  tmap::tm_borders() +
tmap::tm_shape(DeathsWithin.sf$geometry[keep_rows]) +
  tmap::tm_dots()
```

Filter storms by death date. Test on a sample of death records
```{r}
keep_rows <- sample.int(nrow(DeathsWithin.sf), size = 5000)
merge_Test.sf <- DeathsWithin.sf[keep_rows, ]
#merge_Test.sf <- DeathsWithin.sf

sf::st_geometry(TIC.sf) <- "geom" # rename geometry column. Column names from the two sf data frames must differ

# 1. Filter storms by death date
start_time <- Sys.time()
deaths_storm_filtered <- merge_Test.sf |>
  dplyr::rowwise() |>
  dplyr::mutate(
    matched_storms = list(
      TIC.sf |>
        dplyr::filter(Date == Death_Date))) |>
  tidyr::unnest(matched_storms, keep_empty = TRUE)
Sys.time() - start_time
# ~ 2.07 minutes for processing 100,000 death records under HURRICANE swaths using a M1 Pro 32 GB laptop
# ~ 5.25 hours for processing all death records under TC swaths

# 2. Perform spatial join to check if birth locations fall inside the storm polygons
deaths_with_impacts <- sf::st_join(deaths_storm_filtered, TIC.sf, join = sf::st_within)
# Error: vector memory limit of 32.0 Gb reached, see mem.maxVSize()

# 3. Keep only records where a death was inside the correct storm impact area
deaths_with_impacts <- deaths_with_impacts |>
  dplyr::filter(!is.na(NAME.x)) |>
  dplyr::select(Death_ID, Death_Date,
                NAME = NAME.x, TIC = TIC.x, geometry) |>
  dplyr::distinct()

# 4. Deaths outside correct impact zones but within the union of all impact zones

X <- !merge_Test.sf$Death_ID %in% deaths_with_impacts$Death_ID
deaths_without_impacts <- merge_Test.sf[X, ]
```

Check on a map
```{r}
tmap::tm_shape(Swaths2.sf) +
  tmap::tm_borders() +
tmap::tm_shape(deaths_with_impacts$geometry) +
  tmap::tm_dots()
```

Death rate per day with and without impacts
```{r}
length(unique(deaths_with_impacts$Death_ID)) / length(unique(deaths_with_impacts$Death_Date))
length(unique(deaths_without_impacts$Death_ID)) / length(unique(deaths_without_impacts$Death_Date))

deaths_with_impacts |>
  dplyr::group_by(TIC) |>
  dplyr::summarise(length(unique(Death_ID)) / length(unique(Death_Date)))
```

Convert to regular data frame with X (Longitude) and Y (Latitude)
```{r}
deaths_with_impacts.df <- deaths_with_impacts |>
  dplyr::mutate(
    final_lat = sf::st_coordinates(deaths_with_impacts)[,2],  # Extract latitude
    final_lon = sf::st_coordinates(deaths_with_impacts)[,1]  # Extract longitude
  ) |>
  sf::st_drop_geometry() 
```

## Toy example of a space-time merge using simple feature data frames

```{r}
library(sf)
library(dplyr)

# Example: Storm data (impact polygons and attributes)
storms <- st_sf(
  storm_name = c("Storm A", "Storm B", "Storm C"),
  impact_level = c(1, 2, 3),
  storm_date = as.Date(c("2023-01-15", "2023-02-20", "2023-03-10")),
  geometry = st_sfc(
    st_polygon(list(rbind(c(0, 0), c(0, 5), c(5, 5), c(5, 0), c(0, 0)))),
    st_polygon(list(rbind(c(3, 3), c(3, 8), c(8, 8), c(8, 3), c(3, 3)))),
    st_polygon(list(rbind(c(6, 6), c(6, 9), c(9, 9), c(9, 6), c(6, 6))))
  ),
  crs = 4326
)

# Example: Birth records (point locations and third trimester dates)
births <- st_sf(
  birth_date = as.Date(c("2023-05-01", "2023-03-01", "2023-01-20", "2023-09-01")),
  trimester_start = as.Date(c("2023-02-01", "2022-12-01", "2022-10-01", "2022-12-01")),
  trimester_end = as.Date(c("2023-04-30", "2023-02-28", "2023-01-19", "2023-08-30")),
  geometry = st_sfc(
    st_point(c(2, 2)),
    st_point(c(4, 4)),
    st_point(c(7, 7)),
    st_point(c(1, 7))
  ),
  crs = 4326
)

st_geometry(storms) <- "geom"

# 1. Filter storms by third trimester date range
births_storms_filtered <- births %>%
  dplyr::rowwise() %>%
  dplyr::mutate(
    matched_storms = list(
      storms %>%
        dplyr::filter(storm_date >= trimester_start & storm_date <= trimester_end)
    )
  ) %>%
  tidyr::unnest(matched_storms, keep_empty = TRUE)

# 2. Perform spatial join to check if birth locations fall within the storm polygons
births_with_impacts <- sf::st_join(births_storms_filtered, storms, join = st_within)

# 3. Keep only records where a birth was inside a storm impact area
births_with_impacts <- births_with_impacts %>%
  dplyr::filter(!is.na(storm_name.y)) %>%
  dplyr::select(birth_date, trimester_start, trimester_end, 
                storm_name = storm_name.y, impact_level = impact_level.y, geometry) |>
  dplyr::distinct()

# Display the final dataset
print(births_with_impacts)
```

# Watch-warning data from IA State

Data source: https://mesonet.agron.iastate.edu/request/gis/watchwarn.phtml

```{r}
storms.sf <- sf::st_read(dsn = here::here("data", "wwa_198601010000_202502080000"), 
                         layer = "wwa_198601010000_202502080000")

phenom <- c("HU", "HI", "SS", "TI", "TR")
storms.sf <- storms.sf |>
  dplyr::filter(PHENOM %in% phenom) 
```

SIGNIFICANCE = 
    "W": "Warning",
    "A": "Watch",
    "S": "Statement"
    
PHENOMENA = 
    "HI": "Inland Hurricane",
    "HU": "Hurricane",
    "SS": "Storm Surge",
    "TI": "Inland Tropical Storm",
    "TR": "Tropical Storm"

Parse the character strings to date/time objects
```{r}
storms2.sf <- storms.sf |>
  dplyr::mutate(ISSUED = lubridate::ymd_hm(ISSUED),
                EXPIRED = lubridate::ymd_hm(EXPIRED),
                INIT_ISS = lubridate::ymd_hm(INIT_ISS),
                INIT_EXP = lubridate::ymd_hm(INIT_EXP),
                Duration = EXPIRED - INIT_ISS, 
                Year = lubridate::year(ISSUED),
                DateStart = as.Date(INIT_ISS),
                DateEnd = as.Date(EXPIRED)
                ) |>
  dplyr::select(Year, DateStart, DateEnd, Duration, WFO, PHENOM, SIG, STATUS, NWS_UGC, AREA_KM2, geometry)
units(storms2.sf$Duration) <- "hours"

X.sf <- storms2.sf |>
  dplyr::filter(PHENOM == "HU" & SIG == "W")
```

Statistics
```{r}
storms2.sf |>
  sf::st_drop_geometry() |>
  dplyr::filter(PHENOM == "HU" & SIG == "W") |>
  dplyr::group_by(Year) |>
  dplyr::summarise(Number = dplyr::n(),
                   AvgDuration = mean(Duration),
                   AvgArea = mean(AREA_KM2))
```

No change in the average duration of the warning or the average area warned over the years


Code below is adapted from David Hsu

Here we read in the csv file containing hurricane watch and advisories for Florida from Iowa State University database
```{r}
warning_advisories <- readr::read_csv(here::here("data", "wwa_198601010000_202501010000.csv"))
```

How many unique names in the column labeled "phenomena"
```{r}
unique(warning_advisories$phenomena)
```

Filter the data to only include hurricanes
```{r}
phenom <- c("HU", "HI", "SS", "TI", "TR")
storms <- warning_advisories |>
  dplyr::filter(phenomena %in% phenom) 
```

Florida counties and zone shapefiles
from: https://www.weather.gov/gis/Counties
from: https://www.weather.gov/gis/publiczones
```{r}
counties.sf <- sf::st_read(dsn = here::here("data", "c_18mr25"), 
                           layer = "c_18mr25") |>
  dplyr::filter(STATE == "FL")

zones.sf <- sf::st_read(dsn = here::here("data", "z_18mr25"),
                        layer = "z_18mr25") |>
  dplyr::filter(STATE == "FL")  
```

Note that the `storms` dataset and the `zones.sf` dataset have different nomenclature for the zone names, where the `storms` dataset uses the "ugc" column and the `zones.sf` dataset uses the "STATE_ZONE" column. Also, the zone names in the `storms` dataset includes the letter "z" in the designation so we need to create a new column in the dataset that removes the "z" from the ugc column.
```{r}
storms <- storms |>
  dplyr::mutate(ugc = stringr::str_replace(ugc, "FLZ", "FL"))
```

Make sure the data sets match
```{r}
unique(storms$ugc)

unique(zones.sf$STATE_ZONE)
```

Join `storms` data with `zones.sf` shapefile
```{r}
storm_zones <- zones.sf |>
  dplyr::left_join(storms, by = c("STATE_ZONE" = "ugc"))

head(storm_zones)
```

Create dates from character strings
```{r}
storm_zones <- storm_zones |>
  dplyr::mutate(utc_issue = as.Date(utc_issue),
                utc_expire = as.Date(utc_expire),
                year = lubridate::year(utc_issue)) |>
  dplyr::arrange(utc_issue)
```

Time series graph of the number of issuances
```{r}
library(ggplot2)

storm_zones |>
  sf::st_drop_geometry() |>
  dplyr::group_by(phenomena, year) |>
  dplyr::summarise(Number = dplyr::n()) |>
  print(n = 100)
```

