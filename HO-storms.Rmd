---
title: "Health Outcomes & Storms"
output: html_document
editor_options: 
  chunk_output_type: console
---

## Storm threat and impact model

Get and import the IBTraCS storm data

https://www.ncei.noaa.gov/data/international-best-track-archive-for-climate-stewardship-ibtracs/v04r00/access/shapefile/
https://www.ncei.noaa.gov/data/international-best-track-archive-for-climate-stewardship-ibtracs/v04r00/doc/IBTrACS_v04_Technical_Details.pdf
https://www.ncei.noaa.gov/sites/default/files/2021-07/IBTrACS_v04_column_documentation.pdf

```{r}
L <- "https://www.ncei.noaa.gov/data/international-best-track-archive-for-climate-stewardship-ibtracs/v04r00/access/shapefile/IBTrACS.NA.list.v04r00.lines.zip"
  
if(!"IBTrACS.NA.list.v04r00.lines.zip" %in% list.files(here::here("data"))) {
download.file(url = L,
              destfile = here::here("data",
                                    "IBTrACS.NA.list.v04r00.lines.zip"))
unzip(here::here("data", "IBTrACS.NA.list.v04r00.lines.zip"),
      exdir = here::here("data"))
}

Tracks.sf <- sf::st_read(dsn = here::here("data"), 
                         layer = "IBTrACS.NA.list.v04r00.lines") |>
  sf::st_transform(crs = 32616)
```

Geometry type is LINESTRING. Wind speed is in units of knots (nautical mile per hour)

Keep only storms having USA_WIND >= 64 occurring between 1981 and 2022
```{r}
Tracks2.sf <- Tracks.sf |>
  dplyr::filter(year >= 1981 & year <= 2022) |>
  dplyr::filter(USA_WIND >= 55) |> # change to 34 for tropical storms and hurricanes
  dplyr::select(SID, SEASON, year, month, day, hour, min,
                NAME, SUBBASIN, ISO_TIME,
                USA_WIND, USA_PRES, USA_RMW, USA_EYE, USA_ROCI)
```

Average radius to maximum wind 1981-2022. Values of USA_RMW (radius to maximum winds) and USA_EYE (eye diameter) are in nautical miles. ROCI: Radius of outer closed isobar. To convert to kilometers multiply by 1.852
```{r}
Tracks2.sf |>
  sf::st_drop_geometry() |>
#  dplyr::group_by(USA_PRES) |>
  dplyr::summarize(avgRMW = mean(USA_RMW, na.rm = TRUE) * 1.852,
                   avgEYE = mean(USA_EYE, na.rm = TRUE) * 1.852,
                   avgROCI = mean(USA_ROCI, na.rm = TRUE) * 1.852)
```

Fill in missing RMW values. Start with pedigree, then use minimum pressure, and finish again with pedigree
```{r}
Tracks2.sf <- Tracks2.sf |>
  dplyr::group_by(SID) |>  # pedigree
  dplyr::mutate(USA_RMW = ifelse(is.na(USA_RMW), mean(USA_RMW, na.rm = TRUE), USA_RMW))

Tracks2.sf <- Tracks2.sf |>
  dplyr::group_by(USA_PRES) |> # minimum pressure
  dplyr::mutate(USA_RMW = ifelse(is.na(USA_RMW), mean(USA_RMW, na.rm = TRUE), USA_RMW))

Tracks2.sf <- Tracks2.sf |>
  dplyr::group_by(SID) |> # pedigree
  dplyr::mutate(USA_RMW = ifelse(is.na(USA_RMW), mean(USA_RMW, na.rm = TRUE), USA_RMW))
```

Add a buffer to the tracks to make segmented wind swaths
```{r}
Swaths.sf <- Tracks2.sf |>
  sf::st_buffer(dist = Tracks2.sf$USA_RMW * 1852) # 1852 converts to meters

#tmap::tmap_mode("view")
#tmap::tm_shape(Swaths.sf) +
#  tmap::tm_borders()
```

Wind swaths that cross Florida. `USAboundaries` package no longer maintained on CRAN
```{r}
devtools::install_github("ropensci/USAboundariesData")
devtools::install_github("ropensci/USAboundaries")

Boundaries.sf <- USAboundaries::us_states(resolution = "low", states = "FL") |> 
  sf::st_transform(crs = 32616)

X <- Swaths.sf |>
  sf::st_intersects(Boundaries.sf, sparse = FALSE)
Swaths.sf <- Swaths.sf[X, ]
Swaths.sf <- Swaths.sf |>
  dplyr::mutate(Date = lubridate::as_date(ISO_TIME)) # add a m-y-d column

tmap::tm_shape(Swaths.sf) +
  tmap::tm_borders()
```

Extract the boundaries of storm impacts by storm ID. Dissolve the overlap borders of the individual swaths by storm to create a single storm swath
```{r}
Swaths2.sf <- Swaths.sf |>
  dplyr::group_by(SID) |>
  dplyr::summarize(Date0 = dplyr::first(Date),
                   NAME = dplyr::first(NAME),
                   Max_Intensity_mps = round(max(USA_WIND) * .5144, digits = 0),
                   geometry = sf::st_union(geometry))

tmap::tm_shape(Swaths2.sf) +
  tmap::tm_borders()
```

Each storm in `Swaths2.sf` is represented by a unique polygon (or multipolygon) with an impact date

What to do with storms having multiple landfalls (e.g., Peninsula then Panhandle like Hurricane Erin in 1995)? In those cases the geometry will be MULTIPOLYGON.

Check to see which storms these are
```{r}
x <- sf::st_geometry_type(Swaths2.sf$geometry) == "MULTIPOLYGON"
Swaths2.sf$NAME[x]
```

The situations are different for Erin, Fay, and Irma so it is not clear what to do. One way is simple to remove these storms from further consideration
```{r}
Swaths2.sf <- Swaths2.sf[!x,]
```

Transform the geometry to a geographic CRS (4326) and unionize the swaths
```{r}
Swaths2.sf <- Swaths2.sf |>
    sf::st_transform(crs = 4326)
union_Swaths2.sfg <- Swaths2.sf |>
  sf::st_union()
```

Expand `Swaths2.sf` by adding rows based on the increment value of the attribute `Date0`. Dates prior to the impact date are threat dates and those after the impact date are cleanup dates. Here we used one day prior and one day after impact
```{r}
deltaT <- 1   # One day increment
n_new <- 1    # Number of new rows to create for each feature (row) times 2

TIC.sf <- Swaths2.sf |>
    dplyr::rowwise() |>
    dplyr::mutate(new_features = list(tidyr::tibble(
                  Date = Date0 + (-n_new:n_new) * deltaT))) |>
    tidyr::unnest(new_features) |>
    dplyr::ungroup() |>
    dplyr::select(Date, NAME, Max_Intensity_mps)

TIC <- rep(rep(c("Threat", "Impact", "Cleanup"), times = c(n_new, 1, n_new)), 
           times = nrow(Swaths2.sf))
TIC.sf$TIC <- TIC
```

Add birth month and year indicators
```{r}
TIC.sf <- TIC.sf |>
  dplyr::mutate(Month = lubridate::month(Date),
                Year = lubridate::year(Date))
storm_months <- unique(TIC.sf$Month)
storm_year_range <- range(TIC.sf$Year)
```

```{r}
sf::st_geometry(TIC.sf) <- "geom" # can't have matching column names
```

## Merge with birth records

Email from Jihoon on December 11, 2024
1. ID 2. DATE_OF_BIRTH 3. final_lat  4. final_lon 5. trimester1_s_date: The first Trimester start date
6. trimester1_e_date : The first Trimester end date 7. trimester2_s_date : The second Trimester start date
8. trimester2_e_date : The second Trimester end date 9. trimester3_s_date: The third Trimester start date
The first Trimester: the weeks 1-13, The second Trimester: the weeks 14-28, The third Trimester: the weeks 29 ~
```{r}
Births.df <- read.csv(file = "data/all_births_hurricane.csv")
```

7.2 million births. Time period is 1981-1-1 until 2022-12-31. Only dates are given not times. Lat/lon ranges appear to include Texas through Florida 

Remove rows with missing values
```{r}
Births.df <- na.omit(Births.df)
```

Make the data frame a simple feature data frame with POINT geometry
```{r}
Births.sf <- Births.df |>
  dplyr::mutate(Date = lubridate::as_date(DATE_OF_BIRTH),
                DateT1s = lubridate::as_date(trimester1_s_date),
                DateT1e = lubridate::as_date(trimester1_e_date),
                DateT2s = lubridate::as_date(trimester2_s_date),
                DateT2e = lubridate::as_date(trimester2_e_date),
                DateT3s = lubridate::as_date(trimester3_s_date),
                Year_of_Birth = lubridate::year(Date),
                Month_of_Birth = lubridate::month(Date),
                Month_of_T1s = lubridate::month(DateT1s),
                Month_of_T1e = lubridate::month(DateT1e),
                Month_of_T2s = lubridate::month(DateT2s),
                Month_of_T2e = lubridate::month(DateT2e),
                Month_of_T3s = lubridate::month(DateT3s)
                ) |>
  sf::st_as_sf(coords = c("final_lon", "final_lat"),
               crs = 4326) |>
  dplyr::select(Birth_ID = ID, Birth_Date = Date, DateT1s, DateT1e,
                DateT2s, DateT2e, DateT3s,
                Month_of_Birth, Year_of_Birth, 
                Month_of_T1s, Month_of_T1e, 
                Month_of_T2s, Month_of_T2e, Month_of_T3s)
```

# Start of code that parses by trimester

Keep only births in which the birth year falls within the range of storm years 
```{r}
withIn <- Births.sf$Year_of_Birth >= storm_year_range[1] & Births.sf$Year_of_Birth <= storm_year_range[2]
Births.sf <- Births.sf[withIn, ]
```

Keep only births in which the trimester start or end months are within the storm season. Here we start with the third trimester
```{r}
withIn <- Births.sf$Month_of_Birth %in% storm_months | Births.sf$Month_of_T3s %in% storm_months
Births.sf <- Births.sf[withIn, ]
```

Keep only births occurring inside the union of all threat/impact/cleanup polygons. We assume the trimester dates have the same spatial location (the mother did not relocate during pregnancy)
```{r}
withIn <- Births.sf |>
  sf::st_within(union_Swaths2.sfg, sparse = FALSE)
Births.sf <- Births.sf[withIn, ]
```

Check with a map. Sample 1,000 birth records
```{r}
nrows <- nrow(Births.sf)
keep_rows <- sample.int(nrows, size = 1000)

tmap::tm_shape(Swaths2.sf) +
  tmap::tm_borders() +
tmap::tm_shape(Births.sf$geometry[keep_rows]) +
  tmap::tm_dots()
```

Filter storms by third trimester date range. Test on a sample of births
```{r}
start_time <- Sys.time()
#keep_rows <- sample.int(nrow(Births.sf), size = 5000)

num_obs <- nrow(Births.sf)
chunk_size <- 50000
num_chunks <- ceiling(num_obs / chunk_size)
print(paste("Number of chunks to process", num_chunks))

# Pre-allocate an empty list to store chunks
results_list <- vector("list", num_chunks)

for (i in seq(1, num_obs, by = chunk_size)) {
  chunk_index <- (i - 1) / chunk_size + 1
  chunk_indices <- i:min(i + chunk_size - 1, num_obs)
  
  print(paste("Processing chunk", chunk_index))

  chunk.sf <- Births.sf[chunk_indices, ]

# 1. Filter storms by trimester date range
births_storm_filtered <- chunk.sf |>
  dplyr::rowwise() |>
  dplyr::mutate(
    matched_storms = list(
      TIC.sf |>
        dplyr::filter(Date >= DateT3s & Date <= Birth_Date))) |>
  tidyr::unnest(matched_storms, keep_empty = TRUE)

# 2. Perform spatial join to check if birth locations fall within the storm polygons
births_with_impacts <- sf::st_join(births_storm_filtered, TIC.sf, join = sf::st_within)

# 3. Keep only records where a birth was inside a storm impact area during the specified trimester
births_with_impacts <- births_with_impacts |>
  dplyr::filter(!is.na(NAME.x)) |>
  dplyr::select(Birth_ID, Birth_Date, DateT3s,
                NAME = NAME.x, Max_Intensity_mps = Max_Intensity_mps.x,
                TIC = TIC.x, geometry) |>
  dplyr::distinct()

results_list[[chunk_index]] <- births_with_impacts

gc() #ensures memory from previous iterations is released

}

final_data <- do.call(rbind, results_list)
Sys.time() - start_time
```

Pivot to wider format
```{r}
Y <- final_data |>
  sf::st_drop_geometry()
Y <- Y[1:100000, ]

X <- Y |> 
  dplyr::summarise(n = dplyr::n(),
                   .by = c(Birth_ID, Birth_Date, DateT3s, NAME, Max_Intensity_mps)) |>
  dplyr::filter(n >= 1)

XX <- Y |>
  tidyr::pivot_wider(names_from = NAME, values_from = TIC)
                     
```


Check on a map
```{r}
tmap::tm_shape(Swaths2.sf) +
  tmap::tm_borders() +
tmap::tm_shape(births_with_impacts$geometry) +
  tmap::tm_dots()
```

## Merge with death records

File received from Jihoon on December 7, 2024 via email. From email:
1) Whether impacted or not when a hurricane passes on the date of death? What was impacted? 2) Whether impacted or not during a warning on the date of death
3) Whether impacted or not during a watch on the date of death. 4) Whether impacted or not during an advisory on the date of death
5) (If available) Whether impacted or not during an evacuation order on the date of death

```{r}
Deaths.df <- read.csv(file = "data/all_deaths_hurricane.csv")
```

6.2 million deaths. Time period is 1981-1-1 until 2022-12-31. Only dates are given not times. Note: Watch/warning data are only available starting in 2008. lat/lon ranges appear to include Texas through Florida

Create a simple feature data frame with point geometry from the original data frame
```{r}
Deaths.sf <- Deaths.df |>
  dplyr::mutate(Date = lubridate::as_date(DATE_OF_DEATH)) |>
  sf::st_as_sf(coords = c("final_lon", "final_lat"),
               crs = 4326) |>
  dplyr::select(Death_ID = ID, Death_Date = Date)
```

Keep only deaths occurring inside the union of all threat/impact/cleanup polygons. Note: when using tropical cyclones and Florida this will be almost all deaths
```{r}
withIn <- Deaths.sf |>
  sf::st_within(union_Swaths2.sfg, sparse = FALSE)
DeathsWithin.sf <- Deaths.sf[withIn, ]
```

Check with a map by plotting locations from a sample of 10,000 death records. Note: this is only useful for hurricanes
```{r}
nrows <- nrow(DeathsWithin.sf)
keep_rows <- sample.int(nrows, size = 10000)

tmap::tm_shape(Swaths2.sf) +
  tmap::tm_borders() +
tmap::tm_shape(DeathsWithin.sf$geometry[keep_rows]) +
  tmap::tm_dots()
```

Filter storms by death date. Test on a sample of death records
```{r}
keep_rows <- sample.int(nrow(DeathsWithin.sf), size = 5000)
merge_Test.sf <- DeathsWithin.sf[keep_rows, ]
#merge_Test.sf <- DeathsWithin.sf

sf::st_geometry(TIC.sf) <- "geom" # rename geometry column. Column names from the two sf data frames must differ

# 1. Filter storms by death date
start_time <- Sys.time()
deaths_storm_filtered <- merge_Test.sf |>
  dplyr::rowwise() |>
  dplyr::mutate(
    matched_storms = list(
      TIC.sf |>
        dplyr::filter(Date == Death_Date))) |>
  tidyr::unnest(matched_storms, keep_empty = TRUE)
Sys.time() - start_time
# ~ 2.07 minutes for processing 100,000 death records under HURRICANE swaths using a M1 Pro 32 GB laptop
# ~ 5.25 hours for processing all death records under TC swaths

# 2. Perform spatial join to check if birth locations fall inside the storm polygons
deaths_with_impacts <- sf::st_join(deaths_storm_filtered, TIC.sf, join = sf::st_within)
# Error: vector memory limit of 32.0 Gb reached, see mem.maxVSize()

# 3. Keep only records where a death was inside the correct storm impact area
deaths_with_impacts <- deaths_with_impacts |>
  dplyr::filter(!is.na(NAME.x)) |>
  dplyr::select(Death_ID, Death_Date,
                NAME = NAME.x, TIC = TIC.x, geometry) |>
  dplyr::distinct()

# 4. Deaths outside correct impact zones but within the union of all impact zones

X <- !merge_Test.sf$Death_ID %in% deaths_with_impacts$Death_ID
deaths_without_impacts <- merge_Test.sf[X, ]
```

Check on a map
```{r}
tmap::tm_shape(Swaths2.sf) +
  tmap::tm_borders() +
tmap::tm_shape(deaths_with_impacts$geometry) +
  tmap::tm_dots()
```

Death rate per day with and without impacts
```{r}
length(unique(deaths_with_impacts$Death_ID)) / length(unique(deaths_with_impacts$Death_Date))
length(unique(deaths_without_impacts$Death_ID)) / length(unique(deaths_without_impacts$Death_Date))

deaths_with_impacts |>
  dplyr::group_by(TIC) |>
  dplyr::summarise(length(unique(Death_ID)) / length(unique(Death_Date)))
```

Convert to regular data frame with X (Longitude) and Y (Latitude)
```{r}
deaths_with_impacts.df <- deaths_with_impacts |>
  dplyr::mutate(
    final_lat = sf::st_coordinates(deaths_with_impacts)[,2],  # Extract latitude
    final_lon = sf::st_coordinates(deaths_with_impacts)[,1]  # Extract longitude
  ) |>
  sf::st_drop_geometry() 
```

## Toy example of a space-time merge using simple feature data frames

```{r}
library(sf)
library(dplyr)

# Example: Storm data (impact polygons and attributes)
storms <- st_sf(
  storm_name = c("Storm A", "Storm B", "Storm C"),
  impact_level = c(1, 2, 3),
  storm_date = as.Date(c("2023-01-15", "2023-02-20", "2023-03-10")),
  geometry = st_sfc(
    st_polygon(list(rbind(c(0, 0), c(0, 5), c(5, 5), c(5, 0), c(0, 0)))),
    st_polygon(list(rbind(c(3, 3), c(3, 8), c(8, 8), c(8, 3), c(3, 3)))),
    st_polygon(list(rbind(c(6, 6), c(6, 9), c(9, 9), c(9, 6), c(6, 6))))
  ),
  crs = 4326
)

# Example: Birth records (point locations and third trimester dates)
births <- st_sf(
  birth_date = as.Date(c("2023-05-01", "2023-03-01", "2023-01-20", "2023-09-01")),
  trimester_start = as.Date(c("2023-02-01", "2022-12-01", "2022-10-01", "2022-12-01")),
  trimester_end = as.Date(c("2023-04-30", "2023-02-28", "2023-01-19", "2023-08-30")),
  geometry = st_sfc(
    st_point(c(2, 2)),
    st_point(c(4, 4)),
    st_point(c(7, 7)),
    st_point(c(1, 7))
  ),
  crs = 4326
)

st_geometry(storms) <- "geom"

# 1. Filter storms by third trimester date range
births_storms_filtered <- births %>%
  dplyr::rowwise() %>%
  dplyr::mutate(
    matched_storms = list(
      storms %>%
        dplyr::filter(storm_date >= trimester_start & storm_date <= trimester_end)
    )
  ) %>%
  tidyr::unnest(matched_storms, keep_empty = TRUE)

# 2. Perform spatial join to check if birth locations fall within the storm polygons
births_with_impacts <- sf::st_join(births_storms_filtered, storms, join = st_within)

# 3. Keep only records where a birth was inside a storm impact area
births_with_impacts <- births_with_impacts %>%
  dplyr::filter(!is.na(storm_name.y)) %>%
  dplyr::select(birth_date, trimester_start, trimester_end, 
                storm_name = storm_name.y, impact_level = impact_level.y, geometry) |>
  dplyr::distinct()

# Display the final dataset
print(births_with_impacts)
```

# Watch-warning data from IA State

Data source: https://mesonet.agron.iastate.edu/request/gis/watchwarn.phtml

```{r}
storms.sf <- sf::st_read(dsn = here::here("data", "wwa_198601010000_202502080000"), 
                         layer = "wwa_198601010000_202502080000")

phenom <- c("HU", "HI", "SS", "TI", "TR")
storms.sf <- storms.sf |>
  dplyr::filter(PHENOM %in% phenom) 
```

SIGNIFICANCE = 
    "W": "Warning",
    "A": "Watch",
    "S": "Statement"
    
PHENOMENA = 
    "HI": "Inland Hurricane",
    "HU": "Hurricane",
    "SS": "Storm Surge",
    "TI": "Inland Tropical Storm",
    "TR": "Tropical Storm"

Parse the character strings to date/time objects
```{r}
storms2.sf <- storms.sf |>
  dplyr::mutate(ISSUED = lubridate::ymd_hm(ISSUED),
                EXPIRED = lubridate::ymd_hm(EXPIRED),
                INIT_ISS = lubridate::ymd_hm(INIT_ISS),
                INIT_EXP = lubridate::ymd_hm(INIT_EXP),
                Duration = EXPIRED - INIT_ISS, 
                Year = lubridate::year(ISSUED),
                DateStart = as.Date(INIT_ISS),
                DateEnd = as.Date(EXPIRED)
                ) |>
  dplyr::select(Year, DateStart, DateEnd, Duration, WFO, PHENOM, SIG, STATUS, NWS_UGC, AREA_KM2, geometry)
units(storms2.sf$Duration) <- "hours"

X.sf <- storms2.sf |>
  dplyr::filter(PHENOM == "HU" & SIG == "W")
```

Statistics
```{r}
storms2.sf |>
  sf::st_drop_geometry() |>
  dplyr::filter(PHENOM == "HU" & SIG == "W") |>
  dplyr::group_by(Year) |>
  dplyr::summarise(Number = dplyr::n(),
                   AvgDuration = mean(Duration),
                   AvgArea = mean(AREA_KM2))
```

No change in the average duration of the warning or the average area warned over the years


Code below is adapted from David Hsu

Here we read in the csv file containing hurricane watch and advisories for Florida from Iowa State University database
```{r}
warning_advisories <- readr::read_csv(here::here("data", "wwa_198601010000_202501010000.csv"))
```

How many unique names in the column labeled "phenomena"
```{r}
unique(warning_advisories$phenomena)
```

Filter the data to only include hurricanes
```{r}
phenom <- c("HU", "HI", "SS", "TI", "TR")
storms <- warning_advisories |>
  dplyr::filter(phenomena %in% phenom) 
```

Florida counties and zone shapefiles
from: https://www.weather.gov/gis/Counties
from: https://www.weather.gov/gis/publiczones
```{r}
counties.sf <- sf::st_read(dsn = here::here("data", "c_18mr25"), 
                           layer = "c_18mr25") |>
  dplyr::filter(STATE == "FL")

zones.sf <- sf::st_read(dsn = here::here("data", "z_18mr25"),
                        layer = "z_18mr25") |>
  dplyr::filter(STATE == "FL")  
```

Note that the `storms` dataset and the `zones.sf` dataset have different nomenclature for the zone names, where the `storms` dataset uses the "ugc" column and the `zones.sf` dataset uses the "STATE_ZONE" column. Also, the zone names in the `storms` dataset includes the letter "z" in the designation so we need to create a new column in the dataset that removes the "z" from the ugc column.
```{r}
storms <- storms |>
  dplyr::mutate(ugc = stringr::str_replace(ugc, "FLZ", "FL"))
```

Make sure the data sets match
```{r}
unique(storms$ugc)

unique(zones.sf$STATE_ZONE)
```

Join `storms` data with `zones.sf` shapefile
```{r}
storm_zones <- zones.sf |>
  dplyr::left_join(storms, by = c("STATE_ZONE" = "ugc"))

head(storm_zones)
```

Create dates from character strings
```{r}
storm_zones <- storm_zones |>
  dplyr::mutate(utc_issue = as.Date(utc_issue),
                utc_expire = as.Date(utc_expire),
                year = lubridate::year(utc_issue)) |>
  dplyr::arrange(utc_issue)
```

Time series graph of the number of issuances
```{r}
library(ggplot2)

storm_zones |>
  sf::st_drop_geometry() |>
  dplyr::group_by(phenomena, year) |>
  dplyr::summarise(Number = dplyr::n()) |>
  print(n = 100)
```

