---
title: "Daily death counts prior to storm impacts"
output: html_document
editor_options:
  chunk_output_type: console
---

## Create an empirical storm threat model

```{r}
library(sf)
library(dplyr)

storm_intensity <- 55 # all tropical storms and hurricanes
begin_year <- 1985 # 1985 for 55+ winds, 1981 for 34+ winds
end_year <- 2022
begin_date <- paste0(begin_year, "-01-01")
end_date <- paste0(end_year, "-12-31")

L <- "https://www.ncei.noaa.gov/data/international-best-track-archive-for-climate-stewardship-ibtracs/v04r00/access/shapefile/IBTrACS.NA.list.v04r00.lines.zip"
  
if(!"IBTrACS.NA.list.v04r00.lines.zip" %in% list.files(here::here("data"))) {
download.file(url = L,
              destfile = here::here("data",
                                    "IBTrACS.NA.list.v04r00.lines.zip"))
unzip(here::here("data", "IBTrACS.NA.list.v04r00.lines.zip"),
      exdir = here::here("data"))
}

Tracks.sf <- st_read(dsn = here::here("data"), 
                     layer = "IBTrACS.NA.list.v04r00.lines") |>
  st_transform(crs = 32616)

Tracks.sf <- Tracks.sf |>
  filter(year >= begin_year & year <= end_year) |>
  filter(USA_WIND >= storm_intensity) |>
  select(SID, SEASON, year, month, day, hour, min,
         NAME, SUBBASIN, ISO_TIME, USA_WIND, USA_PRES, USA_RMW, USA_EYE, USA_ROCI)

Tracks.sf |>
  st_drop_geometry() |>
  summarize(avgRMW = mean(USA_RMW, na.rm = TRUE) * 1.852,
                   avgEYE = mean(USA_EYE, na.rm = TRUE) * 1.852,
                   avgROCI = mean(USA_ROCI, na.rm = TRUE) * 1.852)

# Fill in missing RMW values. Start with pedigree, then use minimum pressure, and finish again with pedigree
Tracks.sf <- Tracks.sf |>
  group_by(SID) |>  # pedigree
  mutate(USA_RMW = ifelse(is.na(USA_RMW), mean(USA_RMW, na.rm = TRUE), USA_RMW))

Tracks.sf <- Tracks.sf |>
  group_by(USA_PRES) |> # minimum pressure
  mutate(USA_RMW = ifelse(is.na(USA_RMW), mean(USA_RMW, na.rm = TRUE), USA_RMW))

Tracks.sf <- Tracks.sf |>
  group_by(SID) |> # pedigree
  mutate(USA_RMW = ifelse(is.na(USA_RMW), mean(USA_RMW, na.rm = TRUE), USA_RMW))

#Add a buffer to the tracks to make segmented wind swaths
Swaths.sf <- Tracks.sf |>
  st_buffer(dist = Tracks.sf$USA_RMW * 1852) # 1852 converts to meters

# Wind swaths that cross Florida. `USAboundaries` package no longer maintained on CRAN
#devtools::install_github("ropensci/USAboundariesData")
#devtools::install_github("ropensci/USAboundaries", force = TRUE)
#install.packages("USAboundariesData", repos = "https://ropensci.r-universe.dev", type = "source")
Boundaries.sf <- USAboundaries::us_states(resolution = "low", states = "FL") |> 
  st_transform(crs = 32616)

X <- Swaths.sf |>
  st_intersects(Boundaries.sf, sparse = FALSE) #Does the swath intersect the state border?
Swaths.sf <- Swaths.sf[X, ]
Swaths.sf <- Swaths.sf |>
  mutate(Date = lubridate::as_date(ISO_TIME)) #Add a date column

# Extract the boundaries of storm impacts by storm ID. Dissolve the overlap borders of the individual swaths by storm to create a single storm swath. Add the storm category (Saffir-Simpson) based on wind speed
Swaths.sf <- Swaths.sf |>
  group_by(SID) |>
  summarize(Date0 = first(Date),
            NAME = first(NAME),
            Wind = max(USA_WIND),
            geometry = st_union(geometry)) |>
  mutate(Storm_Category = case_when(
    Wind >= 34 & Wind <= 63 ~ 0,
    Wind >= 64 & Wind <= 82 ~ 1,
    Wind >= 83 & Wind <= 95 ~ 2,
    Wind >= 96 & Wind <= 112 ~ 3,
    Wind >= 113 & Wind <= 136 ~ 4,
    Wind >= 137 ~ 5
  ))

# Transform the geometry to a geographic CRS (4326) and unionize the swaths
sf_use_s2(TRUE)

Swaths.sf <- Swaths.sf |>
    st_transform(crs = 4326)
union_Swaths2.sfg <- Swaths.sf |>
  st_union()
```

## Expand `Swaths.sf` by adding rows based on the increment value of the attribute `Date0`

```{r}
library(tidyr)
library(lubridate)

min_lag  <- -14   # days before impact
max_lead <- 10   # days after impact

TIC.sf <- Swaths.sf %>%
  rowwise() %>%
  mutate(event_days = list(
    tibble(
      Date     = Date0 + (min_lag:max_lead),
      rel_day  = min_lag:max_lead
    )
  )) %>%
  unnest(event_days) %>%
  ungroup() %>%
  select(
    Date,
    rel_day,
    Storm_Name = NAME,
    Storm_Category
  )

# Create event-time factors
TIC.sf <- TIC.sf %>%
  mutate(
    rel_day_f = factor(rel_day)
  )

# Add month and year indicators change name of the geometry column (this is needed for spatial merges)
TIC.sf <- TIC.sf |>
  mutate(Month = lubridate::month(Date),
         Year = lubridate::year(Date))
storm_months <- unique(TIC.sf$Month)
storm_year_range <- range(TIC.sf$Year)

st_geometry(TIC.sf) <- "geom"
```

## Build a zip x date relative day calendar

~ 25 sec
```{r}
library(data.table)
library(tigris)
library(sf)
library(dplyr)

options(tigris_use_cache = TRUE)

# --- ZCTAs for Florida (2020) ---
zcta_us <- zctas(cb = TRUE, year = 2020) %>%
  select(zip = ZCTA5CE20, geometry)

fl <- states(cb = TRUE, year = 2020) %>%
  filter(STUSPS == "FL") %>%
  st_transform(st_crs(zcta_us))

zctas_fl <- st_join(zcta_us, fl, join = st_intersects, left = FALSE) %>%
  select(zip, geometry)

# --- Align CRS ---
tic_aligned <- st_transform(TIC.sf, st_crs(zctas_fl))

# --- Keep only what we need from TIC: Date + rel_day (or rel_day_f) + geometry ---
# Prefer storing numeric rel_day and constructing rel_day_f later.
tic_keep <- tic_aligned %>%
  select(Date, rel_day, geom)

# --- Spatial join: which zips intersect a TIC polygon on that date ---
zcta_date_rel_sf <- st_join(
  zctas_fl,
  tic_keep,
  join = st_intersects,
  left = FALSE
)

# --- Drop geometry and convert to data.table ---
zcta_date_rel <- as.data.table(st_drop_geometry(zcta_date_rel_sf))
setnames(zcta_date_rel, c("zip", "Date", "rel_day"), c("zip", "date", "rel_day"))

# --- Normalize types ---
zcta_date_rel[, `:=`(
  zip     = as.character(zip),
  date    = as.IDate(date),
  rel_day = as.integer(rel_day)
)]

# --- Resolve multiple hits per ZIP–date ---
# Rule: choose rel_day closest to 0 (impact). Tie-break: prefer negative (pre-impact).
zcta_date_rel <- zcta_date_rel[
  , .SD[order(abs(rel_day), rel_day)][1],
  by = .(zip, date)
]

# --- Build full ZIP × date grid (choose your desired universe) ---
all_zips <- sort(unique(zctas_fl$zip))
all_dates <- as.IDate(seq(as.Date(begin_date), as.Date(end_date), by = "day"))
grid <- CJ(zip = as.character(all_zips), date = all_dates)

# --- Left-join observed rel_day onto full grid ---
rel_calendar <- zcta_date_rel[grid, on = .(zip, date)]

# --- Construct rel_day_f (factor) with "None" for non-storm days ---
min_lag  <-  -14L
max_lead <-  10L

rel_levels <- c("None", as.character(min_lag:max_lead))

rel_calendar[, rel_day_f := fifelse(is.na(rel_day), "None", as.character(rel_day))]
rel_calendar[, rel_day_f := factor(rel_day_f, levels = rel_levels)]

# --- Sanity checks ---
stopifnot(rel_calendar[, .N, by = .(zip, date)][, all(N == 1)])
rel_calendar[, table(rel_day_f)]
```

## Merge storms with death records

Files received from Jihoon on November 27, 2025 

~ 2 minutes
```{r}
start_time <- Sys.time()
load("data/all_data.Rdata")

# load("data/cardio_subset_all.Rdata")
# load("data/respir_subset_all.Rdata")
# load("data/injury_subset_all.Rdata")
# load("data/neuropsych_subset_all.Rdata")


#Deaths.df <- neuropsych_subset_all %>% # Use for subsets
#  mutate(Date = as_date(DATE_OF_DEATH)) %>%
#  filter(Date >= as.Date(begin_date)) %>%
#  select(Death_ID = ID, Death_Date = Date, final_lon, final_lat)
  

Deaths.df <- all_data %>%   # Use for all deaths
  mutate(Date = as_date(DATE_OF_DEATH)) %>%
  filter(Date >= as.Date(begin_date)) %>% 
  select(Death_ID = ID, Death_Date = Date, final_lon, final_lat, 
         age = COMPUTED_AGE, white = RACE_WHITE, sex = SEX, 
         marital_code = MARITAL_CODE, army = ARMY_YESNO)

Deaths.sf <- Deaths.df %>%
  st_as_sf(coords = c("final_lon", "final_lat"), crs = 4326)
rm(all_data, Deaths.df)
Sys.time() - start_time
```

## Filter deaths by date of death and storm effect zones

~ 10 minutes
```{r}
library(sf)
library(dplyr)

start_time <- Sys.time()

target_crs <- 5070
sf_use_s2(FALSE)

# IMPORTANT: set window here
min_lag  <-  -14L
max_lead <-  10L

TIC_proj    <- st_transform(TIC.sf,    target_crs)
Deaths_proj <- st_transform(Deaths.sf, target_crs)

num_obs <- nrow(Deaths_proj)
chunk_size <- 50000
num_chunks <- ceiling(num_obs / chunk_size)
print(paste("Number of chunks to process", num_chunks))

results_list <- vector("list", num_chunks)

for (i in seq(1, num_obs, by = chunk_size)) {
  chunk_index <- (i - 1) / chunk_size + 1
  chunk_indices <- i:min(i + chunk_size - 1, num_obs)

  print(paste("Processing chunk", chunk_index))

  chunk.sf <- Deaths_proj[chunk_indices, ]

  chunk_dates <- unique(chunk.sf$Death_Date)
  TIC_chunk <- TIC_proj %>% filter(Date %in% chunk_dates)

  if (nrow(TIC_chunk) == 0) {
    results_list[[chunk_index]] <- NULL
    gc()
    next
  }

  deaths_joined <- st_join(chunk.sf, TIC_chunk, join = st_within, left = FALSE) %>%
    filter(Date == Death_Date) %>%                 # enforce same-day
    mutate(rel_day = as.integer(rel_day)) %>%
    filter(rel_day >= min_lag, rel_day <= max_lead)

  if (nrow(deaths_joined) == 0) {
    results_list[[chunk_index]] <- NULL
    gc()
    next
  }

  deaths_with_impacts <- deaths_joined %>%
    group_by(Death_ID, Death_Date) %>%
    arrange(abs(rel_day), rel_day) %>%             # tie-break toward pre-impact
    slice(1) %>%
    ungroup() %>%
    transmute(
      Death_ID, Death_Date,
      Storm_Name, Storm_Category,
      rel_day,
      rel_day_f = factor(as.character(rel_day), levels = as.character(min_lag:max_lead)),
#      age, white, sex, marital_code, army, # use for all deaths
      geometry
    ) %>%
    distinct()

  results_list[[chunk_index]] <- deaths_with_impacts
  gc()
}

deaths_with_impacts <- do.call(rbind, results_list)
Sys.time() - start_time
```

## Combine with all deaths

```{r}
impacts_keep <- deaths_with_impacts %>%
  st_drop_geometry() %>%
  select(Death_ID, Death_Date, Storm_Name, Storm_Category, rel_day, rel_day_f)

deaths_all_effects <- Deaths.sf %>%
  left_join(impacts_keep, by = c("Death_ID", "Death_Date")) %>%
  mutate(
    rel_day_f = replace_na(as.character(rel_day_f), "None"),
    rel_day_f = factor(rel_day_f, levels = c("None", as.character(min_lag:max_lead))),
    rel_day   = if_else(rel_day_f == "None", NA_integer_, as.integer(rel_day))
  )
```

Export (optional)
```{r}
# To CSV
st_drop_geometry(deaths_all_effects) %>%
  readr::write_csv("data/outputs/Deaths/All_Deaths_55kt.csv")

# To GeoPackage
st_write(
  deaths_all_effects,              # full sf object with long names
  dsn = "data/outputs/Deaths/All_Deaths_55kt.gpkg",
  layer = "All_Deaths_Threat_55kt",
  delete_dsn = TRUE
)
```

## Analytics of deaths and storms

Compute statewide death rates
```{r}
# Inputs:
# - deaths_all_effects: all deaths with rel_day_f already attached ("None", "-7"... "+3")
# - TIC.sf: storm swath polygons expanded with Date and rel_day (event window -7:+3)

# -----------------------------
# 1) Death totals by rel_day_f
# -----------------------------
Deaths.df <- deaths_all_effects %>% st_drop_geometry()

total_deaths <- Deaths.df %>%
  mutate(rel_day_f = as.character(rel_day_f)) %>%
  group_by(rel_day_f) %>%
  summarise(total_deaths = n(), .groups = "drop")

# -----------------------------------------
# 2) Statewide calendar: one rel_day per day
# -----------------------------------------
# Collapse TIC.sf to one rel_day per Date using:
#  - min abs(rel_day)
#  - tie-break toward negative rel_day (pre-impact)
state_calendar <- TIC.sf %>%
  st_drop_geometry() %>%
  transmute(Date = as.Date(Date), rel_day = as.integer(rel_day)) %>%
  filter(rel_day >= min_lag, rel_day <= max_lead) %>%
  group_by(Date) %>%
  arrange(abs(rel_day), rel_day) %>%   # pre-impact wins ties
  slice(1) %>%
  ungroup() %>%
  transmute(
    date = Date,
    rel_day_f = as.character(rel_day)
  )

# Full calendar over the study period (use deaths date range as the master range)
all_dates <- tibble(
  date = seq(min(Deaths.df$Death_Date, na.rm = TRUE),
             max(Deaths.df$Death_Date, na.rm = TRUE),
             by = "day")
)

state_calendar_full <- all_dates %>%
  left_join(state_calendar, by = "date") %>%
  mutate(rel_day_f = replace_na(rel_day_f, "None"))

# Count calendar-days in each rel_day_f category
num_days <- state_calendar_full %>%
  count(rel_day_f, name = "num_days")

# 3) Calendar-day death rates by rel_day

death_rates <- total_deaths %>%
  full_join(num_days, by = "rel_day_f") %>%
  mutate(
    total_deaths = replace_na(total_deaths, 0L),
    daily_death_rate = total_deaths / num_days
  ) %>%
  mutate(rel_day_f = factor(rel_day_f, levels = c("None", as.character(min_lag:max_lead)))) %>%
  arrange(rel_day_f)

print(death_rates)
```

## Add zip codes to each death

Spatial join deaths to zips
~ 1.3 hours
```{r}
begin <- Sys.time()

target_crs <- 5070
sf_use_s2(FALSE)

zctas_fl_proj <- st_transform(zctas_fl, target_crs)
Deaths_proj <- st_transform(deaths_all_effects, target_crs)

deaths_with_zip <- st_join(
  Deaths_proj,
  zctas_fl_proj,
  join    = st_intersects,
  left    = TRUE,
  largest = TRUE
)

deaths_with_zip <- st_transform(deaths_with_zip, 4326)

# How many didn't land in any ZCTA?
sum(is.na(deaths_with_zip$zip))

# Drop if desired
deaths_with_zip <- deaths_with_zip %>%
  filter(!is.na(zip))

Sys.time() - begin
```

Save
```{r}
saveRDS(deaths_with_zip, "data/outputs/Results/deaths_with_zip_55kt.rds")

deaths_with_zip_55kt <- readRDS("data/outputs/Results/deaths_with_zip_55kt.rds")
deaths_with_zip_34kt <- readRDS("data/outputs/Results/deaths_with_zip_34kt.rds")
```

## Panel-ready daily counts

One row per zip-date
~ 20 sec
```{r}
begin <- Sys.time()
# 1) deaths per ZIP–date (no rel_day_f here)
 daily_deaths <- deaths_with_zip_55kt %>% # use for all deaths
# daily_deaths <- deaths_with_zip %>%
  st_drop_geometry() %>%
  transmute(
    zip   = as.character(zip),
    date  = as.IDate(Death_Date)
#    age   = as.integer(age),
#    white = white,
#    sex   = sex,
#    marital_code = marital_code,
#    army = army
  ) %>%
  # optional filters:
  # filter(age >= 60) %>%
  # filter(sex == "F") %>%
  # filter(white == "N") %>%
  # filter(marital_code %in% c("3", "4", "6")) %>%
  # filter(army == "Y") %>%
  group_by(zip, date) %>%
  summarise(deaths = n(), .groups = "drop")

# 2) Join to ZIP×date exposure calendar (must have unique zip-date rel_day_f)
#    Assuming your calendar is named rel_calendar with columns: zip, date, rel_day_f
dat <- as.data.table(daily_deaths)
cal <- as.data.table(rel_calendar)

# Ensure consistent types
dat[, `:=`(zip = as.character(zip), date = as.IDate(date))]
cal[, `:=`(zip = as.character(zip), date = as.IDate(date))]

# Left join calendar -> deaths (keeps all ZIP-days in calendar)
panel <- dat[cal, on = .(zip, date)]

# Fill no-death days with 0
panel[is.na(deaths), deaths := 0L]

# 3) Ensure rel_day_f factor levels are correct
panel[, rel_day_f := factor(as.character(rel_day_f), levels = c("None", as.character(min_lag:max_lead)))]

# Sanity check: exactly one row per ZIP–date
stopifnot(panel[, .N, by = .(zip, date)][, all(N == 1)])

Sys.time() - begin
```

Export (optional)
```{r}
readr::write_csv(panel, "data/outputs/Deaths/daily_panel_55kt.csv")

panel_34kt <- readr::read_csv("data/outputs/Deaths/daily_panel_34kt.csv")
panel_55kt <- readr::read_csv("data/outputs/Deaths/daily_panel_55kt.csv")

panel <- panel_34kt
```

## Poisson model with stratum fixed effects

Next we fit a Poisson with stratum fixed effects. This is equivalent to conditioning on the stratum totals and largely removes serial confounding. Use robust (clustered) SEs by stratum to protect against residual correlation within a stratum

Add strata variables. Include only months during the hurricane season (May-November)
~ 3.5 minutes
```{r}
library(fixest)
library(broom)

begin <- Sys.time()
# zip_day must already have one row per zip-date: columns: zip, date, deaths, rel_day_f

dat <- panel %>%
  filter(lubridate::month(as.Date(date)) %in% 5:11) %>%  # hurricane season only
  transmute(
    zip      = as.character(zip),
    date     = as.IDate(date),
    deaths   = as.integer(deaths),
    rel_day_f = factor(as.character(rel_day_f), levels = c("None", as.character(min_lag:max_lead))),
    dow      = factor(strftime(as.Date(date), "%u"),
                      levels = as.character(1:7),
                      labels = c("Mon","Tue","Wed","Thu","Fri","Sat","Sun")),
    ym       = format(as.Date(date), "%Y-%m")
  )

m_event <- fepois(
  deaths ~ i(rel_day_f, ref = "None") | zip + ym + dow,
  data    = dat,
  cluster = ~ zip + ym,
  notes   = FALSE
)

summary(m_event)

Sys.time() - begin
```

```{r}
rr_tbl <- tidy(m_event, conf.int = TRUE) %>%
  filter(grepl("^rel_day_f::", term)) %>%
  transmute(
    rel_day = sub("^rel_day_f::", "", term),
    RR      = exp(estimate),
    LCI     = exp(conf.low),
    UCI     = exp(conf.high),
    p       = p.value
  ) %>%
  mutate(rel_day = factor(rel_day, levels = as.character(min_lag:max_lead))) %>%  
  arrange(rel_day)

rr_tbl
```

## Plot results
```{r}
library(ggplot2)

plot_rr_event <- function(model, prefix = "rel_day_f::") {

  rr_df <- tidy(model, conf.int = TRUE) %>%
    # keep only lag/lead terms
    filter(startsWith(term, prefix)) %>%
    mutate(
      rel_day = as.integer(sub(prefix, "", term)),
      RR  = exp(estimate),
      LCI = exp(conf.low),
      UCI = exp(conf.high)
    ) %>%
    arrange(rel_day)

  ggplot(rr_df, aes(x = rel_day, y = RR)) +
    geom_hline(yintercept = 1, linetype = "dashed") +
    geom_ribbon(aes(ymin = LCI, ymax = UCI), fill = "grey80", alpha = 0.7) +
    geom_line(linewidth = 0.9) +
    geom_point(size = 2) +
    scale_x_continuous(breaks = sort(unique(rr_df$rel_day))) +
    labs(
      x = "Days relative to impact (lag/lead)",
      y = "Rate ratio (RR)",
      title = "RR by lag/lead day with 95% CI"
    ) +
    theme_minimal()
}

# run it
plot_rr_event(m_event)
```

## Expected excess deaths spatially

```{r}
beta_lag1 <- coef(m_event)["rel_day_f::-1"]
RR_lag1   <- exp(beta_lag1)

beta_lag1
RR_lag1

# RR_lag1 ≈ multiplicative increase in mortality on day −1 vs None.
```

This is the key idea: We use the fitted model to predict: 
1. expected deaths if the day were None
2. expected deaths if the day were −1
The difference is the modeled excess

```{r}
library(data.table)
setDT(dat)

dat_none <- copy(dat)
dat_none[, rel_day_f := factor("None", levels = levels(dat$rel_day_f))]

dat_lag1 <- copy(dat)
dat_lag1[, rel_day_f := factor("-1", levels = levels(dat$rel_day_f))]

mu_none <- predict(m_event, newdata = dat_none, type = "response")
mu_lag1 <- predict(m_event, newdata = dat_lag1, type = "response")

dat[, `:=`(
  mu_none = mu_none,
  mu_lag1 = mu_lag1,
  excess_lag1 = mu_lag1 - mu_none
)]

zip_contrib <- dat[, .(
  mean_baseline = mean(mu_none, na.rm = TRUE),
  mean_excess   = mean(excess_lag1, na.rm = TRUE),
  total_excess  = sum(excess_lag1, na.rm = TRUE),
  n_lag1_days   = sum(rel_day_f == "-1")
), by = zip]

library(ggplot2)
library(dplyr)

zip_map <- zctas_fl %>%
  mutate(zip = as.character(zip)) %>%
  left_join(zip_contrib, by = "zip")

ggplot(zip_map) +
  geom_sf(aes(fill = total_excess), color = NA) +
  scale_fill_viridis_c(
    name = "Total excess deaths\n(day −1)",
    na.value = "grey85"
  ) +
  labs(
    title = "Spatial distribution of pre-impact mortality burden",
    subtitle = "Expected excess deaths attributable to day −1 threat conditions"
  ) +
  theme_minimal()
```

## Average pre- and post-storm effects

```{r}
library(fixest)

avg_lags <- function(m, days, prefix = "rel_day_f::", include_missing_as_zero = TRUE){
  beta <- coef(m)
  V    <- vcov(m)

  wanted_terms <- paste0(prefix, days)
  present <- wanted_terms[wanted_terms %in% names(beta)]
  missing <- setdiff(wanted_terms, present)

  if(length(present) == 0){
    stop("None of the requested terms are in the model. Check coef(m) names and your prefix/days.")
  }

  # weights for the average
  denom <- if(include_missing_as_zero) length(wanted_terms) else length(present)
  w_present <- rep(1/denom, length(present))

  # estimate (log scale)
  est <- sum(w_present * beta[present])

  # variance: w' V w (only over present coefficients)
  Vpp <- V[present, present, drop = FALSE]
  var <- as.numeric(t(w_present) %*% Vpp %*% w_present)
  se  <- sqrt(var)

  # 95% CI on log scale
  lo <- est - 1.96 * se
  hi <- est + 1.96 * se

  out <- data.frame(
    days_min = min(days),
    days_max = max(days),
    n_days_requested = length(days),
    n_terms_present = length(present),
    n_terms_missing = length(missing),
    include_missing_as_zero = include_missing_as_zero,
    logRR = est,
    SE = se,
    logRR_L = lo,
    logRR_U = hi,
    RR = exp(est),
    RR_L = exp(lo),
    RR_U = exp(hi)
  )

  attr(out, "present_terms") <- present
  attr(out, "missing_terms") <- missing
  out
}

# Define windows (adjust as you like)
pre_days  <- -8:-1
post_days <-  1:6

pre_avg  <- avg_lags(m_event, pre_days,  include_missing_as_zero = TRUE)
post_avg <- avg_lags(m_event, post_days, include_missing_as_zero = TRUE)

pre_avg
post_avg

# If you want to see which terms were missing (often the reference day):
attr(pre_avg, "missing_terms")
attr(post_avg, "missing_terms")
```


## zip level effects on day -1

```{r}
library(data.table)
library(fixest)

setDT(dat)
dat[, ym := as.factor(ym)]
fixest::setFixest_notes(FALSE)

# For fast subsetting by zip
setkey(dat, zip)

fit_one_zip_lag1 <- function(dz) {
  if (nrow(dz) < 50) return(NULL)
  if (all(dz$deaths == 0L)) return(NULL)

  # must have both None and -1 days in this ZIP
  levs <- dz[, unique(as.character(rel_day_f))]
  if (!("None" %in% levs) || !("-1" %in% levs)) return(NULL)
  if (length(levs) < 2) return(NULL)

  m <- tryCatch(
    fepois(deaths ~ i(rel_day_f, ref = "None") | ym + dow,
           data = dz, notes = FALSE),
    error = function(e) NULL
  )
  if (is.null(m)) return(NULL)

  term <- "rel_day_f::-1"
  cf <- coef(m)
  if (!(term %in% names(cf))) return(NULL)

  vc <- tryCatch(vcov(m), error = function(e) NULL)
  if (is.null(vc) || !(term %in% rownames(vc))) return(NULL)

  beta <- unname(cf[term])
  se   <- sqrt(vc[term, term])

  data.table(
    zip  = dz$zip[1],
    beta = beta,                 # log RR
    se   = as.numeric(se),
    RR   = exp(beta),
    LCI  = exp(beta - 1.96 * se),
    UCI  = exp(beta + 1.96 * se)
  )
}

zips <- dat[, unique(zip)]

start <- Sys.time()
zip_lag1_raw <- rbindlist(
  lapply(zips, function(z) fit_one_zip_lag1(dat[J(z)])),
  fill = TRUE
)
Sys.time() - start

zip_lag1_raw[]
```

## Between zip variance (check for spatial heteogeneity)

Estimate statewide prior mean for day -1
```{r}
library(data.table)
library(fixest)

setDT(dat)
dat[, ym := as.factor(ym)]
fixest::setFixest_notes(FALSE)

m_pooled_event <- fepois(
  deaths ~ i(rel_day_f, ref = "None") | zip + ym + dow,
  data    = dat,
  cluster = ~ zip + ym,
  notes   = FALSE
)

mu <- as.numeric(coef(m_pooled_event)["rel_day_f::-1"])  # statewide log RR for day -1
mu
```

Estimate between-zip variance (heterogeneity)
```{r}
library(data.table)

setDT(zip_lag1_raw)

# Ensure numeric
zip_lag1_raw[, beta := as.numeric(beta)]
zip_lag1_raw[, se   := as.numeric(se)]

# Within-ZIP variance
zip_lag1_raw[, v := se^2]

# Choose a prior mean (mu0)
# Option A: statewide pooled estimate for day -1 (recommended if you have it)
# mu0 <- as.numeric(coef(m_event)["rel_day_f::-1"])

# Option B: empirical mean of the ZIP estimates (works even without pooled model)
mu0 <- zip_lag1_raw[is.finite(beta) & is.finite(v), mean(beta)]

# Method-of-moments tau^2 for random effects:
# Var(beta) = tau^2 + mean(v)  => tau^2 ≈ Var(beta) - mean(v)
S2   <- zip_lag1_raw[is.finite(beta) & is.finite(v), var(beta)]
vbar <- zip_lag1_raw[is.finite(beta) & is.finite(v), mean(v)]
tau2 <- max(0, S2 - vbar)

tau2
```

tau2 is zero 
