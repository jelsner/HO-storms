---
title: "Daily death counts prior to storm impact--sensitivity"
output: html_document
editor_options:
  chunk_output_type: console
---

## Create an empirical storm threat model
```{r}
library(sf)
library(dplyr)

storm_intensity <- 64
begin_year <- 1985 
end_year <- 2022
begin_date <- paste0(begin_year, "-01-01")
end_date <- paste0(end_year, "-12-31")

L <- "https://www.ncei.noaa.gov/data/international-best-track-archive-for-climate-stewardship-ibtracs/v04r00/access/shapefile/IBTrACS.NA.list.v04r00.lines.zip"
  
if(!"IBTrACS.NA.list.v04r00.lines.zip" %in% list.files(here::here("data"))) {
download.file(url = L,
              destfile = here::here("data",
                                    "IBTrACS.NA.list.v04r00.lines.zip"))
unzip(here::here("data", "IBTrACS.NA.list.v04r00.lines.zip"),
      exdir = here::here("data"))
}

Tracks.sf <- st_read(dsn = here::here("data"), 
                     layer = "IBTrACS.NA.list.v04r00.lines") |>
  st_transform(crs = 32616)

Tracks.sf <- Tracks.sf |>
  filter(year >= begin_year & year <= end_year) |>
  filter(USA_WIND >= storm_intensity) |>
  select(SID, SEASON, year, month, day, hour, min,
         NAME, SUBBASIN, ISO_TIME, USA_WIND, USA_PRES, USA_RMW, USA_EYE, USA_ROCI,
         USA_R50_NE, USA_R50_NW, USA_R50_SE, USA_R50_SW)

Tracks.sf |>
  st_drop_geometry() |>
  summarize(avgRMW = mean(USA_RMW, na.rm = TRUE) * 1.852,
            avgEYE = mean(USA_EYE, na.rm = TRUE) * 1.852,
            avgROCI = mean(USA_ROCI, na.rm = TRUE) * 1.852,
            avgR50_NE = mean(USA_R50_NE, na.rm = TRUE) * 1.852,
            avgR50_SE = mean(USA_R50_SE, na.rm = TRUE) * 1.852,
            avgR50_SW = mean(USA_R50_SW, na.rm = TRUE) * 1.852,
            avgR50_NW = mean(USA_R50_NW, na.rm = TRUE) * 1.852
            )

# Fill in missing RMW values. Start with pedigree, then use minimum pressure, and finish again with pedigree
Tracks.sf <- Tracks.sf |>
  group_by(SID) |>  # pedigree
  mutate(USA_RMW = ifelse(is.na(USA_RMW), mean(USA_RMW, na.rm = TRUE), USA_RMW))

Tracks.sf <- Tracks.sf |>
  group_by(USA_PRES) |> # minimum pressure
  mutate(USA_RMW = ifelse(is.na(USA_RMW), mean(USA_RMW, na.rm = TRUE), USA_RMW))

Tracks.sf <- Tracks.sf |>
  group_by(SID) |> # pedigree
  mutate(USA_RMW = ifelse(is.na(USA_RMW), mean(USA_RMW, na.rm = TRUE), USA_RMW))

#Add a buffer to the tracks to make segmented wind swaths
Swaths.sf <- Tracks.sf |>
  st_buffer(dist = Tracks.sf$USA_RMW * 1852 * 2.5) # 1852 converts to meters # 2.5xRMW ~ R50

# Wind swaths that cross Florida. `USAboundaries` package no longer maintained on CRAN
#devtools::install_github("ropensci/USAboundariesData")
#devtools::install_github("ropensci/USAboundaries", force = TRUE)
#install.packages("USAboundariesData", repos = "https://ropensci.r-universe.dev", type = "source")
Boundaries.sf <- USAboundaries::us_states(resolution = "low", states = "FL") |> 
  st_transform(crs = 32616)

X <- Swaths.sf |>
  st_intersects(Boundaries.sf, sparse = FALSE) #Does the swath intersect the state border?
Swaths.sf <- Swaths.sf[X, ]
Swaths.sf <- Swaths.sf |>
  mutate(Date = lubridate::as_date(ISO_TIME)) #Add a date column

# Extract the boundaries of storm impacts by storm ID. Dissolve the overlap borders of the individual swaths by storm to create a single storm swath. Add the storm category (Saffir-Simpson) based on wind speed
Swaths.sf <- Swaths.sf |>
  group_by(SID) |>
  summarize(Date0 = first(Date),
            NAME = first(NAME),
            Wind = max(USA_WIND),
            geometry = st_union(geometry)) |>
  mutate(Storm_Category = case_when(
    Wind >= 34 & Wind <= 63 ~ 0,
    Wind >= 64 & Wind <= 82 ~ 1,
    Wind >= 83 & Wind <= 95 ~ 2,
    Wind >= 96 & Wind <= 112 ~ 3,
    Wind >= 113 & Wind <= 136 ~ 4,
    Wind >= 137 ~ 5
  ))

# Transform the geometry to a geographic CRS (4326) and unionize the swaths
sf_use_s2(TRUE)

Swaths.sf <- Swaths.sf |>
    st_transform(crs = 4326)
union_Swaths2.sfg <- Swaths.sf |>
  st_union()
```

```{r}
sum(table(Swaths.sf$Storm_Category))
table(Swaths.sf$Storm_Category)
```

Strafing storms now caught
32
 1  2  3  4  5 
12  8  7  2  3 

```{r}
table(lubridate::year(Swaths.sf$Date0))
```


64 kt
1985 1987 1992 1995 1997 1998 1999 2000 2004 2005 2012 2016 2017 
   3    1    1    2    1    2    1    1    4    4    2    2    1 
2018 2019 2020 2021 2022 
   1    1    2    1    2 

## Expand `Swaths.sf` by adding rows based on the increment value of the attribute `Date0`
```{r}
library(tidyr)
library(lubridate)

min_lag  <- -8   # days before impact
max_lead <- 8   # days after impact

TIC.sf <- Swaths.sf %>%
  rowwise() %>%
  mutate(event_days = list(
    tibble(
      Date     = Date0 + (min_lag:max_lead),
      rel_day  = min_lag:max_lead
    )
  )) %>%
  unnest(event_days) %>%
  ungroup() %>%
  select(
    Date,
    rel_day,
    Storm_Name = NAME,
    Storm_Category
  )

# Create event-time factors
TIC.sf <- TIC.sf %>%
  mutate(
    rel_day_f = factor(rel_day)
  )

# Add month and year indicators change name of the geometry column (this is needed for spatial merges)
TIC.sf <- TIC.sf |>
  mutate(Month = lubridate::month(Date),
         Year = lubridate::year(Date))
storm_months <- unique(TIC.sf$Month)
storm_year_range <- range(TIC.sf$Year)

st_geometry(TIC.sf) <- "geom"
```

## Build a zip x date relative day calendar
~ 25 sec
```{r}
library(data.table)
library(tigris)

options(tigris_use_cache = TRUE)

# --- ZCTAs for Florida (2020) ---
zcta_us <- zctas(cb = TRUE, year = 2020) %>%
  select(zip = ZCTA5CE20, geometry)

fl <- states(cb = TRUE, year = 2020) %>%
  filter(STUSPS == "FL") %>%
  st_transform(st_crs(zcta_us))

zctas_fl <- st_join(zcta_us, fl, join = st_intersects, left = FALSE) %>%
  select(zip, geometry)

# --- Align CRS ---
tic_aligned <- st_transform(TIC.sf, st_crs(zctas_fl))

# --- Keep only what we need from TIC: Date + rel_day (or rel_day_f) + geometry ---
# Prefer storing numeric rel_day and constructing rel_day_f later.
tic_keep <- tic_aligned %>%
  select(Date, rel_day, geom)

# --- Spatial join: which zips intersect a TIC polygon on that date ---
zcta_date_rel_sf <- st_join(
  zctas_fl,
  tic_keep,
  join = st_intersects,
  left = FALSE
)

# --- Drop geometry and convert to data.table ---
zcta_date_rel <- as.data.table(st_drop_geometry(zcta_date_rel_sf))
setnames(zcta_date_rel, c("zip", "Date", "rel_day"), c("zip", "date", "rel_day"))

# --- Normalize types ---
zcta_date_rel[, `:=`(
  zip     = as.character(zip),
  date    = as.IDate(date),
  rel_day = as.integer(rel_day)
)]

# --- Resolve multiple hits per ZIP–date ---
# Rule: choose rel_day closest to 0 (impact). Tie-break: prefer negative (pre-impact).
zcta_date_rel <- zcta_date_rel[
  , .SD[order(abs(rel_day), rel_day)][1],
  by = .(zip, date)
]

# --- Build full ZIP × date grid (choose your desired universe) ---
all_zips <- sort(unique(zctas_fl$zip))
all_dates <- as.IDate(seq(as.Date(begin_date), as.Date(end_date), by = "day"))
grid <- CJ(zip = as.character(all_zips), date = all_dates)

# --- Left-join observed rel_day onto full grid ---
rel_calendar <- zcta_date_rel[grid, on = .(zip, date)]

# --- Construct rel_day_f (factor) with "None" for non-storm days ---
min_lag  <-  -8L
max_lead <-  8L

rel_levels <- c("None", as.character(min_lag:max_lead))

rel_calendar[, rel_day_f := fifelse(is.na(rel_day), "None", as.character(rel_day))]
rel_calendar[, rel_day_f := factor(rel_day_f, levels = rel_levels)]

# --- Sanity checks ---
stopifnot(rel_calendar[, .N, by = .(zip, date)][, all(N == 1)])
rel_calendar[, table(rel_day_f)]
```

1046 zip codes

64kt
rel_day_f
    None       -8       -7       -6       -5       -4       -3 
14427147     5311     5311     5311     5311     5311     5311 
      -2       -1        0        1        2        3        4 
    5311     5311     5311     5311     5311     5311     5311 
       5        6        7        8 
    5311     5311     5311     5311 

## Merge storms with death records
Files received from Jihoon on November 27, 2025 
~ 2 minutes
```{r}
start_time <- Sys.time()
load("data/all_data.Rdata")

#load("data/cardio_subset_all.Rdata")
#load("data/respir_subset_all.Rdata")
#load("data/injury_subset_all.Rdata")
#load("data/neuropsych_subset_all.Rdata")

#Deaths.df <- neuropsych_subset_all %>% # Use for subsets
#  mutate(Date = as_date(DATE_OF_DEATH)) %>%
#  filter(Date >= as.Date(begin_date)) %>%
#  select(Death_ID = ID, Death_Date = Date, final_lon, final_lat)
  
Deaths.df <- all_data %>%   # Use for all deaths
 mutate(Date = as_date(DATE_OF_DEATH)) %>%
  filter(Date >= as.Date(begin_date)) %>% 
  select(Death_ID = ID, Death_Date = Date, final_lon, final_lat, 
         age = COMPUTED_AGE, white = RACE_WHITE, sex = SEX, 
         marital_code = MARITAL_CODE, army = ARMY_YESNO)

Deaths.sf <- Deaths.df %>%
  st_as_sf(coords = c("final_lon", "final_lat"), crs = 4326)
rm(all_data, Deaths.df)
Sys.time() - start_time
```

5,851,050 all deaths

## Filter deaths by date of death and storm effect zones
~ 10 minutes
```{r}
library(sf)
library(dplyr)

start_time <- Sys.time()

target_crs <- 5070
sf_use_s2(FALSE)

# IMPORTANT: set window here
min_lag  <-  -8L
max_lead <-  8L

TIC_proj    <- st_transform(TIC.sf,    target_crs)
Deaths_proj <- st_transform(Deaths.sf, target_crs)

num_obs <- nrow(Deaths_proj)
chunk_size <- 50000
num_chunks <- ceiling(num_obs / chunk_size)
print(paste("Number of chunks to process", num_chunks))

results_list <- vector("list", num_chunks)

for (i in seq(1, num_obs, by = chunk_size)) {
  chunk_index <- (i - 1) / chunk_size + 1
  chunk_indices <- i:min(i + chunk_size - 1, num_obs)

  print(paste("Processing chunk", chunk_index))

  chunk.sf <- Deaths_proj[chunk_indices, ]

  chunk_dates <- unique(chunk.sf$Death_Date)
  TIC_chunk <- TIC_proj %>% filter(Date %in% chunk_dates)

  if (nrow(TIC_chunk) == 0) {
    results_list[[chunk_index]] <- NULL
    gc()
    next
  }

  deaths_joined <- st_join(chunk.sf, TIC_chunk, join = st_within, left = FALSE) %>%
    filter(Date == Death_Date) %>%                 # enforce same-day
    mutate(rel_day = as.integer(rel_day)) %>%
    filter(rel_day >= min_lag, rel_day <= max_lead)

  if (nrow(deaths_joined) == 0) {
    results_list[[chunk_index]] <- NULL
    gc()
    next
  }

  deaths_with_impacts <- deaths_joined %>%
    group_by(Death_ID, Death_Date) %>%
    arrange(abs(rel_day), rel_day) %>%             # tie-break toward pre-impact
    slice(1) %>%
    ungroup() %>%
    transmute(
      Death_ID, Death_Date,
      Storm_Name, Storm_Category,
      rel_day,
      rel_day_f = factor(as.character(rel_day), levels = as.character(min_lag:max_lead)),
#      age, white, sex, marital_code, army, # use for all deaths
      geometry
    ) %>%
    distinct()

  results_list[[chunk_index]] <- deaths_with_impacts
  gc()
}

deaths_with_impacts <- do.call(rbind, results_list)
Sys.time() - start_time
```

## Combine with all deaths

```{r}
impacts_keep <- deaths_with_impacts %>%
  st_drop_geometry() %>%
  select(Death_ID, Death_Date, Storm_Name, Storm_Category, rel_day, rel_day_f)

deaths_all_effects <- Deaths.sf %>%
  left_join(impacts_keep, by = c("Death_ID", "Death_Date")) %>%
  mutate(
    rel_day_f = replace_na(as.character(rel_day_f), "None"),
    rel_day_f = factor(rel_day_f, levels = c("None", as.character(min_lag:max_lead))),
    rel_day   = if_else(rel_day_f == "None", NA_integer_, as.integer(rel_day))
  )
```

## Analytics of deaths and storms

Compute statewide death rates
```{r}
# Death totals by rel_day_f
Deaths.df <- deaths_all_effects %>% st_drop_geometry()

total_deaths <- Deaths.df %>%
  mutate(rel_day_f = as.character(rel_day_f)) %>%
  group_by(rel_day_f) %>%
  summarise(total_deaths = n(), .groups = "drop")

# Statewide calendar: one rel_day per day
state_calendar <- TIC.sf %>%
  st_drop_geometry() %>%
  transmute(Date = as.Date(Date), rel_day = as.integer(rel_day)) %>%
  filter(rel_day >= min_lag, rel_day <= max_lead) %>%
  group_by(Date) %>%
  arrange(abs(rel_day), rel_day) %>%   # pre-impact wins ties
  slice(1) %>%
  ungroup() %>%
  transmute(
    date = Date,
    rel_day_f = as.character(rel_day)
  )

# Full calendar over the study period (use deaths date range as the master range)
all_dates <- tibble(
  date = seq(min(Deaths.df$Death_Date, na.rm = TRUE),
             max(Deaths.df$Death_Date, na.rm = TRUE),
             by = "day")
)

state_calendar_full <- all_dates %>%
  left_join(state_calendar, by = "date") %>%
  mutate(rel_day_f = replace_na(rel_day_f, "None"))

# Count calendar-days in each rel_day_f category
num_days <- state_calendar_full %>%
  count(rel_day_f, name = "num_days")

# Calendar-day death rates by rel_day
death_rates <- total_deaths %>%
  full_join(num_days, by = "rel_day_f") %>%
  mutate(
    total_deaths = replace_na(total_deaths, 0L),
    daily_death_rate = total_deaths / num_days
  ) %>%
  mutate(rel_day_f = factor(rel_day_f, levels = c("None", as.character(min_lag:max_lead)))) %>%
  arrange(rel_day_f)

print(n = 35, death_rates)
```

  rel_day_f total_deaths num_days daily_death_rate
   <fct>            <int>    <int>            <dbl>
 1 None           5812517    13348            435. 
 2 -8                2233       30             74.4
 3 -7                2242       30             74.7
 4 -6                2158       31             69.6
 5 -5                2203       31             71.1
 6 -4                2383       32             74.5
 7 -3                2178       32             68.1
 8 -2                2217       32             69.3
 9 -1                2244       32             70.1
10 0                 2231       32             69.7
11 1                 2401       32             75.0
12 2                 2355       32             73.6
13 3                 2289       32             71.5
14 4                 2187       32             68.3
15 5                 2343       31             75.6
16 6                 2262       30             75.4
17 7                 2267       30             75.6
18 8                 2340       30             78


## Add zip codes to each death

Spatial join deaths to zips
~ 1.3 hours
```{r}
begin <- Sys.time()

target_crs <- 5070
sf_use_s2(FALSE)

zctas_fl_proj <- st_transform(zctas_fl, target_crs)
Deaths_proj <- st_transform(deaths_all_effects, target_crs)

deaths_with_zip <- st_join(
  Deaths_proj,
  zctas_fl_proj,
  join    = st_intersects,
  left    = TRUE,
  largest = TRUE
)

deaths_with_zip <- st_transform(deaths_with_zip, 4326)

# How many deaths didn't land in any ZCTA?
sum(is.na(deaths_with_zip$zip))

# Drop if desired
deaths_with_zip <- deaths_with_zip %>%
  filter(!is.na(zip))

Sys.time() - begin
```

How many deaths didn't land in any ZCTA?
sum(is.na(deaths_with_zip$zip))
[1] 9478

Save
```{r}
saveRDS(deaths_with_zip, "data/outputs/Results/deaths_with_zip_34kt.rds")
deaths_with_zip<- readRDS("data/outputs/Results/deaths_with_zip_64kt.rds")
```

## Panel-ready daily counts

One row per zip-date
~ 20 sec
```{r}
begin <- Sys.time()

# 1) deaths per ZIP–date (unchanged)
daily_deaths <- deaths_with_zip %>%
  st_drop_geometry() %>%
  transmute(
    zip  = as.character(zip),
    date = as.IDate(Death_Date),
#    age = as.integer(age),
#    sex = sex,
#    white = white,
#    marital_code = marital_code
  ) %>%
# add subgroup filters here if needed
#  filter(age >= 60) %>%
#  filter(sex == "M") %>%
#  filter(white == "Y") %>%
#  filter(marital_code == 2) %>%
  group_by(zip, date) %>%
  summarise(deaths = n(), .groups = "drop")

# 2) Join to ZIP×date exposure calendar (NO de-overlap)
library(data.table)

dat <- as.data.table(daily_deaths)
cal <- as.data.table(rel_calendar)

# Ensure consistent types
dat[, `:=`(zip = as.character(zip), date = as.IDate(date))]
cal[, `:=`(zip = as.character(zip), date = as.IDate(date))]

# Left join calendar -> deaths (keeps all ZIP-days present in calendar)
panel <- dat[cal, on = .(zip, date)]

# Fill no-death days with 0
panel[is.na(deaths), deaths := 0L]

# 3) Ensure rel_day_f factor levels are correct
#     (uses min_lag and max_lead already defined in your environment)
panel[, rel_day_f := factor(as.character(rel_day_f),
                            levels = c("None", as.character(min_lag:max_lead)))]

# Sanity check: exactly one row per ZIP–date
stopifnot(panel[, .N, by = .(zip, date)][, all(N == 1)])

Sys.time() - begin
```

Export (optional)
```{r}
readr::write_csv(panel, "data/outputs/Deaths/daily_panel_34kt.csv")
panel <- readr::read_csv("data/outputs/Deaths/daily_panel_55kt.csv")
```

## Poisson model with stratum fixed effects

Next we fit a Poisson with stratum fixed effects. This is equivalent to conditioning on the stratum totals and largely removes serial confounding. Use robust (clustered) SEs by stratum to protect against residual correlation within a stratum

Add strata variables. Include only months during the hurricane season (May-November)
~ 3.5 minutes
```{r}
library(fixest)
library(broom)
library(lubridate)
library(data.table)
library(dplyr)

begin <- Sys.time()

# zip_day must already have one row per zip-date: zip, date, deaths, rel_day_f
dat <- panel %>%
  filter(lubridate::month(as.Date(date)) %in% 5:11) %>%  # hurricane season only
#  filter(lubridate::year(as.Date(date)) < 2020) %>% # remove COVID years
  transmute(
    zip       = as.character(zip),
    date      = as.IDate(date),
    deaths    = as.integer(deaths),
    rel_day_f = factor(as.character(rel_day_f),
                       levels = c("None", as.character(min_lag:max_lead))),
    dow       = factor(strftime(as.Date(date), "%u"),
                       levels = as.character(1:7),
                       labels = c("Mon","Tue","Wed","Thu","Fri","Sat","Sun")),
    ym        = factor(format(as.Date(date), "%Y-%m")),
    yw        = interaction(lubridate::year(as.Date(date)),
                            lubridate::isoweek(as.Date(date)), drop = TRUE),
    covid     = as.integer(date >= as.IDate("2020-01-01") & date <= as.IDate("2022-12-31"))
  ) %>%
  mutate(
    # Interaction FE for ZIP-specific COVID shift (absorbed efficiently in FE part)
    zip_covid = interaction(zip, covid, drop = TRUE),
    zip       = factor(zip)  # optional but helps compact FE
  )

# Event-time model with ZIP-specific COVID shift **as a fixed effect** and year-week stratum
m_event <- fepois(
  deaths ~ i(rel_day_f, ref = "None") | zip + yw + dow + zip_covid,
  data    = dat,
  cluster = ~ zip + yw,
  notes   = TRUE
)

summary(m_event)
Sys.time() - begin
```

```{r}
saveRDS(m_event, file = "m_event_34kt.rds")
```

## Read in model results
```{r}
m_event_34kt <- readRDS("m_event_34kt.rds")
m_event_34kt_placebo <- readRDS("m_event_34kt_placebo.rds")
m_event_55kt <- readRDS("m_event_55kt.rds")
m_event_55kt_placebo <- readRDS("m_event_55kt_placebo.rds")
m_event_64kt <- readRDS("m_event_64kt.rds")
m_event_64kt_placebo <- readRDS("m_event_64kt_placebo.rds")
```

## Profile plot across days
```{r}
library(ggplot2)
library(broom)

plot_rr_event <- function(model, prefix = "rel_day_f::") {

  rr_df <- tidy(model, conf.int = TRUE) %>%
    # keep only lag/lead terms
    filter(startsWith(term, prefix)) %>%
    mutate(
      rel_day = as.integer(sub(prefix, "", term)),
      RR  = exp(estimate),
      LCI = exp(conf.low),
      UCI = exp(conf.high)
    ) %>%
    arrange(rel_day)

  ggplot(rr_df, aes(x = rel_day, y = RR)) +
    geom_hline(yintercept = 1, linetype = "dashed") +
    geom_ribbon(aes(ymin = LCI, ymax = UCI), fill = "grey80", alpha = 0.7) +
    geom_line(linewidth = 0.9) +
    geom_point(size = 2) +
    scale_x_continuous(breaks = sort(unique(rr_df$rel_day))) +
    labs(
      x = "Days relative to impact (lag/lead)",
      y = "Rate ratio (RR)",
      title = "RR by lag/lead day with 95% CI"
    ) +
    theme_minimal()
}

# run it
plot_rr_event(m_event)
```

## Daily average pre- and post-storm effects across lead/lag days
```{r}
library(fixest)
library(dplyr)
library(ggplot2)

# --- Averaging function (as given) ---
avg_lags <- function(m, days, prefix = "rel_day_f::", include_missing_as_zero = TRUE){
  beta <- coef(m)
  V    <- vcov(m)

  wanted_terms <- paste0(prefix, days)
  present <- wanted_terms[wanted_terms %in% names(beta)]
  missing <- setdiff(wanted_terms, present)

  if(length(present) == 0){
    stop("None of the requested terms are in the model. Check coef(m) names and your prefix/days.")
  }

  # weights for the average
  denom <- if(include_missing_as_zero) length(wanted_terms) else length(present)
  w_present <- rep(1/denom, length(present))

  # estimate (log scale)
  est <- sum(w_present * beta[present])

  # variance: w' V w (only over present coefficients)
  Vpp <- V[present, present, drop = FALSE]
  var <- as.numeric(t(w_present) %*% Vpp %*% w_present)
  se  <- sqrt(var)

  # 95% CI on log scale
  lo <- est - 1.96 * se
  hi <- est + 1.96 * se

  out <- data.frame(
    days_min = min(days),
    days_max = max(days),
    n_days_requested = length(days),
    n_terms_present = length(present),
    n_terms_missing = length(missing),
    include_missing_as_zero = include_missing_as_zero,
    logRR = est,
    SE = se,
    logRR_L = lo,
    logRR_U = hi,
    RR = exp(est),
    RR_L = exp(lo),
    RR_U = exp(hi)
  )

  attr(out, "present_terms") <- present
  attr(out, "missing_terms") <- missing
  out
}

# --- Helper: summarize pre/post for one model ---
summarize_prepost <- function(model, model_label,
                              pre_days, post_days,
                              prefix = "rel_day_f::",
                              include_missing_as_zero = TRUE) {
  pre  <- avg_lags(model, pre_days,
                   prefix = prefix,
                   include_missing_as_zero = include_missing_as_zero) |>
    mutate(period = "Pre",  model = model_label)

  post <- avg_lags(model, post_days,
                   prefix = prefix,
                   include_missing_as_zero = include_missing_as_zero) |>
    mutate(period = "Post", model = model_label)

  bind_rows(pre, post) |>
    mutate(period = factor(period, levels = c("Pre", "Post")))
}

# --- Windows ---
pre_days  <- -8:-1
post_days <-  1:8
coef_prefix <- "rel_day_f::"

# --- Compute averages for Main only (remove placebo) ---
res_main <- summarize_prepost(m_event_64kt, "Main (64kt)", pre_days, post_days)

# --- Extract per-day estimates (assumes all terms present) ---
extract_days <- function(m, period_label, days, prefix = "rel_day_f::") {
  beta <- coef(m)
  tibble(
    day   = days,
    term  = paste0(prefix, day),
    logRR = unname(beta[term]),
    RR    = exp(logRR),
    period = factor(period_label, levels = c("Pre", "Post"))
  )
}

res_days <- bind_rows(
  extract_days(m_event_64kt, "Pre",  pre_days,  prefix = coef_prefix),
  extract_days(m_event_64kt, "Post", post_days, prefix = coef_prefix)
)

# --- Encode darkness via alpha: darker near -1/+1, lighter at -8/+8 ---
# Pre: -1 darkest (alpha=1), -8 lightest (alpha_min)
# Post: +1 darkest (alpha=1), +8 lightest (alpha_min)
alpha_min <- 0.4
alpha_range <- 1 - alpha_min

res_days <- res_days |>
  mutate(
    alpha = case_when(
      period == "Pre"  ~ alpha_min + alpha_range * (1 - (abs(day) - 1) / (max(abs(pre_days)) - 1)),  # -1 -> 1, -8 -> 0.4
      period == "Post" ~ alpha_min + alpha_range * (1 - (day - 1) / (max(post_days) - 1)),          # +1 -> 1, +8 -> 0.4
      TRUE ~ 1
    )
  )

# --- Plot: averages + CIs + per-day jittered dots (one model) ---
set.seed(2026)  # reproducible jitter

p <- ggplot() +
  geom_hline(yintercept = 1, linetype = "dashed") +

  # Per-day dots for PRE (fixed color, alpha encodes proximity to -1)
  geom_point(
    data = res_days |> filter(period == "Pre"),
    aes(x = period, y = RR),
    position = position_jitter(width = 0.12, height = 0),
    size = 1.8,
    color = "grey10",
    alpha = res_days |> filter(period == "Pre") |> pull(alpha),
    show.legend = FALSE
  ) +

  # Per-day dots for POST (fixed color, alpha encodes proximity to +1; darkest at +1)
  geom_point(
    data = res_days |> filter(period == "Post"),
    aes(x = period, y = RR),
    position = position_jitter(width = 0.15, height = 0),
    size = 1.8,
    color = "grey10",
    alpha = res_days |> filter(period == "Post") |> pull(alpha),
    show.legend = FALSE
  ) +

  # Average ±95% CI (no dodge needed—single model)
  geom_errorbar(
    data = res_main,
    aes(x = period, ymin = RR_L, ymax = RR_U),
    width = 0.12, linewidth = 0.8, color = "black"
  ) +
  geom_point(
    data = res_main,
    aes(x = period, y = RR),
    size = 3, color = "black"
  ) +

  labs(
    x = "Window",
    y = "Rate ratio (RR)",
    title = "Average pre/post RR with daily coefficients (Main 64 kt only)",
    subtitle = "Daily dots jittered; darker = closer to impact day (−1 for Pre, +1 for Post)"
  ) +
  theme_minimal()

print(p)

# --- Optional: log scale for RRs (often clearer) ---
# p + scale_y_log10()
```

## Compare 34kt, 55kt, and 64kt model results
```{r}
library(fixest)
library(dplyr)
library(ggplot2)

# --- Your averaging function (unchanged) ---
avg_lags <- function(m, days, prefix = "rel_day_f::", include_missing_as_zero = TRUE){
  beta <- coef(m)
  V    <- vcov(m)

  wanted_terms <- paste0(prefix, days)
  present <- wanted_terms[wanted_terms %in% names(beta)]

  if(length(present) == 0){
    stop("None of the requested terms are in the model. Check coef(m) names and your prefix/days.")
  }

  # weights for the average
  denom <- if(include_missing_as_zero) length(wanted_terms) else length(present)
  w_present <- rep(1/denom, length(present))

  # estimate (log scale)
  est <- sum(w_present * beta[present])

  # variance: w' V w (only over present coefficients)
  Vpp <- V[present, present, drop = FALSE]
  var <- as.numeric(t(w_present) %*% Vpp %*% w_present)
  se  <- sqrt(var)

  # 95% CI on log scale
  lo <- est - 1.96 * se
  hi <- est + 1.96 * se

  out <- data.frame(
    days_min = min(days),
    days_max = max(days),
    n_days_requested = length(days),
    n_terms_present = length(present),
    n_terms_missing = length(setdiff(wanted_terms, present)),
    include_missing_as_zero = include_missing_as_zero,
    logRR = est,
    SE = se,
    logRR_L = lo,
    logRR_U = hi,
    RR = exp(est),
    RR_L = exp(lo),
    RR_U = exp(hi)
  )

  attr(out, "present_terms") <- present
  out
}

# --- Helper: summarize PRE-only for one model ---
summarize_pre_only <- function(model, label,
                               pre_days,
                               prefix = "rel_day_f::",
                               include_missing_as_zero = TRUE) {
  avg_lags(model, pre_days,
           prefix = prefix,
           include_missing_as_zero = include_missing_as_zero) |>
    mutate(group = label)
}

# --- Define the pre-impact window (adjust as needed) ---
pre_days <- -8:-1
coef_prefix <- "rel_day_f::"

# --- Compute pre-impact averages for 34kt, 55kt, 64kt ---
res_pre <- bind_rows(
  summarize_pre_only(m_event_34kt, "≥34 kt", pre_days),
  summarize_pre_only(m_event_55kt, "≥55 kt", pre_days),
  summarize_pre_only(m_event_64kt, "≥64 kt", pre_days)
) |>
  # Ensure left-to-right order: 34 -> 55 -> 64
  mutate(group = factor(group, levels = c("≥34 kt", "≥55 kt", "≥64 kt")))

# --- NEW: extract per-day estimates (all days present) ---
extract_day_estimates <- function(m, label, days, prefix = "rel_day_f::") {
  beta <- coef(m)
  tibble(
    day = days,
    term = paste0(prefix, day),
    logRR = unname(beta[term]),
    RR = exp(logRR)
  ) |>
    mutate(group = label)
}

res_days <- bind_rows(
  extract_day_estimates(m_event_34kt, "≥34 kt", pre_days, prefix = coef_prefix),
  extract_day_estimates(m_event_55kt, "≥55 kt", pre_days, prefix = coef_prefix),
  extract_day_estimates(m_event_64kt, "≥64 kt", pre_days, prefix = coef_prefix)
) |>
  mutate(
    group = factor(group, levels = c("≥34 kt", "≥55 kt", "≥64 kt")),
    # Map day to a 0..1 scale so -8 is light, -1 is dark
    day_rank = (day - min(pre_days)) / (max(pre_days) - min(pre_days))
  )

# --- Plot: averages + CIs + per-day jittered points ---
set.seed(2026)  # reproducible jitter

p <- ggplot() +
  geom_hline(yintercept = 1, linetype = "dashed") +

  # Per-day dots (draw first so CI/average sit on top)
  geom_point(
    data = res_days,
    aes(x = group, y = RR, color = day),
    position = position_jitter(width = 0.15, height = 0),  # small horizontal jitter
    size = 1.8,
    alpha = 0.6,
    show.legend = FALSE
  ) +

  # Average CI bars and points (your original layers)
  geom_errorbar(
    data = res_pre,
    aes(x = group, ymin = RR_L, ymax = RR_U),
    width = 0.12,
    linewidth = 0.8,
    color = "black"
  ) +
  geom_point(
    data = res_pre,
    aes(x = group, y = RR),
    size = 3,
    color = "black"
  ) +

  # Greyscale: lighter for -8, darker for -1
  scale_color_gradient(low = "grey75", high = "grey10") +

  labs(
    x = "Wind speed exceedance",
    y = "Pre-impact average RR (95% CI)",
    title = "Pre-impact average rate ratios by wind speed threshold",
    subtitle = "Points show daily coefficients (−8…−1); darker = closer to impact day (−1)"
  ) +
  theme_minimal()

print(p)

# --- Optional: log scale for RRs (often clearer) ---
# p + scale_y_log10()
```

## Compare all deaths with subgroups
```{r}
# --- Compute pre-impact average (lag -8:-1) for All vs Age ≥65 at ≥34 kt ---
res_pre_34kt_groups <- bind_rows(
  summarize_pre_only(m_event_34kt,      "All ages", pre_days),
  summarize_pre_only(m_event_34kt_age,  "Age ≥ 65", pre_days)
) |>
  mutate(group = factor(group, levels = c("All ages", "Age ≥ 65")))

# Inspect (optional)
res_pre_34kt_groups
# attributes(res_pre_34kt_groups)$present_terms  # to see which day terms were present
# attributes(res_pre_34kt_groups)$missing_terms  # to see which were missing

# --- Plot: two error bars (hash marks) with points ---
p_subgroup <- ggplot(res_pre_34kt_groups, aes(x = group, y = RR)) +
  geom_hline(yintercept = 1, linetype = "dashed") +
  geom_errorbar(aes(ymin = RR_L, ymax = RR_U), width = 0.12, linewidth = 0.8) +
  geom_point(size = 3) +
  labs(
    x = "Population subgroup",
    y = "Pre-impact average RR (95% CI)",
    title = "Pre-impact average rate ratio for ≥34 kt: All ages vs Age ≥ 65",
    subtitle = "Window: rel_day_f −8 to −1; missing day terms counted as zero effect"
  ) +
  theme_minimal()

print(p_subgroup)

# --- Optional: log scale for RRs (often clearer) ---
# p_subgroup + scale_y_log10()
```




## Expected excess deaths spatially

```{r}
beta_lag1 <- coef(m_event)["rel_day_f::-1"]
RR_lag1   <- exp(beta_lag1)

beta_lag1
RR_lag1

# RR_lag1 ≈ multiplicative increase in mortality on day −1 vs None.
```

This is the key idea: We use the fitted model to predict: 
1. expected deaths if the day were None
2. expected deaths if the day were −1
The difference is the modeled excess

```{r}
library(data.table)
setDT(dat)

dat_none <- copy(dat)
dat_none[, rel_day_f := factor("None", levels = levels(dat$rel_day_f))]

dat_lag1 <- copy(dat)
dat_lag1[, rel_day_f := factor("-1", levels = levels(dat$rel_day_f))]

mu_none <- predict(m_event, newdata = dat_none, type = "response")
mu_lag1 <- predict(m_event, newdata = dat_lag1, type = "response")

dat[, `:=`(
  mu_none = mu_none,
  mu_lag1 = mu_lag1,
  excess_lag1 = mu_lag1 - mu_none
)]

zip_contrib <- dat[, .(
  mean_baseline = mean(mu_none, na.rm = TRUE),
  mean_excess   = mean(excess_lag1, na.rm = TRUE),
  total_excess  = sum(excess_lag1, na.rm = TRUE),
  n_lag1_days   = sum(rel_day_f == "-1")
), by = zip]
```

```{r}
library(sf)
library(dplyr)
library(tigris)

options(tigris_use_cache = TRUE)
sf_use_s2(FALSE)  # robust for polygon ops; switch back to TRUE later if you prefer

# 1) Florida state boundary (same year as your ZCTAs to align geometry as much as possible)
fl <- states(cb = TRUE, year = 2020) %>%
  filter(STUSPS == "FL") %>%
  st_transform(st_crs(zctas_fl)) %>%
  st_make_valid()

# 2) Ensure ZCTAs are valid and clipped to Florida
zctas_fl_clip <- zctas_fl %>%
  st_make_valid() %>%
  st_intersection(fl)   # clip to the Florida boundary

# 3) Compute "non-ZCTA within Florida" grey polygons
fl_union <- st_union(fl)                      # single-part Florida
zcta_union <- st_union(zctas_fl_clip)         # union of all ZIPs (as one geometry)
non_zcta_within_fl <- st_make_valid(st_difference(fl_union, zcta_union))

zip_map <- zctas_fl_clip %>%
  mutate(zip = as.character(zip)) %>%
  left_join(zip_contrib, by = "zip")

library(ggplot2)

ggplot() +
  # A) Grey fill for areas of Florida not covered by any ZCTA polygon
  geom_sf(data = non_zcta_within_fl, fill = "grey90", color = NA) +

  # B) ZIP polygons colored by modeled total excess deaths on day -1
  geom_sf(data = zip_map, aes(fill = total_excess), color = NA) +

  # C) Florida boundary to crisply clip the edges
  geom_sf(data = st_boundary(fl_union), color = "white", linewidth = 0.3) +
  scale_fill_viridis_c(trans = "sqrt", breaks = scales::pretty_breaks(6),
    name    = "Total excess deaths\n(day −1)",
    na.value = "grey70"
    ) +
  coord_sf(expand = 0) +
  labs(
    title    = "Spatial distribution of pre-impact mortality burden",
    subtitle = "Modeled excess deaths relative to 'None' days (May–Nov)"
  ) +
  theme_void(base_size = 11) +
  theme(
    legend.position = "right",
    plot.title = element_text(face = "bold")
  )
```

## Average pre- and post-storm effects

```{r}
library(fixest)

avg_lags <- function(m, days, prefix = "rel_day_f::", include_missing_as_zero = TRUE){
  beta <- coef(m)
  V    <- vcov(m)

  wanted_terms <- paste0(prefix, days)
  present <- wanted_terms[wanted_terms %in% names(beta)]
  missing <- setdiff(wanted_terms, present)

  if(length(present) == 0){
    stop("None of the requested terms are in the model. Check coef(m) names and your prefix/days.")
  }

  # weights for the average
  denom <- if(include_missing_as_zero) length(wanted_terms) else length(present)
  w_present <- rep(1/denom, length(present))

  # estimate (log scale)
  est <- sum(w_present * beta[present])

  # variance: w' V w (only over present coefficients)
  Vpp <- V[present, present, drop = FALSE]
  var <- as.numeric(t(w_present) %*% Vpp %*% w_present)
  se  <- sqrt(var)

  # 95% CI on log scale
  lo <- est - 1.96 * se
  hi <- est + 1.96 * se

  out <- data.frame(
    days_min = min(days),
    days_max = max(days),
    n_days_requested = length(days),
    n_terms_present = length(present),
    n_terms_missing = length(missing),
    include_missing_as_zero = include_missing_as_zero,
    logRR = est,
    SE = se,
    logRR_L = lo,
    logRR_U = hi,
    RR = exp(est),
    RR_L = exp(lo),
    RR_U = exp(hi)
  )

  attr(out, "present_terms") <- present
  attr(out, "missing_terms") <- missing
  out
}

# Define windows (adjust as you like)
pre_days  <- -8:-1
post_days <-  1:6

pre_avg  <- avg_lags(m_event, pre_days,  include_missing_as_zero = TRUE)
post_avg <- avg_lags(m_event, post_days, include_missing_as_zero = TRUE)

pre_avg
post_avg

# If you want to see which terms were missing (often the reference day):
attr(pre_avg, "missing_terms")
attr(post_avg, "missing_terms")
```


## zip level effects on day -1

```{r}
library(data.table)
library(fixest)

setDT(dat)
dat[, ym := as.factor(ym)]
fixest::setFixest_notes(FALSE)

# For fast subsetting by zip
setkey(dat, zip)

fit_one_zip_lag1 <- function(dz) {
  if (nrow(dz) < 50) return(NULL)
  if (all(dz$deaths == 0L)) return(NULL)

  # must have both None and -1 days in this ZIP
  levs <- dz[, unique(as.character(rel_day_f))]
  if (!("None" %in% levs) || !("-1" %in% levs)) return(NULL)
  if (length(levs) < 2) return(NULL)

  m <- tryCatch(
    fepois(deaths ~ i(rel_day_f, ref = "None") | ym + dow,
           data = dz, notes = FALSE),
    error = function(e) NULL
  )
  if (is.null(m)) return(NULL)

  term <- "rel_day_f::-1"
  cf <- coef(m)
  if (!(term %in% names(cf))) return(NULL)

  vc <- tryCatch(vcov(m), error = function(e) NULL)
  if (is.null(vc) || !(term %in% rownames(vc))) return(NULL)

  beta <- unname(cf[term])
  se   <- sqrt(vc[term, term])

  data.table(
    zip  = dz$zip[1],
    beta = beta,                 # log RR
    se   = as.numeric(se),
    RR   = exp(beta),
    LCI  = exp(beta - 1.96 * se),
    UCI  = exp(beta + 1.96 * se)
  )
}

zips <- dat[, unique(zip)]

start <- Sys.time()
zip_lag1_raw <- rbindlist(
  lapply(zips, function(z) fit_one_zip_lag1(dat[J(z)])),
  fill = TRUE
)
Sys.time() - start

zip_lag1_raw[]
```

## Between zip variance (check for spatial heteogeneity)

Estimate statewide prior mean for day -1
```{r}
library(data.table)
library(fixest)

setDT(dat)
dat[, ym := as.factor(ym)]
fixest::setFixest_notes(FALSE)

m_pooled_event <- fepois(
  deaths ~ i(rel_day_f, ref = "None") | zip + ym + dow,
  data    = dat,
  cluster = ~ zip + ym,
  notes   = FALSE
)

mu <- as.numeric(coef(m_pooled_event)["rel_day_f::-1"])  # statewide log RR for day -1
mu
```

Estimate between-zip variance (heterogeneity)
```{r}
library(data.table)

setDT(zip_lag1_raw)

# Ensure numeric
zip_lag1_raw[, beta := as.numeric(beta)]
zip_lag1_raw[, se   := as.numeric(se)]

# Within-ZIP variance
zip_lag1_raw[, v := se^2]

# Choose a prior mean (mu0)
# Option A: statewide pooled estimate for day -1 (recommended if you have it)
# mu0 <- as.numeric(coef(m_event)["rel_day_f::-1"])

# Option B: empirical mean of the ZIP estimates (works even without pooled model)
mu0 <- zip_lag1_raw[is.finite(beta) & is.finite(v), mean(beta)]

# Method-of-moments tau^2 for random effects:
# Var(beta) = tau^2 + mean(v)  => tau^2 ≈ Var(beta) - mean(v)
S2   <- zip_lag1_raw[is.finite(beta) & is.finite(v), var(beta)]
vbar <- zip_lag1_raw[is.finite(beta) & is.finite(v), mean(v)]
tau2 <- max(0, S2 - vbar)

tau2
```

tau2 is zero 
