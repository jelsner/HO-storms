---
title: "Daily ER visits relative to storm effects"
output: html_document
editor_options:
  chunk_output_type: console
---

## Create an empirical storm effects model

```{r}
library(sf)
library(dplyr)

storm_intensity <- 34 # all tropical storms and hurricanes
begin_year <- 2004
end_year <- 2022

L <- "https://www.ncei.noaa.gov/data/international-best-track-archive-for-climate-stewardship-ibtracs/v04r00/access/shapefile/IBTrACS.NA.list.v04r00.lines.zip"
  
if(!"IBTrACS.NA.list.v04r00.lines.zip" %in% list.files(here::here("data"))) {
download.file(url = L,
              destfile = here::here("data",
                                    "IBTrACS.NA.list.v04r00.lines.zip"))
unzip(here::here("data", "IBTrACS.NA.list.v04r00.lines.zip"),
      exdir = here::here("data"))
}

Tracks.sf <- st_read(dsn = here::here("data"), 
                     layer = "IBTrACS.NA.list.v04r00.lines") |>
  st_transform(crs = 32616)

Tracks.sf <- Tracks.sf |>
  filter(year >= begin_year & year <= end_year) |>
  filter(USA_WIND >= storm_intensity) |>
  select(SID, SEASON, year, month, day, hour, min,
         NAME, SUBBASIN, ISO_TIME, USA_WIND, USA_PRES, USA_RMW, USA_EYE, USA_ROCI)

Tracks.sf |>
  st_drop_geometry() |>
  summarize(avgRMW = mean(USA_RMW, na.rm = TRUE) * 1.852,
                   avgEYE = mean(USA_EYE, na.rm = TRUE) * 1.852,
                   avgROCI = mean(USA_ROCI, na.rm = TRUE) * 1.852)

# Fill in missing RMW values. Start with pedigree, then use minimum pressure, and finish again with pedigree
Tracks.sf <- Tracks.sf |>
  group_by(SID) |>  # pedigree
  mutate(USA_RMW = ifelse(is.na(USA_RMW), mean(USA_RMW, na.rm = TRUE), USA_RMW))

Tracks.sf <- Tracks.sf |>
  group_by(USA_PRES) |> # minimum pressure
  mutate(USA_RMW = ifelse(is.na(USA_RMW), mean(USA_RMW, na.rm = TRUE), USA_RMW))

Tracks.sf <- Tracks.sf |>
  group_by(SID) |> # pedigree
  mutate(USA_RMW = ifelse(is.na(USA_RMW), mean(USA_RMW, na.rm = TRUE), USA_RMW))

#Add a buffer to the tracks to make segmented wind swaths
Swaths.sf <- Tracks.sf |>
  st_buffer(dist = Tracks.sf$USA_RMW * 1852) # 1852 converts to meters

# Wind swaths that cross Florida. `USAboundaries` package no longer maintained on CRAN
#devtools::install_github("ropensci/USAboundariesData")
#devtools::install_github("ropensci/USAboundaries", force = TRUE)
#install.packages("USAboundariesData", repos = "https://ropensci.r-universe.dev", type = "source")
Boundaries.sf <- USAboundaries::us_states(resolution = "low", states = "FL") |> 
  st_transform(crs = 32616)

X <- Swaths.sf |>
  st_intersects(Boundaries.sf, sparse = FALSE) #Does the swath intersect the state border?
Swaths.sf <- Swaths.sf[X, ]
Swaths.sf <- Swaths.sf |>
  mutate(Date = lubridate::as_date(ISO_TIME)) #Add a date column

# Extract the boundaries of storm impacts by storm ID. Dissolve the overlap borders of the individual swaths by storm to create a single storm swath. Add the storm category (Saffir-Simpson) based on wind speed
Swaths.sf <- Swaths.sf |>
  group_by(SID) |>
  summarize(Date0 = first(Date),
            NAME = first(NAME),
            Wind = max(USA_WIND),
            geometry = st_union(geometry)) |>
  mutate(Storm_Category = case_when(
    Wind >= 34 & Wind <= 63 ~ 0,
    Wind >= 64 & Wind <= 82 ~ 1,
    Wind >= 83 & Wind <= 95 ~ 2,
    Wind >= 96 & Wind <= 112 ~ 3,
    Wind >= 113 & Wind <= 136 ~ 4,
    Wind >= 137 ~ 5
  ))

# Transform the geometry to a geographic CRS (4326) and unionize the swaths
sf_use_s2(TRUE)

Swaths.sf <- Swaths.sf |>
    st_transform(crs = 4326)
union_Swaths2.sfg <- Swaths.sf |>
  st_union()
```

Expand `Swaths.sf` by adding rows based on the increment value of the attribute `Date0`. Dates prior to the impact date are threat dates and those after the impact date are cleanup dates
```{r}
library(tidyr)
library(lubridate)

deltaT <- 1   # One day increment
n_new <- 7    # Number of days before & after impact

TIC.sf <- Swaths.sf |>
    rowwise() |>
    mutate(new_features = list(tidyr::tibble(
           Date = Date0 + (-n_new:n_new) * deltaT))) |>
    unnest(new_features) |>
    ungroup() |>
    select(Date, Storm_Name = NAME, Storm_Category)

TIC <- rep(rep(c("Threat", "Impact", "Cleanup"), times = c(n_new, 1, n_new)), 
           times = nrow(Swaths.sf))
TIC.sf$Storm_Effect <- TIC

# Add month and year indicators change name of the geometry column (this is needed for spatial merges)
TIC.sf <- TIC.sf |>
  mutate(Month = month(Date),
         Year = year(Date))
storm_months <- unique(TIC.sf$Month)
storm_year_range <- range(TIC.sf$Year)

st_geometry(TIC.sf) <- "geom"
```

## Build a zip x date effect calendar

```{r}
library(data.table)
library(tigris)
options(tigris_use_cache = TRUE)

# Florida zips
zctas_fl <- zctas(cb = FALSE, year = 2010, state = "FL") %>%
  select(zip = ZCTA5CE10, geometry)

#Align coordinate reference systems
tic_aligned  <- st_transform(TIC.sf,  st_crs(zctas_fl))

# Spatial join: which zips intersect each TIC polygon (by date & effect)
# Keep only the columns we need from TIC
tic_keep <- tic_aligned |> 
  select(Date, Storm_Effect, geom)

# Point-in-polygon for polygons
zcta_date_effect_sf <- st_join(
  zctas_fl,
  tic_keep,
  join = st_intersects,
  left = FALSE   # keep only zips that intersect a TIC polygon on that date
)

# Drop geometry for the calendar
zcta_date_effect <- as.data.table(st_drop_geometry(zcta_date_effect_sf))
setnames(zcta_date_effect, c("zip","Date","Storm_Effect"),
         c("zip","date","effect"))

# Normalize types
zcta_date_effect[, `:=`(
  zip    = as.character(zip),
  date   = as.IDate(date),
  effect = as.character(effect)
)]

# Resolve multiple effects on the same zip-date: Choose most severe ranking (None < Cleanup < Threat < Impact)
sev <- data.table(effect = c("None","Cleanup","Threat","Impact"),
                  rank   = 0:3)
zde <- sev[zcta_date_effect, on = "effect"]

# Keep the single most-severe label per ZIP–date
zde <- zde[ , .SD[which.max(rank)], by = .(zip, date)][ , .(zip, date, effect)]

# Build the full zip x date data frame & fill missing days with "None"
all_zips  <- sort(unique(zde$zip))
all_dates <- as.IDate(seq(as.Date("2004-12-31"), as.Date("2022-12-31"), by = "day"))
grid <- CJ(zip = as.character(all_zips), date = all_dates)

# Left-join the observed effects; fill NA → "None"
effect_calendar <- zde[grid, on = .(zip, date)]
effect_calendar[is.na(effect), effect := "None"]

# Keep effect as a factor with ordered levels
effect_calendar[, effect := factor(effect, levels = c("None","Cleanup","Threat","Impact"))]

# Exactly one effect per ZIP–day?
stopifnot(effect_calendar[, .N, by = .(zip, date)][, all(N == 1)])

# Effects present?
effect_calendar[, table(effect)]
```

## Merge with emergency room visits

Data received from David Hsu on October 16, 2025 via email
```{r}
ER_visits.dt <- data.table::fread(here::here("data", "Combined_ER_Data.csv"))
ER_visits.df <- ER_visits.dt |>
  as.data.frame()
```

Clean the data
```{r}
setDT(ER_visits.df)

# Pad ZipCode to 5 chars; drop non-sensical codes (0/NA/too short/too big)
ER_visits.df[
  , ZipCode := fifelse(ZipCode > 0 & ZipCode < 1e6, sprintf("%05d", ZipCode), NA_character_)
]
ER_visits.df[, Date := as.IDate(Date)]

ER_visits_clean <- ER_visits.df[!is.na(ZipCode) & nchar(ZipCode) == 5]

# Standardize names; optionally collapse duplicates
setnames(ER_visits_clean, c("ZipCode","Date","Total_Visits"),
                           c("zip",    "date","er_visits"))

# If there can be multiple ER rows per (zip,date), aggregate to daily total
ER_visits_clean <- ER_visits_clean[, .(er_visits = sum(er_visits, na.rm = TRUE)),
                                   by = .(zip, date)]

merged <- merge(
  effect_calendar,
  ER_visits_clean,
  by = c("zip", "date"),
  all.x = TRUE  # keep all ZIP–dates from the calendar
)

merged[is.na(er_visits), er_visits := 0L]

library(lubridate)   # for yearmonth(), wday()
setDT(merged)

# Add day of week (factor, ordered Monday–Sunday)
merged[, dow := lubridate::wday(date, label = TRUE, abbr = TRUE)]

# Add year-month (IDate stored as "YYYY-MM-01" for convenience)
merged[, year_month := as.IDate(floor_date(date, unit = "month"))]
```

## Model using logit and Poisson separately

A two-part “threshold (≤6) + truncated count (>6)” approximation (stays in `fixest`). The process is modeled as: Part A (logit): probability the true count is ≤6 (we observe 6) and part B (Poisson on truncated support): conditional mean for counts >6

```{r}
library(fixest)

dat <- merged %>% mutate(
  is_le6 = as.integer(er_visits == 6L)) %>% # proxy for "true ≤ 6"
  filter(month(date) %in% 5:11)  # hurricane season only

# Part A: logistic model for being ≤ 6
mA <- feglm(
  is_le6 ~ i(effect, ref="None") | zip + year_month + dow,
  data = dat, family = binomial("logit"),
  cluster = ~ zip + year_month
)

# Part B: Poisson on Y>6 (treat observed counts >6 as exact)
d_gt6 <- filter(dat, er_visits > 6L)
mB <- fepois(
  er_visits ~ i(effect, ref="None") | zip + year_month + dow,
  data = d_gt6, cluster = ~ zip + year_month
)

summary(mA); summary(mB)
```

Binomial model:
GLM estimation, family = binomial, Dep. Var.: is_le6
Observations: 2,588,544
Fixed-effects: zip: 672,  year_month: 126,  dow: 7
Standard-errors: Clustered (zip & year_month) 
                 Estimate Std. Error   z value   Pr(>|z|)    
effect::Cleanup  0.007657   0.027531  0.278111 7.8093e-01    
effect::Threat  -0.004921   0.030158 -0.163164 8.7039e-01    
effect::Impact   0.194508   0.047288  4.113296 3.9005e-05 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Log-Likelihood: -406,796.1   Adj. Pseudo R2: 0.150202
           BIC:  825,494.1     Squared Cor.: 0.052679

Interpretation
Cleanup and Threat days: No meaningful evidence that these periods change the probability of low ER visits.
Impact days (landfall): A positive and significant log-odds means ER visit counts are more likely to fall in the ≤6 range. In other words, ER utilization tends to drop during direct storm impact

This makes sense: Many people stay home. Clinics/hospitals reduce service hours. Travel becomes difficult.
This model focuses on the lower tail of demand and here, Impact days show a clear reduction

Poisson model
Poisson estimation, Dep. Var.: er_visits
Observations: 2,722,224
Fixed-effects: zip: 912,  year_month: 126,  dow: 7
Standard-errors: Clustered (zip & year_month) 
                 Estimate Std. Error  z value  Pr(>|z|)    
effect::Cleanup  0.023700   0.008545  2.77347 0.0055461 ** 
effect::Threat   0.018231   0.008404  2.16917 0.0300695 *  
effect::Impact  -0.085411   0.026034 -3.28079 0.0010352 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Log-Likelihood: -8,500,999.7   Adj. Pseudo R2: 0.604913
           BIC: 17,017,498.0     Squared Cor.: 0.880416


Interpretation
Cleanup days: Shows a small but significant +2–3% ER increase. This makes intuitive sense as delayed care resumes injuries from debris removal reconnecting the healthcare system after outages. Threat days (pre-impact conditions): ER visits increase (+1–2%). People may seek care before the storm hits (filling prescriptions, addressing minor issues early)
Impact days: ER visits drop sharply (≈ –8%). This lines up perfectly with the binomial model’s finding: ER demand suppresses during the storm’s immediate impact

Jointly the models show storm Impact periods suppress ER utilization — likely due to mobility constraints, closures, and shelter-in-place behavior. Pre-storm Threat periods elevate ER visits, consistent with precautionary care. Cleanup periods also show elevated ER visits as expected with resuming delayed care, debris removal injuries and reconnecting with heathcare system after outages. This is exactly the kind of pattern you’d expect in a population with both anticipatory behavior and acute disruption around major storms

During hurricane season in Florida, patients ramp up ER usage slightly the week before storms (Threat). On the direct storm impact day, ER demand collapses (Impact). After storms, ER visits rebound and exceed normal levels (Cleanup). This is a classic displacement–disruption pattern. Pre-storm increase → mid-storm suppression → post-storm rebound. The fixed effects ensure this is not confounded by ZIP characteristics, seasonal cycles, or weekly patterns

* Compare results with a subset of the stronger storms 55+
* Use longer and shorter times (number of days) before (Threat) and after (Cleanup)
