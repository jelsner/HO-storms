---
title: "All-Cause Mortality & Storms"
output: html_document
editor_options:
  chunk_output_type: console
---

## Summary
Experimental setup. We assemble a multi-million–record, geocoded dataset of all-cause deaths (simple-feature points in WGS84) and tag each death as one of four “storm effect” states: None, Threat, Impact, or Cleanup from an empirical storm model. We then aggregate to daily counts by zip code areas and create zip x date storm effect panel data using only days during hurricane seasons (May-November). We fit a fixed-effects Poisson model where the dependent variable is the daily death count and the fixed effects are indicator variables including zip code, year-month, and day of the week. The zip variable controls for all time-invariant differences between zip codes (e.g., population size, socioeconomic level, health infrastructure), the year-month variable controls for month-specific factors common to all zip areas (e.g., statewide seasonality, flu/COVID waves, long-term trends) and the day of the week variable controls for weekly cycles in mortality (e.g., weekend vs. weekday effects). We include a clustering option with the zip and year-month variables to ensure the standard errors are robust to residual correlation within zip codes over time and within months across zip codes. Conceptually the model estimates how daily mortality rates differ on storm-effect days (Threat, Impact, Cleanup) relative to normal “None” days, after adjusting for: differences between zip codes, monthly/seasonal patterns and day-of-week patterns.

Results. The model shows that compared with “None” days, statewide Threat days are associated with an ~8% increase in daily deaths (95% CI ~2%–15%), Cleanup days with ~8%, and Impact days with ~7% (borderline statistical evidence). The adjusted pseudo R-squared is 0.195 and the squared correlation is 0.241 indicating reasonable explanatory power given these are daily death counts with strong fixed effects. A time-stratified model per zip code yields choropleths of these rate ratios, and the Local Moran’s I highlights pockets of elevated (“hot”) and depressed (“cold”) Threat ratios, indicating geographic clustering in storm-related mortality risk. Taken together, the analysis suggests modest but reliable short-run increases in all-cause mortality around storm exposure, with non-uniform spatial patterns across Florida.

## Analytics of deaths and storms

Read the data table to a csv file
```{r}
Deaths.dt <- data.table::fread(here::here("data", "outputs", "All_Deaths_Storm_Effects.csv"))
Deaths.df <- Deaths.dt |>
  as.data.frame()
Deaths.sf <- sf::st_read(dsn = "data/outputs/Deaths", 
                         layer = "All_Deaths_Storm_Effects")
names(Deaths.sf)[1:5] <- names(Deaths.df)[1:5] # match column names with those in the csv
```

Remove rows before 1985 as the first storm since 1981 occurred in that year
```{r}
library(dplyr)
Deaths.df <- Deaths.df |>
  filter(Death_Date >= as.Date("1985-01-01"))
Deaths.sf <- Deaths.sf |>
  filter(Death_Date >= as.Date("1985-01-01"))
Deaths.dt <- Deaths.dt |>
  filter(Death_Date >= as.Date("1985-01-01"))
```

Compute statewide death rates
```{r}
#Deaths.df <- Deaths.df |>
#  mutate(SH = Storm_Category > 0)

total_deaths <- nrow(Deaths.df)
exposed_total <- Deaths.dt[Storm_Effect %in% c("Cleanup", "Impact", "Threat"), .N]

# Calculate death counts and rates for specified storm effects
death_rates <- Deaths.df |>
  filter(Storm_Effect %in% c("Cleanup", "Impact", "Threat")) |>
  group_by(Storm_Effect) |>
#  group_by(Storm_Effect, SH) |>
  summarise(Deaths = n(), .groups = "drop") |>
  mutate(Death_Rate = Deaths / exposed_total)

print(death_rates)
```

This tells you, for example, that .333% of all deaths statewide occurred under an Impact storm condition.

## Death rates per day

Get statewide number of deaths per day by storm effect
```{r}
# Range of dates for which there was a storm effect
range <- Deaths.df |>
  filter(Storm_Effect != "None")
range(range$Death_Date)

# Group deaths by date and storm effect
daily_deaths <- Deaths.df |>
#  group_by(Death_Date, Storm_Effect, SH) |>
  group_by(Death_Date, Storm_Effect) |>
  summarise(deaths = n(), .groups = "drop")
```

Get number of days statewide for each storm effect. This assumes storm effect applies to the whole day — i.e., each death has a unique storm effect
```{r}
num_days <- daily_deaths |>
#  distinct(Death_Date, Storm_Effect, SH) |>
  distinct(Death_Date, Storm_Effect) |>
#  count(Storm_Effect, SH, name = "num_days")
  count(Storm_Effect, name = "num_days")

print(num_days)
```

Total statewide deaths per storm effect
```{r}
total_deaths <- daily_deaths |>
#  group_by(Storm_Effect, SH) |>
  group_by(Storm_Effect) |>
  summarise(total_deaths = sum(deaths), .groups = "drop")

print(total_deaths)
```

Calculate statewide daily death rate
```{r}
# death_rates <- left_join(total_deaths, num_days, by = c("Storm_Effect", "SH")) |>
death_rates <- left_join(total_deaths, num_days, by = "Storm_Effect") |>
  mutate(daily_death_rate = total_deaths / num_days)

print(death_rates)
```

This tells us that in this sample the statewide daily death rate is somewhat higher on non-storm-effect days. But a lot is going on: time and spatial trends, seasonality, autocorrelation. We need a model that accounts for these things

## Model using a negative binomial generalized additive model

Setup the data frame
```{r}
library(lubridate)

dat <- daily_deaths %>%
  transmute(
    date  = as.Date(Death_Date),
    y     = as.integer(deaths),
    effect = factor(Storm_Effect,
                    levels = c("None", "Threat", "Impact", "Cleanup")),
    dow   = wday(date, label = TRUE, week_start = 1), # Mon..Sun
    doy   = yday(date),                               # 1..366 (handles leap days too)
    tnum  = as.integer(date - min(date))              # days since start (well-scaled)
  ) %>%
  mutate(
    # make sure 'dow' is UNordered and with desired reference level:
    dow = factor(as.character(dow),  # drop any ordered class
                 levels = c("Mon","Tue","Wed","Thu","Fri","Sat","Sun"))
  )
```

Generalized linear model (negative binomial)
```{r}
m_glm <- MASS::glm.nb(
  y ~ effect + dow + poly(doy, 3) + poly(tnum, 3),
  data = dat, link =  log
)
summary(m_glm)

acf(residuals(m_glm))
```

This looks good. Exponentiating the Threat effect term of .0283 yields 1.0287. This says that on Threat days the number of deaths is 2.9% higher than on non-effect days and it is marginally significant (p = .015)

But there is large autocorrelation in the residuals. Let's try another model that includes autocorrelation

```{r}
library(gratia)
library(mgcv)

# mark start of series (single series here)
dat$ARstart <- c(TRUE, rep(FALSE, nrow(dat)-1))

# Cyclic smooth needs knots that wrap (0.5, 366.5) for stability
knots_list <- list(doy = c(0.5, 366.5))

m_ar1 <- bam(
  y ~ effect + dow + s(doy, bs = "cc", k = 10) + s(tnum, k = 100),
  family = nb(link = "log"),
  data = dat,
  method = "fREML",
  discrete = TRUE,
  knots = knots_list,
  AR.start = dat$ARstart,
  rho = 0.57  # set to the estimated lag-1 corr
)

summary(m_ar1)
```

Effect days are no longer statistically significant. Autocorrelation inflates z-scores. Plain (quasi/NB) GLMs assume independent residuals. Daily mortality has strong autoregression (AR) structure; ignoring it underestimates SEs, making small effects look “significant.” Adding AR(1) (or equivalently, using time-stratified controls) raises SEs → p-values go up. Also there is collinearity with seasonality/trend. Storms cluster in certain seasons & years. Flexible smooths (or fine time strata) explain a lot of the same variance; the unique contribution of storm indicators shrinks.

Multiple testing/time-of-day confounding (more relevant at ZIP scale), but with many ZIPs, some will look “significant” by chance unless we account for dependence and control false detection rate.

## Poisson model with stratum fixed effects at the state level

Setup the data (as a panel)

Get Florida zips
```{r}
library(tigris)
library(sf)
options(tigris_use_cache = TRUE)

# Get national ZCTAs and subset to those intersecting Florida
zctas_all <- zctas(cb = TRUE, starts_with = NULL, year = 2020)
florida <- states(cb = TRUE) %>% filter(STUSPS == "FL")

# Filter ZCTAs that intersect Florida
zctas_fl <- st_intersection(zctas_all, st_transform(florida, st_crs(zctas_all)))
zctas_fl <- st_make_valid(zctas_fl) # in case of invalid geometries
```

Spatial join deaths to zips
```{r}
# Inspect CRS
st_crs(zctas_fl)   # NAD83 (likely EPSG:4269)
st_crs(Deaths.sf)  # WGS84 (EPSG:4326)

# Harmonize CRS (transform points to the polygons' CRS)
Deaths_aligned <- st_transform(Deaths.sf, st_crs(zctas_fl))

# Make polygons valid (fix self-intersections, rings, etc.)
zctas_valid <- st_make_valid(zctas_fl) %>% dplyr::select(GEOID20, geometry)

sf_use_s2(TRUE)

# Point-in-polygon join: add GEOID20 from polygon to each point
#    st_within() = strictly inside (excludes points exactly on boundary)
deaths_with_zip <- st_join(
  Deaths_aligned,
  zctas_valid,
  join = st_intersects,   # or st_intersects if you want to include boundary points
  left = TRUE
)

# QA checks
# Any points that didn't land in a polygon? (e.g., offshore or boundary issues)
sum(is.na(deaths_with_zip$GEOID20))
# returns 1915 (~.03%) when st_within or st_intersects. Could snap unmatched points to nearest polygon

# Sanity: every point should match at most one ZCTA (ZCTAs are non-overlapping)
dup_check <- deaths_with_zip |>
  st_drop_geometry() |>
  count(Death_ID) |>
  filter(n > 1)
dup_check

# Remove those deaths
deaths_with_zip <- deaths_with_zip |> 
  filter(!is.na(GEOID20))
```

List death dates & storm names by zip and storm effect
```{r}
library(purrr)

df <- deaths_with_zip %>%
  st_drop_geometry()

# A compact tibble with a list-column of Death_Dates
dates_by_effect_df <- df %>%
  group_by(GEOID20, Storm_Effect) %>%
  summarise(
    Death_Dates = list(sort(unique(Death_Date))),   # the actual unique dates (could be more than 1 death)
    Storm_Names = list(unique(Storm_Name)),         # storm names
    n_records   = n(),                               # total records
    n_dates     = length(Death_Dates[[1]]),          # unique dates
    .groups = "drop"
  ) %>%
  arrange(GEOID20, Storm_Effect)

dates_by_effect_df
length(unique(dates_by_effect_df$GEOID20))
```

Daily death counts by zip
```{r}
daily_by_zip <- deaths_with_zip %>%
  st_drop_geometry() %>%
  transmute(
    zip    = as.character(GEOID20),
    date   = as.Date(Death_Date),
    effect = as.character(Storm_Effect)
  ) %>%
  group_by(zip, date, effect) %>%
  summarise(deaths = n(), .groups = "drop")

# Sanity check
daily_by_zip %>%
  count(zip, date) %>%
  filter(n > 1)  # rows here mean the same zip–date appears under multiple effects
```

Compute death totals by zip by effect with the effect as separate columns (wide format), then join to zip polygons, then make choropleth maps
```{r}
death_counts_wide <- deaths_with_zip %>%
  sf::st_drop_geometry() %>%  
  filter(Storm_Effect %in% c("Threat","Impact","Cleanup")) %>%
  count(GEOID20, Storm_Effect, name = "deaths") %>%
  tidyr::pivot_wider(
    names_from  = Storm_Effect,
    values_from = deaths,
    values_fill = 0
  )

zcta_poly <- zctas_fl %>%
  mutate(GEOID20 = as.character(GEOID20)) %>%
  { if (is.na(sf::st_crs(.)$epsg) || sf::st_crs(.)$epsg != 4326) sf::st_transform(., 4326) else . } %>%
  sf::st_make_valid()

# keep polygonal features only and cast to MULTIPOLYGON
zcta_poly <- zcta_poly[sf::st_geometry_type(zcta_poly) %in% c("POLYGON","MULTIPOLYGON"), , drop = FALSE]
zcta_poly <- zcta_poly[!sf::st_is_empty(zcta_poly), , drop = FALSE]
zcta_poly <- suppressWarnings(sf::st_cast(zcta_poly, "MULTIPOLYGON", warn = FALSE))

# left join counts (preserves polygon geometry)
death_map_sf <- zcta_poly %>%
  left_join(death_counts_wide, by = "GEOID20") %>%
  mutate(
    Threat = replace_na(Threat, 0L),
    Impact = replace_na(Impact, 0L),
    Cleanup = replace_na(Cleanup, 0L)
  ) %>%
  select(GEOID20, Threat, Impact, Cleanup, geometry)

# for story map
saveRDS(death_map_sf, "data/outputs/Results/zcta_death_counts.rds")
```

Make maps
```{r}
library(ggplot2)
library(dplyr)
library(tidyr)
library(sf)
library(RColorBrewer)

make_map <- function(data_sf, col, title = col) {
  ggplot(data_sf) +
    geom_sf(aes(fill = .data[[col]]), color = "white", size = 0.05) +
    scale_fill_stepsn(
      colors  = pal,
      breaks  = bins,
      limits  = range(bins),
      na.value = "#cccccc",
      name    = "Deaths per ZCTA"
    ) +
    labs(title = title) +
    coord_sf(datum = NA) +
    theme_void(base_size = 12) +
    theme(plot.title = element_text(face = "bold", hjust = 0.5))
}

p_threat  <- make_map(death_map_sf, "Threat",  "Threat deaths")
p_impact  <- make_map(death_map_sf, "Impact",  "Impact deaths")
p_cleanup <- make_map(death_map_sf, "Cleanup", "Cleanup deaths")

# Print or save
# p_threat; p_impact; p_cleanup
# ggsave("zcta_deaths_threat.png",  p_threat,  width = 7.2, height = 6, dpi = 300)
# ggsave("zcta_deaths_impact.png",  p_impact,  width = 7.2, height = 6, dpi = 300)
# ggsave("zcta_deaths_cleanup.png", p_cleanup, width = 7.2, height = 6, dpi = 300)
```

Join deaths to a full calendar. The calendar is created in `HO-storms.Rmd` and is called `effect_calendar`
```{r}
dbz <- as.data.table(daily_by_zip)[ , .(
  zip   = as.character(zip),
  date  = as.IDate(date),
  deaths = as.integer(deaths)
)]

zip_day <- dbz[effect_calendar, on = .(zip, date)]
zip_day[is.na(deaths), deaths := 0L]  # fill missing counts with 0

# After joining deaths: zeros should appear across all effects
zip_day[, .(zeros = sum(deaths == 0L), rows = .N), by = effect][order(effect)]
```

Then we fit a Poisson with stratum fixed effects. This is equivalent to conditioning on the stratum totals and largely removes serial confounding. Use robust (clustered) SEs by stratum to protect against residual correlation within a stratum

Add strata variables
```{r}
dat <- zip_day %>%
  filter(month(date) %in% 5:11) %>%  # hurricane season only
  transmute(
    zip    = as.character(zip),
    date   = as.IDate(date),
    deaths = as.integer(deaths),
    effect = factor(effect, levels = c("None","Threat","Impact","Cleanup")),
    dow     = factor(strftime(date, "%u"), levels = as.character(1:7),
                       labels = c("Mon","Tue","Wed","Thu","Fri","Sat","Sun")),
    ym     = format(date, "%Y-%m")                      # year-month stratum
  )
```

Fit model
```{r}
library(fixest)

m_pooled <- fepois(
  deaths ~ i(effect, ref = "None") | zip + ym + dow,
  data    = dat,
  cluster = ~ zip + ym   # two-way clustered SEs (ZIP & month)
)

summary(m_pooled)

# Rate ratios (RR) with CIs
library(broom)

rr_tbl <- tidy(m_pooled, conf.int = TRUE) %>%                # returns log-scale
  filter(grepl("^effect::", term)) %>%
  transmute(
    effect = sub("^effect::", "", term),
    RR     = exp(estimate),
    LCI    = exp(conf.low),
    UCI    = exp(conf.high),
    p      = p.value
  )
rr_tbl
```

We fit a fixed-effects Poisson with: ZIP fixed effects (| zip) → controls for all time-invariant differences across ZIPs (population size, baseline mortality, socio-demographics, etc.), Year-month fixed effects (| ym) → controls for statewide shocks and seasonality at the month level (flu waves, hurricanes seasons, COVID waves, etc.). Day-of-week fixed effects (| dow) → controls for weekly pattern. So the effect:: coefficients are within-ZIP log rate ratios vs “None”, after removing month and weekday patterns. Exponentiate them to get rate ratios (RR).

What the coefficients mean: 
We fit a fixed-effects Poisson with: ZIP fixed effects (| zip) → controls for all time-invariant differences across ZIPs (population size, baseline mortality, socio-demographics, etc.)
Year-month fixed effects (| ym) → controls for statewide shocks and seasonality at the month level (flu waves, hurricanes seasons, COVID waves, etc.)
Day-of-week fixed effects (| dow) → controls for weekly pattern
So the effect::… coefficients are within-ZIP log rate ratios vs “None”, after removing month and weekday patterns. Exponentiate them to get rate ratios (RR). Convert the estimates to RRs (95% CI)

Threat: β = 0.07994 (SE 0.02950), p = 0.0067
RR = 1.083 (95% CI 1.022–1.148) → ~8.3% higher deaths vs None, adjusted for ZIP / YM / DOW.
Impact: β = 0.06389 (SE 0.03286), p = 0.0518
RR = 1.066 (95% CI 1.000–1.137) → ~6.6% higher; borderline at 0.052.
Cleanup: β = 0.07333 (SE 0.03045), p = 0.0160
RR = 1.076 (95% CI 1.014–1.142) → ~7.6% higher.
(RR = exp(β); CI = exp(β ± 1.96·SE).)

NOTE: 44/0/0 fixed-effects (357,808 observations) removed because of only 0 outcomes or singletons.
You have 3 FE groups (zip / ym / dow). The 44/0/0 means 44 ZIP levels were dropped (0 YM, 0 DOW) because, within those ZIPs, the data contributed no identifying variation (e.g., all days had zero deaths or only singleton cells after stratification). The corresponding 357,808 rows were removed from estimation. That’s expected behavior: those ZIPs can’t help identify contrasts after the FE are applied.

Log-likelihood and BIC are for the Poisson FE fit. “Adj. Pseudo R²: 0.195” and “Squared Cor.: 0.241” indicate reasonable explanatory power given this is daily death counts with strong FE.

Compared with “None” days, statewide Threat days are associated with an ~8% increase in daily deaths (95% CI ~2%–15%), Cleanup days with ~8%, and Impact days with ~7% (borderline statistical evidence).

Try negative binomial fixed effects as a sensitivity
```{r}
m_pooled2 <- fenegbin(
  deaths ~ i(effect, ref = "None") | zip + ym + dow,
  data    = dat,
  cluster = ~ zip + ym   # two-way clustered SEs (ZIP & month)
)

summary(m_pooled2)
```

Results are essentially the same

## Time stratified Poisson fixed effect per zip code area

Choose candidate zips worth modeling
```{r}
cands <- dat[
  , .(
      has_none = any(effect == "None"),
      has_tic  = any(effect != "None"),
      pos_y    = any(deaths > 0L)
    ),
  by = zip
][has_none & has_tic & pos_y, zip]
length(cands) # returns 999

# Optionally require a minimum number of storm-effect days
min_effect_days <- 5L
cands2 <- dat[
  , .(n_tic = sum(effect != "None")),
  by = zip
][n_tic >= min_effect_days, zip]
length(cands2) # returns 993
```

Per-zip poisson fixed effect (stratify by ym + dow)
```{r}
library(pbapply)

# Function for a single zip code
fit_one <- function(df) {
  df[, effect := factor(effect, levels = c("None","Threat","Impact","Cleanup"))]
  # Skip if baseline mean is zero (RR not defined)
  if (!any(df$effect == "None") || mean(df$deaths[df$effect == "None"]) == 0) return(NULL)
  tryCatch(
    fepois(
      deaths ~ i(effect, ref = "None") | ym + dow,
      data    = df,
      cluster = ~ ym
    ),
    error = function(e) NULL
  )
}

# sequential (shows a progress bar)
models <- pblapply(cands, function(z) fit_one(dat[zip == z]))
names(models) <- cands
```

Extract relative rates (and 95% CIs) per zip for Threat, Impact, Cleanup
```{r}
rr_by_zip <- bind_rows(lapply(names(models), function(z) {
  m <- models[[z]]
  if (is.null(m)) return(NULL)
  tidy(m, conf.int = TRUE) %>%
    filter(grepl("^effect::", term)) %>%
    transmute(
      zip    = z,
      effect = sub("^effect::", "", term),
      RR     = exp(estimate),
      LCI    = exp(conf.low),
      UCI    = exp(conf.high),
      p      = p.value
    )
}))

# Optional: keep only Threat/Impact/Cleanup rows
rr_by_zip <- rr_by_zip %>% filter(effect %in% c("Threat","Impact","Cleanup"))
```

The extreme RRs are a data-sparsity/separation artifact of per-ZIP, heavily stratified Poisson fits

Where are the extremes coming from?
```{r}
# per-ZIP support by effect
zip_support <- dat2 %>%      # dat = per-ZIP daily frame 
  group_by(zip, effect) %>%
  summarise(
    days   = n(),
    deaths = sum(deaths),
    mean_y = mean(deaths),
    .groups = "drop"
  )

# join support onto the RR table
rr_chk <- rr_by_zip %>%
  left_join(zip_support %>% rename(days_e = days, deaths_e = deaths, mean_e = mean_y),
            by = c("zip","effect")) %>%
  left_join(zip_support %>% filter(effect=="None") %>%
              select(zip, days_none = days, deaths_none = deaths, mean_none = mean_y),
            by = "zip")

# look at the pathological ones
rr_chk %>%
  filter(RR < 0.2 | RR > 5 | !is.finite(RR)) %>%
  arrange(RR) %>%
  select(zip, effect, RR, LCI, UCI, p, days_e, deaths_e, days_none, deaths_none)
```

Empirical Bayes shrinkage before mapping
```{r}
# Pooled FE Poisson to get a prior mean per effect (log scale)
# This was done above and saved as `m_pooled`
#pooled <- fixest::fepois(deaths ~ i(effect, "None") | zip + ym + dow, data = dat, cluster = ~ zip + ym)
pooled_eff <- broom::tidy(m_pooled, conf.int = FALSE) %>%
  filter(grepl("^effect::", term)) %>%
  transmute(effect = sub("^effect::","",term), mu0 = estimate)   # log-RR prior

# Approximate per-ZIP log-RR variance from the model (delta from CI)
log_rr <- rr_by_zip %>%
  mutate(
    logRR   = log(RR),
    se_log  = (log(UCI) - log(LCI)) / (2*1.96)
  ) %>% filter(is.finite(logRR), se_log > 0)

# Shrink: posterior mean = (logRR / se^2 + mu0 / tau^2) / (1/se^2 + 1/tau^2)
#    Use a single tau per effect (between-ZIP SD). Estimate tau via robust SD.
tau_by_eff <- log_rr %>% group_by(effect) %>%
  summarise(tau = stats::mad(logRR, constant = 1), .groups = "drop")

rr_shrunk <- log_rr %>%
  left_join(pooled_eff, by="effect") %>%
  left_join(tau_by_eff, by="effect") %>%
  mutate(
    prec_obs = 1/(se_log^2),
    prec_pri = 1/(pmax(tau, 1e-6)^2),
    post     = (logRR*prec_obs + mu0*prec_pri) / (prec_obs + prec_pri),
    RR_shr   = exp(post)
  ) %>%
  select(zip, effect, RR_shr)

# for story map (Threat only)
rr_wide <- rr_shrunk %>%
  filter(effect == "Threat") %>%
  mutate(GEOID20 = zip,
         Threat = RR_shr) %>%
  select(GEOID20, Threat)

rr_map_sf <- zcta_poly %>%
  left_join(rr_wide, by = "GEOID20") %>%
  select(GEOID20, Threat, geometry)
```

Map the Threat
```{r}
effect_to_map = "Threat"
# Bin RRs: [0,1), [1,2), [2,4), [4,∞)
rr_map_sf <- rr_map_sf %>%
  mutate(
    rr_bin = cut(
      Threat,
      breaks = c(0, 1, 2, 4, Inf),  # last break is Inf
      labels = c("0–1", "1–2", "2–4", ">4"),
      right  = TRUE,                   # (a, b] so last bin is (8, Inf]
      include.lowest = TRUE
    ),
    # lock the level order to match the palette keys
    rr_bin = fct_relevel(rr_bin, "0–1", "1–2", "2–4", ">4")
  )

# for story map
saveRDS(rr_map_sf, "data/outputs/Results/zcta_results.rds")

# Define a discrete palette (colorblind-friendly)
pal <- c(
  "0–1" = "#fbb4b9",  # light
  "1–2" = "#9ecae1",
  "2–4" = "#6baed6",
  ">4"  = "#08519c"
)

# Plot
ggplot(rr_map_sf) +
  geom_sf(aes(fill = rr_bin), color = NA) +
  scale_fill_manual(
    values   = pal,
    limits = levels(rr_map_sf$rr_bin),
    drop     = FALSE,
    na.value = "grey90",
    name     = paste0("RR vs None (", effect_to_map, ")")
  ) +
  labs(
    title = paste("Per-ZIP Rate Ratios on", effect_to_map, "days"),
    subtitle = "Bins: 0–1, 1–2, 2–4, >4",
    caption = "RRs from per-ZIP time-stratified Poisson fixed-effect"
  ) +
  theme_minimal() +
  theme(
    legend.position = "right",
    panel.grid.major = element_line(color = "white"),
    axis.title = element_blank()
  )

# Sanity check
table(rr_map_sf$rr_bin, useNA = "ifany")
sum(is.infinite(rr_map_sf$RR_shr))
```

Save per zip model results for story-map (Threat only)
```{r}
# zcta_results.rds
rr_shrunk_threat_only <- rr_shrunk %>%
  filter(effect == "Threat")
saveRDS(rr_shrunk_threat_only, "data/outputs/Results/zcta_results.rds")
```

## Hot spot analysis
```{r}
library(spdep)

# Prep: keep only rows with RR, keep polygons ---------------------------
g <- Moran_sf %>%
  filter(!is.na(RR)) %>%
  st_make_valid()

# Neighbors & weights (Queen) -------------------------------------------
# Set queen = TRUE (default). Use rook = FALSE (change to TRUE for rook)
nb <- spdep::poly2nb(g, queen = TRUE)
lw <- spdep::nb2listw(nb, style = "W", zero.policy = TRUE)  # allow islands

RR_vec <- as.numeric(log(g$RR))  # logs and make simple vector

# Local Moran's I with permutations -------------------------------------
# 999 permutations; two-sided test; handle islands
lm <- spdep::localmoran_perm(
  x = RR_vec,
  listw = lw,
  nsim = 999,
  alternative = "two.sided",
  zero.policy = TRUE
)
lm <- as.data.frame(lm)
names(lm) <- c("Ii","E.Ii","Var.Ii","Z.Ii","p.value")

# Multiple-testing correction (FDR)
lm$p.adj <- p.adjust(lm$p.value, method = "fdr")

# Spatial lag of RR (for quadrant classification)
lagRR <- spdep::lag.listw(lw, g$RR, zero.policy = TRUE)

# Cluster classification -------------------------------------------------
alpha <- 0.25
mx <- mean(g$RR, na.rm = TRUE)

cluster <- dplyr::case_when(
  g$RR >= mx & lagRR >= mx & lm$Ii > 0 & lm$p.value < alpha ~ "High-High",
  g$RR <= mx & lagRR <= mx & lm$Ii > 0 & lm$p.value < alpha ~ "Low-Low",
  g$RR >= mx & lagRR <= mx & lm$Ii < 0 & lm$p.value < alpha ~ "High-Low",
  g$RR <= mx & lagRR >= mx & lm$Ii < 0 & lm$p.value < alpha ~ "Low-High",
  TRUE ~ "Not Significant"
)

g_locI <- g %>%
  mutate(
    Ii       = lm$Ii,
    E_Ii     = lm$E.Ii,
    Var_Ii   = lm$Var.Ii,
    Z_I      = lm$Z.Ii,
    p_value  = lm$p.value,
    p_adj    = lm$p.adj,
    lag_RR   = lagRR,
    cluster  = factor(cluster,
                      levels = c("High-High","Low-Low","High-Low","Low-High","Not Significant"))
  )

# Join back to the full set (keeps rows where RR was NA) ----------------
Moran_out <- Moran_sf %>%
  left_join(
    g_locI %>%
      st_drop_geometry() %>%
      select(GEOID20, Ii, E_Ii, Var_Ii, Z_I, p_value, p_adj, lag_RR, cluster),
    by = "GEOID20"
  ) %>%
  mutate(
    cluster = ifelse(is.na(cluster), "No data", as.character(cluster)),
    cluster = factor(cluster,
      levels = c("High-High","Low-Low","High-Low","Low-High","Not Significant","No data")
    )
  )
```

Maps
```{r}
# Map local Moran's I
library(scales)

# Pick a symmetric range around 0 so colors are comparable
imax <- max(abs(g_locI$Ii), na.rm = TRUE)

ggplot(g_locI) +
  geom_sf(aes(fill = Ii), color = NA) +
  scale_fill_gradient2(
    low    = "#2b6cb0",   # blue  (negative Ii)
    mid    = "#f7f7f7",   # light
    high   = "#e53e3e",   # red   (positive Ii)
    midpoint = 0,
    limits = c(-imax, imax),
    oob = squish,
    name = "Local Moran's I"
  ) +
  coord_sf(datum = NA) +
  theme_void(base_size = 12) +
  theme(legend.position = "right")

# Map the clusters
pal <- c("High-High"="#e53e3e","Low-Low"="#2b6cb0",
         "High-Low" ="#f59e0b","Low-High"="#10b981",
         "Not Significant"="#d9d9d9","No data"="#cccccc")

ggplot(Moran_out) +
  geom_sf(aes(fill = cluster), color = NA) +
  scale_fill_manual(values = pal, drop = FALSE) +
  theme_void() + labs(fill = "Local Moran clusters")
```

Save per zip local Moran's results for story-map (Threat only)
```{r}
# zcta_clusters.rds
rr_clusters <- g_locI %>%
  select(GEOID20, Ii, geometry)

saveRDS(rr_clusters, "data/outputs/Results/zcta_clusters.rds")
```

## Emergency room visits

Data received from David Hsu on October 16, 2025 via email
```{r}
ER_visits.dt <- data.table::fread(here::here("data", "Combined_ER_Data.csv"))
ER_visits.df <- ER_visits.dt |>
  as.data.frame()
```

Merge with daily mortality counts
```{r}
library(data.table)

setDT(zip_day)
setDT(ER_visits.df)

# Pad ZipCode to 5 chars; drop non-sensical codes (0/NA/too short/too big)
ER_visits.df[
  , ZipCode := fifelse(ZipCode > 0 & ZipCode < 1e6, sprintf("%05d", ZipCode), NA_character_)
]
ER_visits.df[, Date := as.IDate(Date)]

ER_visits_clean <- ER_visits.df[!is.na(ZipCode) & nchar(ZipCode) == 5]

# Standardize names to match zip_day; optionally collapse duplicates
setnames(ER_visits_clean, c("ZipCode","Date","Total_Visits"),
                           c("zip",    "date","er_visits"))

# If there can be multiple ER rows per (zip,date), aggregate to daily total
ER_visits_clean <- ER_visits_clean[, .(er_visits = sum(er_visits, na.rm = TRUE)),
                                   by = .(zip, date)]

# Standardize zip_day types 
zip_day[, date := as.IDate(date)]
zip_day[, zip  := as.character(zip)] # already looks like "31537", but safe

# Merge: keep all zip_day rows (left join) 
zip_day_with_visits <- merge(
  zip_day,
  ER_visits_clean,
  by = c("zip","date"),
  all.x = TRUE,
  allow.cartesian = TRUE  # only matters if duplicates remain
)

# Optional: treat missing ER as 0 instead of NA
zip_day_with_visits[is.na(er_visits), er_visits := 0L]
```

A model for daily death counts using ER visits as a covariate and test whether slopes differ by effect
```{r}
library(fixest)

dat <- copy(zip_day_with_visits)

# --- Build filters and control variables ---
dat[, year  := as.integer(format(date, "%Y"))]
dat[, month := as.integer(format(date, "%m"))]

# Keep hurricane season & target years
dat <- dat[year %between% c(2005, 2022) & month %in% 5:11]

# Seasonality: month-of-year (factor, May..Nov)
dat[, month_fac := factor(month, levels = 5:11, labels = month.abb[5:11])]

# Day-of-week (factor)
dow_levels <- c("Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday")
dat[, dow := factor(weekdays(as.Date(date)), levels = dow_levels)]

# Time trend: numeric index (scale to years for stability)
dat[, t_years := as.numeric(date) / 365.25]

# For clustering by year-month (optional but recommended)
dat[, ym := format(date, "%Y-%m")]

# Optional: handle missing ER visits (choose either 0 or keep as NA and drop)
dat[is.na(er_visits), er_visits := 0L]

# Optional: scale ER for easier interpretation (e.g., per 10 visits)
dat[, er10 := er_visits / 10]

m_er_int <- fepois(
  deaths ~ 
    er10 +                          # baseline ER slope on "None" days
    i(effect, ref = "None") +       # mean shifts for Threat/Impact/Cleanup vs None
    i(effect, var = er10, ref = "None") +  # ER slope *differences* by effect
    poly(t_years, 2) |              # smooth trend
    zip + month_fac + dow,          # fixed effects (ZIP, seasonality, DOW)
  data    = dat,
  cluster = ~ zip + ym
)

summary(m_er_int)
```

The model says that there is a 1.5% increase in daily deaths per 10 additional ER visits, but that increase is not changed by storm effect.

### Model using censored Poison via {brms} package
```{r}
install.packages("cmdstanr",
                 repos = c("https://stan-dev.r-universe.dev", getOption("repos")))
library(cmdstanr)
cmdstanr::check_cmdstan_toolchain(fix = TRUE)    # confirms compiler setup
cmdstanr::install_cmdstan(cores = 2)             # downloads & builds CmdStan
```

```{r}
library(brms)

dat <- copy(zip_day_with_visits)

# Restrict to hurricane season, 2005–2022
dat[, year  := as.integer(format(date, "%Y"))]
dat[, month := as.integer(format(date, "%m"))]
dat <- dat[year %between% c(2005, 2022) & month %in% 5:11]

# How many days have missing values for er_visits

# Storm effect factor with "None" as ref
dat[, effect := factor(effect, levels = c("None","Threat","Impact","Cleanup"))]

# Censoring indicator per brms: "left" for censored obs, "none" otherwise
dat[, cens_flag := ifelse(er_visits == 6L, "left", "none")]

# Controls: weekday, month FE; trend as smooth; ZIP as varying intercept (random effect)
# (Random intercepts approximate ZIP FE while keeping the model computationally feasible.)
dow_levels <- c("Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday")
dat[, dow := factor(weekdays(as.Date(date)), levels = dow_levels)]
dat[, ym  := format(date, "%Y-%m")]
dat[, t_years := as.numeric(date)/365.25]

bf_er <- bf(
  er_visits | cens(cens_flag) ~ 
    effect + dow + factor(month) + s(t_years, k = 6) + (1 | zip),
  family = poisson()
)

# Reasonable, weakly informative priors
pri <- c(
  set_prior("normal(0, 0.5)", class = "b"),       # population-level
  set_prior("exponential(1)", class = "sd"),      # group-level std dev
  set_prior("exponential(1)", class = "sds")      # smooth term
)

# Fit (uses Stan; uses cmdstanr backend)
fit_cens <- brm(
  bf_er, data = dat,
  prior = pri,
  backend = "cmdstanr", chains = 4, cores = 4, iter = 2000
)

summary(fit_cens)
```

This takes very long (~ 85 hours, 3.5 days) and the following warnings:
1: Parts of the model have not converged (some Rhats are > 1.05). Be careful when analysing the results! We recommend running more iterations and/or setting stronger priors. 
2: There were 2003 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup

DO NOT INTERPRET THESE RESULTS

### Model using logit and Poisson separately

Instead use a two-part “threshold (≤6) + truncated count (>6)” approximation (stays in `fixest`). The process is modeled as:
Part A (logit): probability the true count is ≤6 (we observe 6)
Part B (Poisson on truncated support): conditional mean for counts >6
Then combine for interpretation

```{r}
library(fixest)
library(dplyr)

d2 <- dat %>% mutate(
  is_le6 = as.integer(er_visits == 6L)           # proxy for "true ≤ 6"
)

# Part A: logistic model for being ≤ 6
mA <- feglm(
  is_le6 ~ i(effect, ref="None") | zip + ym + dow,
  data = d2, family = binomial("logit"),
  cluster = ~ zip + ym
)

# Part B: Poisson on Y>6 (treat observed counts >6 as exact)
d_gt6 <- filter(d2, er_visits > 6L)
mB <- fepois(
  er_visits ~ i(effect, ref="None") | zip + ym + dow,
  data = d_gt6, cluster = ~ zip + ym
)

summary(mA); summary(mB)
```

Interpretation of the results from a two-part model for ER visits

Model 1: We fit:
Model 1A (mA): logit model for whether ER visits are in the censored bin (coded as 6)
Model 1B (mB): Poisson model for the number of ER visits given that visits > 6
Both control for ZIP, month (ym), and day-of-week with fixed effects, and cluster SEs by ZIP and month

Model 1A
GLM estimation, family = binomial, Dep. Var.: is_le6
Observations: 1,835,341
Fixed-effects: zip: 673,  ym: 126,  dow: 7
Standard-errors: Clustered (zip & ym) 
                 Estimate Std. Error  z value  Pr(>|z|)    
effect::Threat   0.431304   0.164050  2.62910 0.0085612 ** 
effect::Impact   0.881900   0.380303  2.31894 0.0203983 *  
effect::Cleanup -0.281038   0.243237 -1.15541 0.2479237    
Log-Likelihood: -312,448.7   Adj. Pseudo R2: 0.291291
           BIC:  636,536.5     Squared Cor.: 0.185756
Model 1B
Poisson estimation, Dep. Var.: er_visits
Observations: 2,725,008
Fixed-effects: zip: 928,  ym: 126,  dow: 7
Standard-errors: Clustered (zip & ym) 
                 Estimate Std. Error   z value   Pr(>|z|)    
effect::Threat  -0.080044   0.021385 -3.743002 0.00018183 ***
effect::Impact  -0.241847   0.075104 -3.220179 0.00128111 ** 
effect::Cleanup  0.033312   0.039993  0.832946 0.40487508    
Log-Likelihood: -8,509,566.8   Adj. Pseudo R2: 0.604975
 BIC: 17,034,870.3     Squared Cor.: 0.880402
 
Model 1A: Probability that ER visits are in the “≤6” bin
Outcome:
is_le6 = 1 if er_visits == 6 (i.e., the left-censored bin “true ≤ 6"),
is_le6 = 0 if er_visits > 6

Key coefficients (log-odds; exponentiated below as odds ratios):
Threat: 0.4313 → OR ≈ 1.54
Impact: 0.8819 → OR ≈ 2.42
Cleanup: −0.2810 → OR ≈ 0.75 (not statistically significant)

Interpretation (all relative to None days, holding ZIP, ym, and DOW constant):
On Threat days, the odds that the ER count is in the censored bucket (≤6) are about 54% higher than on None days
On Impact days, the odds of being in the censored bucket are about 2.4 times higher than on None days
On Cleanup days, the odds are about 25% lower, but this is not statistically significant (p ≈ 0.25), so we can’t confidently claim a difference from None

In other words, Threat and Impact days are more likely to be “low-visit” days (in the censored sense) compared with normal (None) days, after accounting for location, season, and weekday

Model 1B: Mean ER visits given > 6 visits
Outcome:
er_visits (count) but only on days where er_visits > 6. This is a Poisson FE model.
Exponentiating the coefficients gives rate ratios (RRs):
Threat: −0.0800 → RR ≈ 0.92
Impact: −0.2418 → RR ≈ 0.79
Cleanup: 0.0333 → RR ≈ 1.03 (not significant)

Interpretation (again relative to None days, conditional on being > 6):
On Threat days, among days with >6 visits, the mean ER count is about 7–8% lower than on None days (RR ≈ 0.92, p < 0.001)
On Impact days, given >6 visits, the mean count is about 21–22% lower than on None days (RR ≈ 0.79, p ≈ 0.001)
On Cleanup days, the mean is about 3% higher, but not statistically distinguishable from None (p ≈ 0.40)
So, when the ER is “busy” ( > 6 visits), Threat and Impact days actually show fewer visits than comparable None days, controlling for ZIP, month, and DOW

Putting both parts together
Think of the ER visits distribution on a given day in two stages:
Stage 1 – Are visits in the low bin (≤6) or above it?
Threat and Impact days are more likely than None days to fall in the low (≤6) category.
Cleanup days don’t differ clearly from None.
Stage 2 – If visits are above 6, how large is the count?
On Threat and Impact days with >6 visits, the average ER volume is lower than on comparable None days.
Cleanup days again look similar to None

Qualitatively:
Threat & Impact:
Higher chance of being in the censored/low bucket.
Lower mean when above the threshold.

Overall, these phases shift the ER visit distribution downward relative to None days, given the censoring structure.
Cleanup:
No strong evidence of a difference from None in either the probability of being ≤6 or the mean above 6.
This is all conditional on our controls (ZIP FE, ym FE, DOW FE), so those patterns are net of geography, broad seasonality/trend, and weekday effects

We can
Convert these effects into an approximate overall change in mean ER visits by effect (combining both stages), or
Compare these ER patterns to the death models to see how ER demand vs. mortality co-move across storm effect phases



