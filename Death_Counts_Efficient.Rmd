---
title: "Daily death counts relative to storm effects (Efficient)"
output: html_document
editor_options:
  chunk_output_type: console
---

# Daily death counts relative to storm effects — **Efficient Implementation**

This script is a performance-optimized rewrite of the original analysis. Key changes:

- Avoid `rowwise()` and per-record operations (vectorize and batch joins).
- Use spatially indexed `st_filter()` and minimize `st_union()`.
- Keyed joins via `data.table` for date matching and calendar building.
- Project once where expensive geometry ops are needed.
- Optional polygon simplification for faster joins.

---

## Create an empirical storm effects model (optimized)

```{r}
library(sf)
library(dplyr)
library(data.table)
library(lubridate)
library(tidyr)

storm_intensity <- 34 # all tropical storms and hurricanes
begin_year <- 1981
end_year <- 2022

L <- "https://www.ncei.noaa.gov/data/international-best-track-archive-for-climate-stewardship-ibtracs/v04r00/access/shapefile/IBTrACS.NA.list.v04r00.lines.zip"

if(!"IBTrACS.NA.list.v04r00.lines.zip" %in% list.files(here::here("data"))) {
  download.file(url = L,
                destfile = here::here("data",
                                      "IBTrACS.NA.list.v04r00.lines.zip"))
  unzip(here::here("data", "IBTrACS.NA.list.v04r00.lines.zip"),
        exdir = here::here("data"))
}

Tracks.sf <- st_read(dsn = here::here("data"), 
                     layer = "IBTrACS.NA.list.v04r00.lines") |>
  st_transform(crs = 32616)

Tracks.sf <- Tracks.sf |>
  filter(year >= begin_year & year <= end_year) |>
  filter(USA_WIND >= storm_intensity) |>
  select(SID, SEASON, year, month, day, hour, min,
         NAME, SUBBASIN, ISO_TIME, USA_WIND, USA_PRES, USA_RMW, USA_EYE, USA_ROCI)

# Optional summary of mean sizes in km
Tracks.sf |>
  st_drop_geometry() |>
  summarize(avgRMW = mean(USA_RMW, na.rm = TRUE) * 1.852,
            avgEYE = mean(USA_EYE, na.rm = TRUE) * 1.852,
            avgROCI = mean(USA_ROCI, na.rm = TRUE) * 1.852)

# Efficient fill of missing USA_RMW using data.table (pedigree -> pressure)
Tracks_dt <- as.data.table(st_drop_geometry(Tracks.sf))
rmw_by_sid  <- Tracks_dt[, .(rmw_sid_mean  = mean(USA_RMW, na.rm = TRUE)), by = SID]
rmw_by_pres <- Tracks_dt[, .(rmw_pres_mean = mean(USA_RMW, na.rm = TRUE)), by = USA_PRES]
Tracks_dt <- rmw_by_sid[Tracks_dt, on = "SID"]
Tracks_dt <- rmw_by_pres[Tracks_dt, on = "USA_PRES"]
# First try SID mean, then pressure mean if still NA
Tracks_dt[, USA_RMW := fifelse(is.na(USA_RMW), rmw_sid_mean, USA_RMW)]
Tracks_dt[, USA_RMW := fifelse(is.na(USA_RMW), rmw_pres_mean, USA_RMW)]
# Update back
Tracks.sf$USA_RMW <- Tracks_dt$USA_RMW

# Buffer to make wind swaths (meters)
Swaths.sf <- st_buffer(Tracks.sf, dist = Tracks.sf$USA_RMW * 1852)

# Florida boundary and spatially indexed filter
Boundaries.sf <- USAboundaries::us_states(resolution = "low", states = "FL") |>
  st_transform(crs = 32616)
Swaths.sf <- st_filter(Swaths.sf, Boundaries.sf)
Swaths.sf <- Swaths.sf |>
  mutate(Date = lubridate::as_date(ISO_TIME))

# Dissolve per-storm swaths (union) only once in projected CRS; then back to 4326
sf_use_s2(FALSE)
Swaths_valid <- st_make_valid(Swaths.sf)
Swaths_proj  <- st_transform(Swaths_valid, 5070)  # NAD83 / Conus Albers
Swaths_union <- Swaths_proj |>
  group_by(SID) |>
  summarize(Date0 = first(Date),
            NAME = first(NAME),
            Wind = max(USA_WIND),
            geometry = st_union(geometry), .groups = "drop") |>
  mutate(Storm_Category = case_when(
    Wind >= 34 & Wind <= 63 ~ 0,
    Wind >= 64 & Wind <= 82 ~ 1,
    Wind >= 83 & Wind <= 95 ~ 2,
    Wind >= 96 & Wind <= 112 ~ 3,
    Wind >= 113 & Wind <= 136 ~ 4,
    Wind >= 137 ~ 5
  ))
Swaths_union <- st_transform(Swaths_union, 4326)
sf_use_s2(TRUE)

# Expand by date (±n days) without rowwise/list-columns
n_new  <- 1L   # Number of days before & after impact
offsets <- -n_new:n_new

Swaths_dt <- as.data.table(st_drop_geometry(Swaths_union))
setkey(Swaths_dt, SID)
exp_dates <- CJ(SID = Swaths_dt$SID, offset = offsets)[
  Swaths_dt[, .(SID, Date0, NAME, Storm_Category)], on = "SID"
][, Date := Date0 + offset][, offset := NULL]
exp_dates[, Storm_Effect := fifelse(Date == Date0, "Impact",
                             fifelse(Date < Date0, "Threat", "Cleanup"))]

# Reattach geometry
Swaths_geom <- Swaths_union |>
  select(SID, geometry)
TIC.sf <- left_join(exp_dates, st_drop_geometry(Swaths_geom), by = "SID") |>
  st_as_sf(crs = 4326)

# Add month/year indicators and standard geometry name
TIC.sf <- TIC.sf |>
  mutate(Month = month(Date),
         Year = year(Date))
st_geometry(TIC.sf) <- "geom"
```

---

## Build a ZIP × Date effect calendar (optimized)

```{r}
library(tigris)
options(tigris_use_cache = TRUE)

# Florida ZCTAs
zctas_fl <- zctas(cb = FALSE, year = 2010, state = "FL") %>%
  select(zip = ZCTA5CE10, geometry)

# Align CRS
tic_aligned <- st_transform(TIC.sf,  st_crs(zctas_fl))

# Spatial join: which zips intersect each TIC polygon (by date & effect)
# Keep only columns we need from TIC
tic_keep <- tic_aligned |>
  select(Date, Storm_Effect, geom)

zcta_date_effect_sf <- st_join(
  zctas_fl,
  tic_keep,
  join = st_intersects,
  left = FALSE   # keep only zips that intersect a TIC polygon on that date
)

zcta_date_effect <- as.data.table(st_drop_geometry(zcta_date_effect_sf))
setnames(zcta_date_effect, c("zip","Date","Storm_Effect"),
         c("zip","date","effect"))

# Normalize types & key
zcta_date_effect[, `:=`(
  zip    = as.character(zip),
  date   = as.IDate(date),
  effect = as.character(effect)
)]
setkey(zcta_date_effect, zip, date)

# Resolve multiple effects (rank)
sev <- data.table(effect = c("None","Cleanup","Threat","Impact"),
                  rank   = 0:3)
setkey(sev, effect)
zde <- zcta_date_effect[sev, on="effect"][, .SD[which.max(rank)], by=.(zip, date)][
  , .(zip, date, effect)]

# Build full ZIP–date grid & fill None
all_zips  <- sort(unique(zde$zip))
all_dates <- as.IDate(seq(as.Date("2004-12-31"), as.Date("2022-12-31"), by = "day"))
grid <- CJ(zip = all_zips, date = all_dates)
setkey(grid, zip, date); setkey(zde, zip, date)

effect_calendar <- zde[grid]
effect_calendar[is.na(effect), effect := "None"]

effect_calendar[, effect := factor(effect, levels = c("None","Cleanup","Threat","Impact"))]
stopifnot(effect_calendar[, .N, by = .(zip, date)][, all(N == 1)])
```

---

## Merge storms with death records (optimized)

```{r}
start_time <- Sys.time()
load("data/all_data.Rdata")

Deaths.df <- all_data %>%
  mutate(Date = as_date(DATE_OF_DEATH)) %>%
  filter(Date >= as.Date("1981-01-01")) %>%
  select(Death_ID = ID, Death_Date = Date, final_lon, final_lat, 
         age = COMPUTED_AGE, white = RACE_WHITE, sex = SEX, 
         marital_code = MARITAL_CODE, cardio_code = ICD_CODE, army = ARMY_YESNO)

Deaths.sf <- Deaths.df %>%
  st_as_sf(coords = c("final_lon", "final_lat"), crs = 4326)
rm(all_data, Deaths.df)
Sys.time() - start_time
```

### Filter deaths by date of death and storm effect zones (batched by date)

```{r}
# Work in a projected CRS
target_crs <- 5070  # NAD83 / Conus Albers
sf_use_s2(FALSE)

TIC_proj    <- st_transform(TIC.sf, target_crs)
Deaths_proj <- st_transform(Deaths.sf, target_crs)

# Data tables for fast date partitioning
Deaths_dt <- as.data.table(st_drop_geometry(Deaths_proj))
Deaths_dt[, Death_Date := as.IDate(Death_Date)]

TIC_dt <- as.data.table(st_drop_geometry(TIC_proj))
TIC_dt[, Date := as.IDate(Date)]

dates <- sort(intersect(unique(Deaths_dt$Death_Date), unique(TIC_dt$Date)))

message(sprintf("Processing %d unique dates", length(dates)))

# Batched spatial joins per date
results_list <- vector("list", length(dates))
for (i in seq_along(dates)) {
  d <- dates[i]
  pts_idx <- which(Deaths_dt$Death_Date == d)
  polys_idx <- which(TIC_dt$Date == d)
  if (length(pts_idx) == 0L || length(polys_idx) == 0L) {
    results_list[[i]] <- NULL
    next
  }
  pts   <- Deaths_proj[pts_idx, ]
  polys <- TIC_proj[polys_idx, c("NAME","Storm_Category","Storm_Effect")]
  out <- try(suppressWarnings(st_join(pts, polys, join = st_within, left = FALSE)), silent = TRUE)
  if (inherits(out, "try-error")) {
    results_list[[i]] <- NULL
  } else {
    results_list[[i]] <- out
  }
}

deaths_with_impacts <- do.call(rbind, results_list)
```

### Collapse to one row per death (max storm category)

```{r}
impacts_max_cat <- deaths_with_impacts %>%
  st_drop_geometry() %>%
  group_by(Death_ID, Death_Date) %>%
  slice_max(Storm_Category, n = 1, with_ties = FALSE) %>%
  ungroup()

impacts_keep <- impacts_max_cat %>%
  select(Death_ID, Death_Date, Storm_Name = NAME, Storm_Category, Storm_Effect)

deaths_all_effects <- Deaths.sf %>%
  left_join(impacts_keep, by = c("Death_ID", "Death_Date")) %>%
  mutate(Storm_Effect = replace_na(Storm_Effect, "None"))
```

### Export (optional)

```{r}
# CSV
st_drop_geometry(deaths_all_effects) %>%
  readr::write_csv("data/outputs/Deaths/All_Deaths_Storm_Effects_34kt_1day.csv")

# GeoPackage
st_write(
  deaths_all_effects,
  dsn = "data/outputs/Deaths/All_Deaths_Storm_Effects_34kt_1day.gpkg",
  layer = "All_Deaths_Storm_Effects_34kt_1day",
  delete_dsn = TRUE
)
```

---

## Analytics of deaths and storms

```{r}
library(dplyr)
library(data.table)

Deaths.sf <- deaths_all_effects 
Deaths.df <- Deaths.sf %>% st_drop_geometry()
Deaths.dt <- as.data.table(Deaths.df)
```

### Statewide daily deaths by effect

```{r}
# Range of dates with storm effect
range <- Deaths.df |>
  filter(Storm_Effect != "None")
range(range$Death_Date)

# Group deaths by date & storm effect
daily_deaths <- Deaths.df |>
  group_by(Death_Date, Storm_Effect) |>
  summarise(deaths = n(), .groups = "drop")

# Number of days per effect
num_days <- daily_deaths |>
  distinct(Death_Date, Storm_Effect) |>
  count(Storm_Effect, name = "num_days")

# Total deaths per effect
total_deaths <- daily_deaths |>
  group_by(Storm_Effect) |>
  summarise(total_deaths = sum(deaths), .groups = "drop")

# Daily death rate
death_rates <- left_join(total_deaths, num_days, by = "Storm_Effect") |>
  mutate(daily_death_rate = total_deaths / num_days)

death_rates
```

---

## Add zip codes to each death (optimized)

```{r}
# Inspect CRS
st_crs(zctas_fl)   # NAD83 (EPSG:4269)
st_crs(Deaths.sf)  # WGS84 (EPSG:4326)

# Work in projected CRS for join and optional simplification
target_crs <- 5070
zctas_fl_proj    <- st_transform(zctas_fl, target_crs)
Deaths_proj      <- st_transform(Deaths.sf, target_crs)
sf_use_s2(FALSE)

# Optional light simplification for faster spatial joins (tolerance in meters)
zctas_fl_proj_simpl <- st_simplify(zctas_fl_proj, dTolerance = 50)

begin <- Sys.time()
# Intersects first
deaths_with_zip <- st_join(
  Deaths_proj,
  zctas_fl_proj_simpl,
  join    = st_intersects,
  left    = TRUE,
  largest = TRUE
)

# Fallback to nearest polygon if NA
miss <- is.na(deaths_with_zip$zip)
if (any(miss)) {
  nn <- st_nearest_feature(Deaths_proj[miss, ], zctas_fl_proj_simpl)
  deaths_with_zip$zip[miss] <- zctas_fl_proj_simpl$zip[nn]
}
Sys.time() - begin

# Back to geographic CRS
deaths_with_zip <- st_transform(deaths_with_zip, 4326)
zctas_fl <- st_transform(zctas_fl_proj, 4269)

# Remove deaths outside any zip if desired
# deaths_with_zip <- deaths_with_zip %>% filter(!is.na(zip))
```

### Maps: death totals by ZIP & effect

```{r}
library(tidyr)

death_counts_wide <- deaths_with_zip %>%
  sf::st_drop_geometry() %>%  
  filter(Storm_Effect %in% c("Threat","Impact","Cleanup")) %>%
  count(zip, Storm_Effect, name = "deaths") %>%
  tidyr::pivot_wider(
    names_from  = Storm_Effect,
    values_from = deaths,
    values_fill = 0
  )

# Left join counts (preserves polygon geometry)
death_map_sf <- zctas_fl %>%
  left_join(death_counts_wide, by = "zip") %>%
  mutate(
    Threat = replace_na(Threat, 0L),
    Impact = replace_na(Impact, 0L),
    Cleanup = replace_na(Cleanup, 0L)
  ) %>%
  select(zip, Threat, Impact, Cleanup, geometry)
```

```{r}
library(ggplot2)
library(RColorBrewer)

bins <- c(0, 5, 10, 20, 40, 70, 100)
pal  <- brewer.pal(length(bins) - 1, "YlOrRd")

make_map <- function(data_sf, col, title = col) {
  ggplot(data_sf) +
    geom_sf(aes(fill = .data[[col]]), color = "white", size = 0.05) +
    scale_fill_stepsn(
      colors  = pal,
      breaks  = bins,
      limits  = range(bins),
      na.value = "#cccccc",
      name    = "Deaths per ZCTA"
    ) +
    labs(title = title) +
    coord_sf(datum = NA) +
    theme_void(base_size = 12) +
    theme(plot.title = element_text(face = "bold", hjust = 0.5))
}

p_impact <- make_map(death_map_sf, "Impact",  "Deaths on impact days")
p_impact

ggsave("figs/zcta_deaths_impact.png",  p_impact,  width = 7.2, height = 6, dpi = 300)
```

### Dates & storm names by ZIP/effect

```{r}
library(purrr)

df <- deaths_with_zip %>%
  st_drop_geometry()

dates_by_effect_df <- df %>%
  group_by(zip, Storm_Effect) %>%
  summarise(
    Death_Dates = list(sort(unique(Death_Date))),
    Storm_Names = list(unique(Storm_Name)),
    n_records   = n(),
    n_dates     = length(Death_Dates[[1]]),
    .groups = "drop"
  ) %>%
  arrange(zip, Storm_Effect)

length(unique(dates_by_effect_df$zip))
```

---

## Daily counts by ZIP & join to effect calendar

```{r}
daily_by_zip <- deaths_with_zip %>%
  st_drop_geometry() %>%
  transmute(
    zip    = as.character(zip),
    date   = as.Date(Death_Date),
    effect = as.character(Storm_Effect),
    age = as.integer(age),
    sex = sex,
    cc = cardio_code
  ) %>%
  # filter(cc %in% cardio_code) %>%  # uncomment to subset
  group_by(zip, date, effect) %>%
  summarise(deaths = n(), .groups = "drop")

# Join to the full calendar
dbz <- as.data.table(daily_by_zip)[ , .(
  zip   = as.character(zip),
  date  = as.IDate(date),
  deaths = as.integer(deaths)
)]
setkey(dbz, zip, date)

zip_day <- dbz[effect_calendar, on = .(zip, date)]
zip_day[is.na(deaths), deaths := 0L]
```

---

## Poisson model with stratum fixed effects (statewide)

```{r}
library(fixest)
library(broom)

dat <- zip_day %>%
  filter(month(date) %in% 5:11) %>%  # hurricane season only
  transmute(
    zip    = as.character(zip),
    date   = as.IDate(date),
    deaths = as.integer(deaths),
    effect = factor(effect, levels = c("None","Threat","Impact","Cleanup")),
    dow     = factor(strftime(date, "%u"), levels = as.character(1:7),
                     labels = c("Mon","Tue","Wed","Thu","Fri","Sat","Sun")),
    ym     = format(date, "%Y-%m")
  )

m_pooled <- fepois(
  deaths ~ i(effect, ref = "None") | zip + ym + dow,
  data    = dat,
  cluster = ~ zip + ym,
  notes = TRUE
)

summary(m_pooled)

# Rate ratios (RR) with CIs
rr_tbl <- tidy(m_pooled, conf.int = TRUE) %>%
  filter(grepl("^effect::", term)) %>%
  transmute(
    effect = sub("^effect::", "", term),
    RR     = exp(estimate),
    LCI    = exp(conf.low),
    UCI    = exp(conf.high),
    p      = p.value
  )
rr_tbl
```

### Negative binomial FE (sensitivity)

```{r}
m_pooled2 <- fenegbin(
  deaths ~ i(effect, ref = "None") | zip + ym + dow,
  data    = dat,
  cluster = ~ zip + ym
)
summary(m_pooled2)
```

---

## Time-stratified Poisson fixed-effect model per ZIP (optional)

```{r}
library(pbapply)

cands <- dat[
  , .(
      has_none = any(effect == "None"),
      has_tic  = any(effect != "None"),
      pos_y    = any(deaths > 0L)
    ),
  by = zip
][has_none & has_tic & pos_y, zip]

min_effect_days <- 5L
cands2 <- dat[
  , .(n_tic = sum(effect != "None")),
  by = zip
][n_tic >= min_effect_days, zip]

fit_one <- function(df) {
  df[, effect := factor(effect, levels = c("None","Threat","Impact","Cleanup"))]
  if (!any(df$effect == "None") || mean(df$deaths[df$effect == "None"]) == 0) return(NULL)
  tryCatch(
    fepois(
      deaths ~ i(effect, ref = "None") | ym + dow,
      data    = df,
      cluster = ~ ym,
      notes = FALSE
    ),
    error = function(e) NULL
  )
}

models <- pblapply(cands, function(z) fit_one(dat[zip == z]))
names(models) <- cands

rr_by_zip <- bind_rows(lapply(names(models), function(z) {
  m <- models[[z]]
  if (is.null(m)) return(NULL)
  tidy(m, conf.int = TRUE) %>%
    filter(grepl("^effect::", term)) %>%
    transmute(
      zip    = z,
      effect = sub("^effect::", "", term),
      RR     = exp(estimate),
      LCI    = exp(conf.low),
      UCI    = exp(conf.high),
      p      = p.value
    )
})) %>%
  filter(effect %in% c("Threat","Impact","Cleanup"))
```

### Empirical Bayes shrinkage

```{r}
pooled_eff <- broom::tidy(m_pooled, conf.int = FALSE) %>%
  filter(grepl("^effect::", term)) %>%
  transmute(effect = sub("^effect::","",term), mu0 = estimate)   # log-RR prior

log_rr <- rr_by_zip %>%
  mutate(
    logRR   = log(RR),
    se_log  = (log(UCI) - log(LCI)) / (2*1.96)
  ) %>% filter(is.finite(logRR), se_log > 0)

tau_by_eff <- log_rr %>% group_by(effect) %>%
  summarise(tau = stats::mad(logRR, constant = 1), .groups = "drop")

rr_shrunk <- log_rr %>%
  left_join(pooled_eff, by="effect") %>%
  left_join(tau_by_eff, by="effect") %>%
  mutate(
    prec_obs = 1/(se_log^2),
    prec_pri = 1/(pmax(tau, 1e-6)^2),
    post     = (logRR*prec_obs + mu0*prec_pri) / (prec_obs + prec_pri),
    RR_shr   = exp(post)
  ) %>%
  select(zip, effect, RR_shr)

# Threat-only save for story map
rr_shrunk_threat_only <- rr_shrunk %>%
  filter(effect == "Threat")
saveRDS(rr_shrunk_threat_only, "data/outputs/Results/zcta_results.rds")
```

---

## Hot spot analysis

```{r}
library(spdep)
library(dplyr)
library(sf)

# Prepare geometry for mapping shrinkage results
zctas_valid <- zctas_fl %>% st_make_valid()
rr_map_sf <- zctas_valid %>%
  rename(zip = ZCTA5CE10) %>%
  left_join(rr_shrunk %>% rename(zip = zip), by = "zip") %>%
  select(zip, Effect = RR_shr, geometry)

# Keep only rows with RR
g <- rr_map_sf %>%
  filter(!is.na(Effect)) %>%
  st_make_valid()

nb <- spdep::poly2nb(g, queen = TRUE)
lw <- spdep::nb2listw(nb, style = "W", zero.policy = TRUE)
RR_vec <- as.numeric(log(g$Effect))

lm <- spdep::localmoran_perm(
  x = RR_vec,
  listw = lw,
  nsim = 999,
  alternative = "two.sided",
  zero.policy = TRUE
)
lm <- as.data.frame(lm)
names(lm) <- c("Ii","E.Ii","Var.Ii","Z.Ii","p.value")
lm$p.adj <- p.adjust(lm$p.value, method = "fdr")

lagRR <- spdep::lag.listw(lw, g$Effect, zero.policy = TRUE)
alpha <- 0.35
mx <- mean(g$Effect, na.rm = TRUE)

cluster <- dplyr::case_when(
  g$Effect >= mx & lagRR >= mx & lm$Ii > 0 & lm$p.value < alpha ~ "High-High",
  g$Effect <= mx & lagRR <= mx & lm$Ii > 0 & lm$p.value < alpha ~ "Low-Low",
  g$Effect >= mx & lagRR <= mx & lm$Ii < 0 & lm$p.value < alpha ~ "High-Low",
  g$Effect <= mx & lagRR >= mx & lm$Ii < 0 & lm$p.value < alpha ~ "Low-High",
  TRUE ~ "Not Significant"
)

g_locI <- g %>%
  mutate(
    Ii       = lm$Ii,
    E_Ii     = lm$E.Ii,
    Var_Ii   = lm$Var.Ii,
    Z_I      = lm$Z.Ii,
    p_value  = lm$p.value,
    p_adj    = lm$p.adj,
    lag_RR   = lagRR,
    cluster  = factor(cluster,
                      levels = c("High-High","Low-Low","High-Low","Low-High","Not Significant"))
  )

Moran_out <- rr_map_sf %>%
  left_join(
    g_locI %>%
      st_drop_geometry() %>%
      select(zip, Ii, E_Ii, Var_Ii, Z_I, p_value, p_adj, lag_RR, cluster),
    by = "zip"
  ) %>%
  mutate(
    cluster = ifelse(is.na(cluster), "No data", as.character(cluster)),
    cluster = factor(cluster,
      levels = c("High-High","Low-Low","High-Low","Low-High","Not Significant","No data")
    )
  )
```

### Maps

```{r}
library(scales)

imax <- max(abs(g_locI$Ii), na.rm = TRUE)

ggplot(g_locI) +
  geom_sf(aes(fill = Ii), color = NA) +
  scale_fill_gradient2(
    low    = "#2b6cb0",
    mid    = "#f7f7f7",
    high   = "#e53e3e",
    midpoint = 0,
    limits = c(-imax, imax),
    oob = squish,
    name = "Local Moran's I"
  ) +
  coord_sf(datum = NA) +
  theme_void(base_size = 12) +
  theme(legend.position = "right")

pal <- c("High-High"="#e53e3e","Low-Low"="#2b6cb0",
         "High-Low" ="#f59e0b","Low-High"="#10b981",
         "Not Significant"="#d9d9d9","No data"="#cccccc")

clusters_impact <- ggplot(Moran_out) +
  geom_sf(aes(fill = cluster), color = NA) +
  scale_fill_manual(values = pal, drop = FALSE) +
  theme_void() + labs(fill = "Local Moran clusters")

ggsave("figs/clusters_impact.png",  clusters_impact,  width = 7.2, height = 6, dpi = 300)

# Save for story-map
rr_clusters <- g_locI %>% select(zip, Ii, geometry)
saveRDS(rr_clusters, "data/outputs/Results/zcta_clusters.rds")
```

---

## Figure 1: Study design and workflow

```{r}
library(ggplot2)
library(dplyr)
library(tibble)

boxes <- tibble(
  id = c(
    "deaths_geo",
    "storms",
    "zcta",
    "panel_all",
    "panel_restr",
    "state_model",
    "state_outputs",
    "zip_models",
    "zip_rrs",
    "zip_lisa"
  ),
  xmin = c(0.5, 3.0, 5.5, 2.0, 2.0, 0.5, 0.5, 4.5, 4.5, 4.5),
  xmax = c(2.5, 5.0, 7.5, 6.0, 6.0, 3.5, 3.5, 7.5, 7.5, 7.5),
  ymin = c(8.0, 8.0, 8.0, 6.5, 5.0, 3.5, 2.0, 3.5, 2.0, 0.5),
  ymax = c(9.0, 9.0, 9.0, 7.5, 6.0, 4.5, 3.0, 4.5, 3.0, 1.5)
) %>%
  mutate(x = (xmin + xmax) / 2, y = (ymin + ymax) / 2)

label_map <- c(
  deaths_geo = "Vital records\nGeocoded deaths\n(point-level, daily)",
  storms     = "Storm data\nIBTrACS + windfields\n(Threat/Impact/Cleanup)",
  zcta       = "ZCTA/ZIP boundaries\nPopulation & covariates",
  panel_all  = "ZIP–day mortality panel\nTagged by storm effect\n(1985–2022)",
  panel_restr = "Analysis sample\n1985–2022, May–Nov\nFlorida ZIP–day deaths",
  state_model = "Statewide time-stratified\nfixed-effects Poisson\n(daily deaths ~ storm effect\n+ calendar/time strata)",
  state_outputs = "Statewide rate ratios (RRs)\nThreat / Impact / Cleanup vs None\nEvent-time profiles",
  zip_models = "ZIP-level Poisson models\n(deaths ~ storm effect\n+ time fixed effects)",
  zip_rrs = "Shrunken ZIP-level RRs\nby storm phase\n(Threat / Impact / Cleanup)",
  zip_lisa = "Spatial clustering of RRs\nLocal Moran's I\nHot and cold spot maps"
)

boxes$plot_label <- label_map[boxes$id]

arrows <- tibble(
  from = c("deaths_geo","storms","zcta","panel_all","panel_all","panel_restr","panel_restr","state_model","zip_models","zip_rrs"),
  to   = c("panel_all","panel_all","panel_all","panel_restr","panel_restr","state_model","zip_models","state_outputs","zip_rrs","zip_lisa")
) %>%
  left_join(boxes %>% select(id, x, y), by = c("from" = "id")) %>%
  rename(xstart = x, ystart = y) %>%
  left_join(boxes %>% select(id, x, y), by = c("to" = "id")) %>%
  rename(xend = x, yend = y) %>%
  mutate(ystart = case_when(
    from == "deaths_geo" ~ ystart - 0.1,
    from == "storms"     ~ ystart,
    from == "zcta"       ~ ystart + 0.1,
    TRUE ~ ystart
  ))

p_fig1 <- ggplot() +
  geom_curve(
    data = arrows,
    aes(x = xstart, y = ystart, xend = xend, yend = yend),
    curvature = 0.1,
    arrow = arrow(length = unit(0.15, "inches")),
    linewidth = 0.4
  ) +
  geom_rect(
    data = boxes,
    aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax),
    fill = "white",
    color = "black",
    linewidth = 0.4
  ) +
  geom_text(
    data = boxes,
    aes(x = x, y = y, label = plot_label),
    size = 3.3,
    lineheight = 0.9
  ) +
  coord_fixed(xlim = c(0, 8), ylim = c(0, 9), expand = FALSE) +
  theme_void() +
  theme(
    plot.margin = margin(10, 10, 10, 10),
    panel.background = element_rect(fill = "white", color = NA)
  )

p_fig1
```

---

## Figure 2: Coefficient plot

```{r}
library(broom)
library(dplyr)
library(stringr)
library(ggplot2)

tt <- tidy(m_pooled, conf.int = TRUE)

rr_df <- tt %>%
  filter(str_detect(term, "^effect::")) %>%
  mutate(
    storm_phase = str_remove(term, "^effect::"),
    storm_phase = factor(storm_phase, levels = c("Threat", "Impact", "Cleanup")),
    RR      = exp(estimate),
    RR_low  = exp(conf.low),
    RR_high = exp(conf.high)
  )

p_rr <- ggplot(rr_df, aes(y = storm_phase, x = RR)) +
  geom_vline(xintercept = 1, linetype = "dashed", color = "grey50") +
  geom_point(size = 3) +
  geom_errorbarh(aes(xmin = RR_low, xmax = RR_high), height = 0.2, linewidth = 0.6) +
  labs(
    x = "Rate ratio (RR)", y = "Storm effect",
    title = "Statewide rate ratios by storm effect",
    subtitle = "Threat, Impact, and Cleanup relative to None\nFixed-effects Poisson (ZIP, year–month, DOW FE)"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    panel.grid.minor = element_blank(),
    plot.title = element_text(face = "bold", hjust = 0.5),
    plot.subtitle = element_text(hjust = 0.5)
  )

p_rr

ggsave("figs/state_rr_by_effect.png", p_rr,  width = 7.2, height = 6, dpi = 300)
```

---

## Table 1: Coefficients

```{r}
library(broom)
library(dplyr)
library(stringr)

tt <- tidy(m_pooled, conf.int = TRUE)

rr_tbl <- tt %>%
  filter(str_detect(term, "^effect::")) %>%
  mutate(
    phase = str_remove(term, "^effect::"),
    RR      = exp(estimate),
    RR_low  = exp(conf.low),
    RR_high = exp(conf.high),
    sig     = p.value < 0.05
  ) %>%
  transmute(
    `Storm phase` = phase,
    RR_CI = ifelse(
      sig,
      sprintf("\\textbf{%.2f (%.2f, %.2f)}", RR, RR_low, RR_high),
      sprintf("%.2f (%.2f, %.2f)", RR, RR_low, RR_high)
    ),
    `p-value` = sprintf("%.3f", p.value)
  )
```

```{r}
library(knitr)
library(kableExtra)

kable(
  rr_tbl,
  format = "latex",
  booktabs = TRUE,
  escape = FALSE,
  align = c("l","c","c"),
  col.names = c("Storm phase", "Rate ratio (95\\% CI)", "p-value")
) %>%
  kable_styling(
    latex_options = c("hold_position"),
    font_size = 9
  ) %>%
  add_header_above(
    c(" " = 1, "Effect estimate" = 2),
    bold = TRUE
  ) %>%
  footnote(
    general = "Rate ratios from fixed-effects Poisson model of daily deaths. Reference category is None days. Fixed effects for ZIP, year–month, and day-of-week. Standard errors clustered by ZIP and year–month.",
    general_title = "",
    threeparttable = TRUE
  )
```

---

## Figure 3: ZIP-level maps (RRs)

```{r}
library(dplyr)
library(sf)
library(ggplot2)
library(RColorBrewer)
library(tidyr)

zip_sf <- zctas_fl %>% rename(zip = ZCTA5CE10) %>% st_make_valid()
rr_map <- zip_sf %>%
  left_join(rr_by_zip, by = "zip") %>%
  mutate(
    logRR  = log(RR),
    logLCI = log(LCI),
    logUCI = log(UCI),
    logCIwidth = abs(logUCI - logLCI),
    mask = ifelse(is.na(RR) | is.na(logRR) | logCIwidth > 0.7, TRUE, FALSE),
    effect = factor(effect, levels = c("Threat", "Impact", "Cleanup"))
  )

# (Customize palette & plotting as needed)
```
