---
title: "All-Cause Mortality & Storms"
output: html_document
editor_options:
  chunk_output_type: console
---

Experimental setup. We assemble a multi-million–record, geocoded dataset of all-cause deaths (simple-feature points in WGS84) and tag each death day as one of four “storm effect” states: None, Threat, Impact, or Cleanup. We first aggregate to daily counts by effect, compute the number of effect-days, and fit a Poisson regression of total deaths on storm effect with an offset for log(effect-day count), using “None” as the reference; coefficients are then exponentiated to rate ratios with confidence intervals. Next, for spatial detail we intersect deaths with Florida ZCTAs (2020 vintage), refit the same Poisson model separately within each ZCTA (with basic data sufficiency checks), map rate ratios for Cleanup/Impact/Threat, and run a local hot-spot analysis on the Threat rate ratios using k-nearest neighbors (k=7) and Local Moran’s I to identify hot/cold clusters.

Results. Daily death rates are higher on storm-effect days than on non-storm days (≈416–419 vs ≈402 deaths/day in this sample). The Poisson model quantifies small but statistically precise excesses: Rate ratios ≈ 1.043 (Threat, 95% CI 1.028–1.058), 1.037 (Impact, 1.022–1.051), and 1.034 (Cleanup, 1.020–1.049), all p < 0.001 relative to None. Spatial modeling at the zip code tabulation area level produces choropleths of these rate ratios, and the Local Moran’s I highlights coherent pockets of elevated (“hot”) and depressed (“cold”) Threat ratios, indicating geographic clustering in storm-related mortality risk. Taken together, the analysis suggests modest but reliable short-run increases in all-cause mortality around storm exposure, with non-uniform spatial patterns across Florida.

## Analytics of deaths and storms

Read the data table to a csv file
```{r}
Deaths.dt <- data.table::fread(here::here("data", "outputs", "All_Deaths_Storm_Effects.csv"))
Deaths.df <- Deaths.dt |>
  as.data.frame()
Deaths.sf <- sf::st_read(dsn = "data/outputs/Deaths", 
                         layer = "All_Deaths_Storm_Effects")
names(Deaths.sf)[1:5] <- names(Deaths.df)[1:5] # match column names with those in the csv
```

```{r}
library(dplyr)

#Deaths.df <- Deaths.df |>
#  mutate(SH = Storm_Category > 0)

total_deaths <- nrow(Deaths.df)
exposed_total <- Deaths.dt[Storm_Effect %in% c("Cleanup", "Impact", "Threat"), .N]

# Calculate death counts and rates for specified storm effects
death_rates <- Deaths.df |>
  filter(Storm_Effect %in% c("Cleanup", "Impact", "Threat")) |>
  group_by(Storm_Effect) |>
#  group_by(Storm_Effect, SH) |>
  summarise(Deaths = n(), .groups = "drop") |>
  mutate(Death_Rate = Deaths / exposed_total)

# View result
print(death_rates)
```

This tells you, for example, that .333% of all deaths occurred under an Impact storm condition.

## Death rates per day

Get number of deaths per day by Storm_Effect
```{r}
# Group deaths by date and storm effect
daily_deaths <- Deaths.df |>
#  group_by(Death_Date, Storm_Effect, SH) |>
  group_by(Death_Date, Storm_Effect) |>
  summarise(deaths = n(), .groups = "drop")
```

Get number of days for each storm effect. This assumes Storm_Effect applies to the whole day — i.e., each Death_Date has a unique Storm_Effect
```{r}
num_days <- daily_deaths |>
#  distinct(Death_Date, Storm_Effect, SH) |>
  distinct(Death_Date, Storm_Effect) |>
#  count(Storm_Effect, SH, name = "num_days")
  count(Storm_Effect, name = "num_days")
```

Total deaths per storm effect
```{r}
total_deaths <- daily_deaths |>
#  group_by(Storm_Effect, SH) |>
  group_by(Storm_Effect) |>
  summarise(total_deaths = sum(deaths), .groups = "drop")
```

Calculate daily death rate
```{r}
# death_rates <- left_join(total_deaths, num_days, by = c("Storm_Effect", "SH")) |>
death_rates <- left_join(total_deaths, num_days, by = "Storm_Effect") |>
  mutate(daily_death_rate = total_deaths / num_days)

print(death_rates)
```

This tells us that in this sample the daily death rate is somewhat higher on storm-effect days than on non-storm-effect days

Test it statistically using a Poisson regression model with the reference level being a non-storm-effect day
```{r}
model_data <- left_join(total_deaths, num_days, by = "Storm_Effect")
model_data$Storm_Effect <- relevel(factor(model_data$Storm_Effect), ref = "None")

poisson_model <- glm(total_deaths ~ Storm_Effect + offset(log(num_days)), data = model_data, family = poisson())
summary(poisson_model)
```

This says that given the sample of data and a Poisson model, although the effect size is small mortality is significantly higher on storm-effect days compared to the reference level. Note that this does not model the data at the daily time level so it does not account for possible dispersion, or control for seasonality and trend. 

Create a nice table of the model results. We exponentiate the model coefficients to express them as a rate ratio and add confidence intervals
```{r}
# Extract coefficients and 95% confidence intervals
coefs <- coef(summary(poisson_model))
confint_vals <- confint(poisson_model)  # May take a few seconds

# Build data frame of results
rate_ratios <- data.frame(
  Term = rownames(coefs),
  Estimate = coefs[, "Estimate"],
  Std_Error = coefs[, "Std. Error"],
  z_value = coefs[, "z value"],
  p_value = coefs[, "Pr(>|z|)"],
  RR = exp(coefs[, "Estimate"]),
  RR_lower = exp(confint_vals[, 1]),
  RR_upper = exp(confint_vals[, 2])
)

# Clean up names
rownames(rate_ratios) <- NULL
rate_ratios <- rate_ratios |>
  dplyr::select(Term, RR, RR_lower, RR_upper, p_value)

# Print the table
options(digits = 4)
print(rate_ratios)
```

## Spatialize results using zip code areas ZCTAs

Get Florida ZCTAs
```{r}
library(tigris)
library(sf)
options(tigris_use_cache = TRUE)

# Get national ZCTAs and subset to those intersecting Florida
zctas_all <- zctas(cb = TRUE, starts_with = NULL, year = 2020)
florida <- states(cb = TRUE) %>% filter(STUSPS == "FL")

# Filter ZCTAs that intersect Florida
zctas_fl <- st_intersection(zctas_all, st_transform(florida, st_crs(zctas_all)))
zctas_fl <- st_make_valid(zctas_fl) # in case of invalid geometries
```

Join deaths to ZCTAs
```{r}
Deaths.sf <- st_transform(Deaths.sf, st_crs(zctas_fl))  # Ensure CRS matches
deaths_with_zcta <- st_join(Deaths.sf, zctas_fl["GEOID20"])  # Join only GEOID20
```

Check out specific zip codes, storms, etc
```{r}
X32778 <- deaths_with_zcta |>
  st_drop_geometry() |>
  dplyr::filter(GEOID20 == "32778") |>
  dplyr::filter(!Storm_Effect == "None")
```

List death dates & storm names by ZCTA and storm effect
```{r}
library(purrr)

df <- deaths_with_zcta %>%
  st_drop_geometry()

# A compact tibble with a list-column of Death_Dates
dates_by_effect_df <- df %>%
  group_by(GEOID20, Storm_Effect) %>%
  summarise(
    Death_Dates = list(sort(unique(Death_Date))),   # the actual unique dates (could be more than 1 death)
    Storm_Names = list(unique(Storm_Name)),         # storm names
    n_records   = n(),                               # total records
    n_dates     = length(Death_Dates[[1]]),          # unique dates
    .groups = "drop"
  ) %>%
  arrange(GEOID20, Storm_Effect)

dates_by_effect_df

# Threat only
dates_by_threat_df <- dates_by_effect_df %>%
  filter(Storm_Effect == "Threat")
```

Fit Poisson models at each ZCTA
```{r}
library(dplyr)
library(purrr)
library(rlang)

deaths_with_zcta$Storm_Effect <- relevel(factor(deaths_with_zcta$Storm_Effect), ref = "None")

models_by_zcta <- deaths_with_zcta %>%
  st_drop_geometry() %>%
  group_split(GEOID20) %>%
  map_df(function(df) {
    if (nrow(df) < 30 || n_distinct(df$Storm_Effect) < 2) return(NULL)

    daily_counts <- df %>%
      group_by(Death_Date, Storm_Effect) %>%
      summarise(deaths = n(), .groups = "drop") %>%
      group_by(Storm_Effect) %>%
      summarise(
        total_deaths = sum(deaths),
        num_days = n_distinct(Death_Date),
        .groups = "drop"
      )

    if (any(daily_counts$num_days == 0)) return(NULL)

    model <- try(glm(total_deaths ~ Storm_Effect + offset(log(num_days)),
                     data = daily_counts, family = poisson()), silent = TRUE)
    if (inherits(model, "try-error")) return(NULL)

    rr <- exp(coef(model))
    tibble(
      GEOID20 = df$GEOID20[1],
      RR_Cleanup = rr["Storm_EffectCleanup"] %||% NA,
      RR_Impact  = rr["Storm_EffectImpact"] %||% NA,
      RR_Threat  = rr["Storm_EffectThreat"] %||% NA
    )
  })
```

Join RRs to ZCTA geometry
```{r}
zcta_results <- left_join(zctas_fl, models_by_zcta, by = "GEOID20")

# Make polygons Leaflet-ready (lightweight & clean)
FL_ALBERS <- 3086L
zcta_results_poly <- zcta_results |>
  dplyr::select(GEOID20, RR_Cleanup, RR_Impact, RR_Threat, geometry) |>
  st_transform(FL_ALBERS) |>
  st_simplify(dTolerance = 75, preserveTopology = TRUE) |>
  st_transform(4326)
saveRDS(zcta_results_poly, "data/outputs/Results/zcta_results.rds")

#sf::st_write(zcta_results, "data/outputs/Results/zcta_results.shp")
```

```{r}
library(rlang)

dat <- models_by_zcta
#rr_cols <- c("RR_Cleanup", "RR_Impact", "RR_Threat")
rr_cols <- c("RR_Threat")

dat_ranked <- dat %>%
  # ensure the RR columns are numeric (optional but handy)
  mutate(across(all_of(rr_cols), as.numeric)) %>%
  # overall scores
  mutate(
    rr_mean = rowMeans(across(all_of(rr_cols)), na.rm = TRUE),
    rr_geom = exp(rowMeans(log(across(all_of(rr_cols))), na.rm = TRUE)) # tie-breaker
  ) %>%
  arrange(desc(rr_mean), desc(rr_geom))

top10 <- dat_ranked %>%
  slice_head(n = 10) %>%
  select(GEOID20, all_of(rr_cols), rr_mean, rr_geom)

bottom10 <- dat_ranked %>%
  arrange(rr_mean, rr_geom) %>%
  slice_head(n = 10) %>%
  select(GEOID20, all_of(rr_cols), rr_mean, rr_geom)

top10
bottom10

top_bottom_zctas_by_threat_rates <- c(top10$GEOID20, bottom10$GEOID20)

top_bottom_dates_by_threat.df <- dates_by_threat_df %>%
  filter(GEOID20 %in% top_bottom_zctas_by_threat_rates)

dates_by_threat_df$Death_Dates[dates_by_threat_df$GEOID20 == "32750"]

```


Plot all 3 RR maps in a loop
```{r}
library(ggplot2)
library(purrr)
library(rlang)

rr_vars <- c("RR_Cleanup", "RR_Impact", "RR_Threat")
titles <- c("RR: Cleanup vs None", "RR: Impact vs None", "RR: Threat vs None")

walk2(rr_vars, titles, function(rr_var, plot_title) {
  p <- ggplot(zcta_results) +
    geom_sf(aes(fill = !!sym(rr_var)), color = "gray70", size = 0.1) +
    scale_fill_gradient2(
      low = "blue", mid = "white", high = "red",
      midpoint = 1, na.value = "gray90", name = rr_var
    ) +
    theme_minimal() +
    labs(title = plot_title)
  
  print(p)
  ggsave(paste0(rr_var, "_ZCTA_map.png"), plot = p, width = 8, height = 6)
})
```

## Hot spot analysis

Using Moran's I and k-nearest neighbors for spatial contiguity
```{r}
rr_sf <- zcta_results %>%
  filter(!is.na(RR_Threat)) %>%
  select(GEOID20, RR = RR_Threat, geometry)

rr_sf <- rr_sf %>%
  filter(st_geometry_type(geometry) %in% c("POLYGON", "MULTIPOLYGON")) %>%
  st_cast("MULTIPOLYGON") # Ensure polygon geometry
```

Create spatial weights
```{r}
centroids <- st_centroid(rr_sf)
coords <- st_coordinates(centroids)
# Choose k (e.g., 5 or 8 neighbors)
k <- 7
knn <- spdep::knearneigh(coords, k = k)
nb <- spdep::knn2nb(knn)

# Convert to spatial weights
lw <- spdep::nb2listw(nb, style = "W")
```

Run local hot spot
```{r}
# Calculate Local Moran’s I
lisa <- spdep::localmoran(rr_sf$RR, lw)

# Add results to the sf object
rr_sf <- rr_sf %>%
  mutate(
    local_i = lisa[, 1],         # Local Moran's I
    p_value = lisa[, 5],         # p-value
    hotspot_type = case_when(
      p_value < 0.15 & RR > mean(RR, na.rm = TRUE) ~ "Hot Spot",
      p_value < 0.15 & RR < mean(RR, na.rm = TRUE) ~ "Cold Spot",
      TRUE ~ "Not Significant"
    )
  )

saveRDS(rr_sf, "data/outputs/Results/hot_spot_threat.rds")
```

```{r}
dat <- rr_sf

top10 <- dat %>%
  filter(hotspot_type == "Hot Spot") %>%
  arrange(desc(RR)) %>%
  slice_head(n = 10) %>%
  select(GEOID20, RR, hotspot_type)
top10

bottom10 <- dat %>%
  filter(hotspot_type == "Cold Spot") %>%
  arrange(desc(RR)) %>%
  slice_tail(n = 10) %>%
  select(GEOID20, RR, hotspot_type)
bottom10

top10
bottom10
```

Map the results
```{r}
library(ggplot2)

ggplot(rr_sf) +
  geom_sf(aes(fill = hotspot_type), color = "gray90", size = 0.1) +
  scale_fill_manual(values = c(
    "Hot Spot" = "red",
    "Cold Spot" = "blue",
    "Not Significant" = "gray90"
  )) +
  labs(title = "Local Hot Spot Analysis (Threat)", fill = "Cluster Type") +
  theme_minimal()
```


## Emergency room visits

Data received from David Hsu on October 16, 2025 via email
```{r}
ER_visits.dt <- data.table::fread(here::here("data", "Combined_ER_Data.csv"))
ER_visits.df <- ER_visits.dt |>
  as.data.frame()
```

Clean ER data: pad zips, ensure Date class
```{r}
er_clean <- ER_visits.df %>%
  mutate(
    Date    = as.Date(Date),
    ZipCode = str_pad(as.character(ZipCode), width = 5, pad = "0")
  )
```

Expand list of dates per GEOID20 × Storm_Effect, then join to ER data
```{r}
er_match_long <- dates_by_effect_df %>%
  select(GEOID20, Storm_Effect, Death_Dates, n_dates) %>%
  unnest_longer(Death_Dates, values_to = "Date") %>%  # one row per death date
  rename(Death_Date = Date) %>%
  left_join(
    er_clean,
    by = c("GEOID20" = "ZipCode", "Death_Date" = "Date")
  )
```

Compute averages per GEOID20 × Storm_Effect
```{r}
er_avg_by_effect <- er_match_long %>%
  group_by(GEOID20, Storm_Effect) %>%
  summarise(
    avg_er_visits = if (all(is.na(Total_Visits))) NA_real_ else mean(Total_Visits, na.rm = TRUE),
    n_er_rows     = sum(!is.na(Total_Visits)),
    n_dates       = first(n_dates),
    coverage      = ifelse(n_dates > 0, n_er_rows / n_dates, NA_real_),
    .groups = "drop"
  )
```

Attach the averages back to dates_by_effect_df
```{r}
dates_by_effect_with_er <- dates_by_effect_df %>%
  left_join(er_avg_by_effect, by = c("GEOID20", "Storm_Effect"))

dates_by_effect_with_er %>%
  group_by(Storm_Effect) %>%
  summarise(Averages = mean(avg_er_visits, na.rm = TRUE))
```

## Confirm (or challenge) results of the mortality rates using ER visit data

Build ER effect-day counts (per ZIP × day)
```{r}
library(dplyr); library(tidyr); library(stringr); library(lubridate)

# ER: make sure it's daily per ZIP
er <- ER_visits.df %>%
  mutate(Date = as.Date(Date),
         GEOID20 = str_pad(as.character(ZipCode), width = 5, side = "left", pad = "0")) %>%
  group_by(GEOID20, Date) %>%
  summarise(Total_Visits = sum(Total_Visits), .groups = "drop")

# Storm effect calendar: expand your list-column to a daily calendar
cal <- dates_by_effect_df %>%
  select(GEOID20, Storm_Effect, Death_Dates) %>%
  unnest_longer(Death_Dates, values_to = "Date") %>%
  mutate(flag = 1L) %>%
  pivot_wider(names_from = Storm_Effect, values_from = flag, values_fill = 0L)

# Join ER with the calendar (non-matches => None)
er_cal <- er %>%
  right_join(
    expand_grid(GEOID20 = unique(cal$GEOID20), Date = seq(min(cal$Date), max(cal$Date), by="day")),
    by = c("GEOID20","Date")
  ) %>%
  left_join(cal, by = c("GEOID20","Date")) %>%
  mutate(across(all_of(c("Impact","Threat","Cleanup","None")), ~replace_na(.x, 0L)),
         None = ifelse((Impact+Threat+Cleanup)==0, 1L, None),
         Total_Visits = coalesce(Total_Visits, 0L))   # or NA if you prefer strict coverage
```

Create long data frame of zcta_results
```{r}
mort_rr_long <- zcta_results %>%
  st_drop_geometry() %>%
  transmute(
    GEOID20 = str_pad(as.character(GEOID20), 5, side = "left", pad = "0"),
    RR_Impact  = as.numeric(RR_Impact),
    RR_Threat  = as.numeric(RR_Threat),
    RR_Cleanup = as.numeric(RR_Cleanup)
  ) %>%
  pivot_longer(
    cols = starts_with("RR_"),
    names_to = "Storm_Effect",
    names_pattern = "RR_(.*)",
    values_to = "RR_Mort"
  ) %>%
  filter(is.finite(RR_Mort)) %>%
  mutate(
    Storm_Effect = factor(Storm_Effect, levels = c("Impact","Threat","Cleanup"))
  )
```

Aggregate to weeks so most weeks exceed the threshold even if some days don’t. Model weekly totals with offsets for partial coverage and with storm-effect exposure defined as days per week.
```{r}
library(dplyr)
library(lubridate)
library(mgcv)
# weekly table, but add a compact time index
wk <- er_cal %>%
  mutate(week = floor_date(as.Date(Date), "week", week_start = 1)) %>%
  group_by(GEOID20, week) %>%
  summarise(
    visits_wk    = sum(as.numeric(Total_Visits), na.rm = TRUE),
    n_obs_days   = sum(!is.na(Total_Visits)),
    Impact_days  = sum((Impact  > 0L), na.rm = TRUE),
    Threat_days  = sum((Threat  > 0L), na.rm = TRUE),
    Cleanup_days = sum((Cleanup > 0L), na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(
    GEOID20     = factor(sprintf("%05s", as.character(GEOID20))),
    doy         = yday(week),                                   # 1..366
    tw          = as.integer(difftime(week, min(week), "weeks")),# small integer time
    offset_days = log(pmax(n_obs_days, 1L))
  )

wk_fit <- wk %>% filter(n_obs_days >= 3)

# knots for cyclic day-of-year smooth
knots_list <- list(doy = c(0.5, 366.5))

# Use bam() + discrete = TRUE (fast, memory-efficient) + fREML
m_wk <- bam(
  visits_wk ~ Impact_days + Threat_days + Cleanup_days +
    s(doy, bs = "cc", k = 12) +       # smaller basis is plenty weekly
    s(tw,  k = 30) +                  # trend on compact integer time
    s(GEOID20, bs = "re"),            # random intercept by ZIP
  family   = nb(link = "log"),
  offset   = offset_days,
  data     = wk_fit,
  method   = "fREML",
  discrete = TRUE,
  nthreads = max(1, parallel::detectCores() - 1),
  knots    = knots_list
)
summary(m_wk)
```

```{r}
sel <- c("Impact_days","Threat_days","Cleanup_days")
b   <- coef(m_wk)[sel]
V   <- vcov(m_wk)[sel, sel, drop = FALSE]
se  <- sqrt(diag(V))

out <- data.frame(
  Storm_Effect = c("Impact","Threat","Cleanup"),
  RR_week      = exp(b),
  RR_lo        = exp(b - 1.96*se),
  RR_hi        = exp(b + 1.96*se),
  row.names = NULL
)
out
```

Cleanup days show a clear increase in ER load per additional day that week (≈ +1.7% each). Threat days show a small but significant decrease (≈ −0.8% each) — plausible if people defer non-urgent care on days with active threats. Impact days show no clear aggregate effect at the weekly scale (could reflect offsetting mechanisms: acute spikes in some ZIPs/weeks vs access barriers or suppression elsewhere).

Allow for more zipcode-specific variation (do it in stages)
```{r}
library(dplyr); library(lubridate)

wk <- er_cal %>%
  mutate(week = floor_date(as.Date(Date), "week", week_start = 1)) %>%
  group_by(GEOID20, week) %>%
  summarise(
    visits_wk    = sum(as.numeric(Total_Visits), na.rm = TRUE),
    n_obs_days   = sum(!is.na(Total_Visits)),
    Impact_days  = sum(Impact  > 0L, na.rm = TRUE),
    Threat_days  = sum(Threat  > 0L, na.rm = TRUE),
    Cleanup_days = sum(Cleanup > 0L, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(
    GEOID20     = factor(sprintf("%05s", as.character(GEOID20))),
    doy         = yday(week),
    tw          = as.integer(difftime(week, min(week), "weeks")),
    offset_days = log(pmax(n_obs_days, 1L))
  )

wk_fit <- wk %>% filter(n_obs_days >= 3)

# Fourier seasonality (K = 2)
K <- 2
fourier <- purrr::map_dfc(1:K, function(k) {
  tibble::tibble(
    !!paste0("sin", k) := sin(2*pi*k*wk_fit$doy/366),
    !!paste0("cos", k) := cos(2*pi*k*wk_fit$doy/366)
  )
})
wk_fit <- dplyr::bind_cols(wk_fit, fourier)
fourier_terms <- paste(names(fourier), collapse = " + ")

library(glmmTMB)

# Model A — Random intercept only (should converge)
form_A <- as.formula(
  paste(
    "visits_wk ~ Impact_days + Threat_days + Cleanup_days +",
    fourier_terms, "+ poly(tw, 2) + (1 | GEOID20)"
  )
)

m_A <- glmmTMB(
  form_A,
  family = nbinom2,
  offset = wk_fit$offset_days,
  data   = wk_fit,
  control = glmmTMBControl(optimizer = optim, optArgs = list(method = "BFGS"))
)
summary(m_A)


# Model B - One random slope (Threat term)

library(glmmTMB)

# Keep your wk_fit with K=2 Fourier terms
form_B <- as.formula(
  paste(
    "visits_wk ~ Impact_days + Threat_days + Cleanup_days +",
    "sin1 + cos1 + sin2 + cos2 + poly(tw, 2)",
    "+ (1 | GEOID20) + (0 + Threat_days | GEOID20)"  # or Cleanup_days
  )
)

m_B <- glmmTMB(
  form_B, family = nbinom2,
  offset = wk_fit$offset_days, data = wk_fit,
  control = glmmTMBControl(optimizer = optim, optArgs = list(method = "BFGS"))
)
summary(m_B)

```







## Attach census data

```{r}
# Packages
library(tidycensus)
library(dplyr)
library(tidyr)
library(stringr)
library(purrr)
library(sf)
library(tigris)
```

```{r}
# Make sure your Census key is set once per session
#census_api_key("xxxx", install = FALSE)
options(tigris_use_cache = TRUE)
```

```{r}
# --- Helper: get Decennial 2000 variables ---
# SF1 (population & age/sex); SF3 (income)
sf1_vars <- load_variables(2000, "sf1", cache = TRUE)
sf3_vars <- load_variables(2000, "sf3", cache = TRUE)

# Total population (= P001001 in 2000 SF1)
pop_var <- "P001001"

# Median age (total). In 2000 SF1 this is P013001 "Median age -- Total"
median_age_var <- "P013001"

# For % 65+, we’ll sum all SF1 P012 (sex by age) components whose labels contain "65 years and over"
age_table <- "P012"  # sex by age
age_vars_2000 <- sf1_vars %>%
  filter(str_starts(name, age_table)) %>%
  mutate(is_65plus = str_detect(label, "65 to|66 to|67 to|68 to|69 to|70 to|71 to|72 to|73 to|74 to|75 to|76 to|77 to|78 to|79 to|80 to|81 to|82 to|83 to|84 to|85 years")) %>%
  select(name, is_65plus)

age_vars_65plus <- age_vars_2000 %>% filter(is_65plus) %>% pull(name)

# Median household income (1999$) from 2000 SF3
# In 2000 SF3 this is P053001: "Median household income in 1999 (dollars)"
mhi_var <- "P053001"
```

```{r}
# Pull Decennial 2000 SF1: population + median age
sf1_core <- get_decennial(
  geography = "zcta",
  year = 2000,
  variables = c(total_pop = pop_var, median_age = median_age_var),
  geometry = FALSE,
  output = "wide",
  sumfile = "sf1"
) %>%
  rename(GEOID = GEOID) %>%
  mutate(ZCTA5 = GEOID)

# Pull 65+ counts and compute percent 65+
sf1_age <- get_decennial(
  geography = "zcta",
  year = 2000,
  variables = age_vars_2000$name,
  geometry = FALSE,
  output = "wide",
  sumfile = "sf1"
) %>%
  rename(ZCTA5 = GEOID)

# total population again (P001001) to be safe for % calculation
sf1_pop_only <- get_decennial(
  geography = "zcta",
  year = 2000,
  variables = pop_var,
  geometry = FALSE,
  output = "wide",
  sumfile = "sf1"
) %>%
  rename(ZCTA5 = GEOID, pop2000 = P001001)

# sum 65+ across all matching P012 components
sf1_age_65pct <- sf1_age %>%
  mutate(across(all_of(age_vars_65plus), as.numeric)) %>%
  transmute(ZCTA5,
            age65plus_2000 = rowSums(across(all_of(age_vars_65plus)), na.rm = TRUE)) %>%
  left_join(sf1_pop_only, by = "ZCTA5") %>%
  mutate(pct_65plus_2000 = if_else(pop2000 > 0, 100 * age65plus_2000 / pop2000, NA_real_)) %>%
  select(ZCTA5, age65plus_2000, pop2000, pct_65plus_2000)

# Pull Decennial 2000 SF3: median household income (1999$)
sf3_income <- get_decennial(
  geography = "zcta",
  year = 2000,
  variables = c(median_hh_income_1999 = mhi_var),
  geometry = FALSE,
  output = "wide",
  sumfile = "sf3"
) %>%
  rename(ZCTA5 = GEOID)

# 5) Land & water area for 2020 ZCTAs to match rr_sf$GEOID20
# (ALAND/AWATER are in square meters; this uses 2020 TIGER shapefiles to align with GEOID20)
zcta20 <- tigris::zctas(year = 2020, cb = TRUE, progress_bar = FALSE) %>%
  st_drop_geometry() %>%
  transmute(GEOID20 = ZCTA5CE20,
            land_area_m2_2020 = ALAND20,
            water_area_m2_2020 = AWATER20)

# 6) Assemble a tidy 2000 attributes table keyed to 5-digit ZCTA
attrs2000 <- sf1_core %>%
  transmute(ZCTA5,
            pop_2000 = total_pop,
            median_age_2000 = median_age) %>%
  left_join(sf1_age_65pct, by = "ZCTA5") %>%
  left_join(sf3_income, by = "ZCTA5") %>%
  # Rename income from wide column name
  rename(median_hh_income_1999 = median_hh_income_1999) %>%
  # Make sure ZCTA key is 5-digit character
  mutate(ZCTA5 = str_pad(ZCTA5, width = 5, pad = "0"))

# 7) Join to your rr_sf (assumes rr_sf$GEOID20 is the 5-digit ZCTA code as character)
rr_sf_final <- rr_sf %>%
  mutate(GEOID20 = as.character(GEOID20),
         ZCTA5 = GEOID20) %>%
  left_join(attrs2000, by = "ZCTA5") %>%
  left_join(zcta20, by = "GEOID20") %>%
  # Population density per km^2 using 2020 land area (best align with GEOID20)
  mutate(pop_density_2000_per_km2 = if_else(!is.na(land_area_m2_2020) & land_area_m2_2020 > 0,
                                            pop_2000 / (land_area_m2_2020 / 1e6),
                                            NA_real_))

rr <- rr_sf_final %>%
  filter(hotspot_type %in% c("Hot Spot", "Cold Spot"))
```
