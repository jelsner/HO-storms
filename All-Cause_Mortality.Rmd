---
title: "All-Cause Mortality & Storms"
output: html_document
editor_options:
  chunk_output_type: console
---

Experimental setup. We assemble a multi-million–record, geocoded dataset of all-cause deaths (simple-feature points in WGS84) and tag each death day as one of four “storm effect” states: None, Threat, Impact, or Cleanup. We first aggregate to daily counts by effect, compute the number of effect-days, and fit a Poisson regression of total deaths on storm effect with an offset for log(effect-day count), using “None” as the reference; coefficients are then exponentiated to rate ratios with confidence intervals. Next, for spatial detail we intersect deaths with Florida ZCTAs (2020 vintage), refit the same Poisson model separately within each ZCTA (with basic data sufficiency checks), map rate ratios for Cleanup/Impact/Threat, and run a local hot-spot analysis on the Threat rate ratios using k-nearest neighbors (k=7) and Local Moran’s I to identify hot/cold clusters.

Results. Daily death rates are higher on storm-effect days than on non-storm days (≈416–419 vs ≈402 deaths/day in this sample). The Poisson model quantifies small but statistically precise excesses: Rate ratios ≈ 1.043 (Threat, 95% CI 1.028–1.058), 1.037 (Impact, 1.022–1.051), and 1.034 (Cleanup, 1.020–1.049), all p < 0.001 relative to None. Spatial modeling at the zip code tabulation area level produces choropleths of these rate ratios, and the Local Moran’s I highlights coherent pockets of elevated (“hot”) and depressed (“cold”) Threat ratios, indicating geographic clustering in storm-related mortality risk. Taken together, the analysis suggests modest but reliable short-run increases in all-cause mortality around storm exposure, with non-uniform spatial patterns across Florida.

## Analytics of deaths and storms

Read the data table to a csv file
```{r}
Deaths.dt <- data.table::fread(here::here("data", "outputs", "All_Deaths_Storm_Effects.csv"))
Deaths.df <- Deaths.dt |>
  as.data.frame()
Deaths.sf <- sf::st_read(dsn = "data/outputs/Deaths", 
                         layer = "All_Deaths_Storm_Effects")
names(Deaths.sf)[1:5] <- names(Deaths.df)[1:5] # match column names with those in the csv
```

```{r}
library(dplyr)

#Deaths.df <- Deaths.df |>
#  mutate(SH = Storm_Category > 0)

total_deaths <- nrow(Deaths.df)
exposed_total <- Deaths.dt[Storm_Effect %in% c("Cleanup", "Impact", "Threat"), .N]

# Calculate death counts and rates for specified storm effects
death_rates <- Deaths.df |>
  filter(Storm_Effect %in% c("Cleanup", "Impact", "Threat")) |>
  group_by(Storm_Effect) |>
#  group_by(Storm_Effect, SH) |>
  summarise(Deaths = n(), .groups = "drop") |>
  mutate(Death_Rate = Deaths / exposed_total)

# View result
print(death_rates)
```

This tells you, for example, that .333% of all deaths occurred under an Impact storm condition.

## Death rates per day

Get number of deaths per day by Storm_Effect
```{r}
# Group deaths by date and storm effect
daily_deaths <- Deaths.df |>
#  group_by(Death_Date, Storm_Effect, SH) |>
  group_by(Death_Date, Storm_Effect) |>
  summarise(deaths = n(), .groups = "drop")
```

Get number of days for each storm effect. This assumes Storm_Effect applies to the whole day — i.e., each Death_Date has a unique Storm_Effect
```{r}
num_days <- daily_deaths |>
#  distinct(Death_Date, Storm_Effect, SH) |>
  distinct(Death_Date, Storm_Effect) |>
#  count(Storm_Effect, SH, name = "num_days")
  count(Storm_Effect, name = "num_days")
```

Total deaths per storm effect
```{r}
total_deaths <- daily_deaths |>
#  group_by(Storm_Effect, SH) |>
  group_by(Storm_Effect) |>
  summarise(total_deaths = sum(deaths), .groups = "drop")
```

Calculate daily death rate
```{r}
# death_rates <- left_join(total_deaths, num_days, by = c("Storm_Effect", "SH")) |>
death_rates <- left_join(total_deaths, num_days, by = "Storm_Effect") |>
  mutate(daily_death_rate = total_deaths / num_days)

print(death_rates)
```

This tells us that in this sample the daily death rate is somewhat higher on storm-effect days than on non-storm-effect days

Test it statistically using a Poisson regression model with the reference level being a non-storm-effect day
```{r}
model_data <- left_join(total_deaths, num_days, by = "Storm_Effect")
model_data$Storm_Effect <- relevel(factor(model_data$Storm_Effect), ref = "None")

poisson_model <- glm(total_deaths ~ Storm_Effect + offset(log(num_days)), data = model_data, family = poisson())
summary(poisson_model)
```

This says that given the sample of data and a Poisson model, although the effect size is small mortality is significantly higher on storm-effect days compared to the reference level

Create a nice table of the model results. We exponentiate the model coefficients to express them as a rate ratio and add confidence intervals
```{r}
# Extract coefficients and 95% confidence intervals
coefs <- coef(summary(poisson_model))
confint_vals <- confint(poisson_model)  # May take a few seconds

# Build data frame of results
rate_ratios <- data.frame(
  Term = rownames(coefs),
  Estimate = coefs[, "Estimate"],
  Std_Error = coefs[, "Std. Error"],
  z_value = coefs[, "z value"],
  p_value = coefs[, "Pr(>|z|)"],
  RR = exp(coefs[, "Estimate"]),
  RR_lower = exp(confint_vals[, 1]),
  RR_upper = exp(confint_vals[, 2])
)

# Clean up names
rownames(rate_ratios) <- NULL
rate_ratios <- rate_ratios |>
  dplyr::select(Term, RR, RR_lower, RR_upper, p_value)

# Print the table
options(digits = 4)
print(rate_ratios)
```

## Spatialize results using zip code areas ZCTAs

Get Florida ZCTAs
```{r}
library(tigris)
library(sf)
options(tigris_use_cache = TRUE)

# Get national ZCTAs and subset to those intersecting Florida
zctas_all <- zctas(cb = TRUE, starts_with = NULL, year = 2020)
florida <- states(cb = TRUE) %>% filter(STUSPS == "FL")

# Filter ZCTAs that intersect Florida
zctas_fl <- st_intersection(zctas_all, st_transform(florida, st_crs(zctas_all)))
zctas_fl <- st_make_valid(zctas_fl) # in case of invalid geometries
```

Join deaths to ZCTAs
```{r}
Deaths.sf <- st_transform(Deaths.sf, st_crs(zctas_fl))  # Ensure CRS matches
deaths_with_zcta <- st_join(Deaths.sf, zctas_fl["GEOID20"])  # Join only GEOID20
```

Check out specific zip codes, storms, etc
```{r}
X32778 <- deaths_with_zcta |>
  st_drop_geometry() |>
  dplyr::filter(GEOID20 == "32778") |>
  dplyr::filter(!Storm_Effect == "None")
```

List death dates & storm names by ZCTA and storm effect
```{r}
library(purrr)

df <- deaths_with_zcta %>%
  st_drop_geometry()

# A compact tibble with a list-column of Death_Dates
dates_by_effect_df <- df %>%
  group_by(GEOID20, Storm_Effect) %>%
  summarise(
    Death_Dates = list(sort(unique(Death_Date))),   # the actual unique dates (could be more than 1 death)
    Storm_Names = list(unique(Storm_Name)),         # storm names
    n_records   = n(),                               # total records
    n_dates     = length(Death_Dates[[1]]),          # unique dates
    .groups = "drop"
  ) %>%
  arrange(GEOID20, Storm_Effect)

dates_by_effect_df

# Threat only
dates_by_threat_df <- dates_by_effect_df %>%
  filter(Storm_Effect == "Threat")
```

Fit Poisson models at each ZCTA
```{r}
library(dplyr)
library(purrr)
library(rlang)

deaths_with_zcta$Storm_Effect <- relevel(factor(deaths_with_zcta$Storm_Effect), ref = "None")

models_by_zcta <- deaths_with_zcta %>%
  st_drop_geometry() %>%
  group_split(GEOID20) %>%
  map_df(function(df) {
    if (nrow(df) < 30 || n_distinct(df$Storm_Effect) < 2) return(NULL)

    daily_counts <- df %>%
      group_by(Death_Date, Storm_Effect) %>%
      summarise(deaths = n(), .groups = "drop") %>%
      group_by(Storm_Effect) %>%
      summarise(
        total_deaths = sum(deaths),
        num_days = n_distinct(Death_Date),
        .groups = "drop"
      )

    if (any(daily_counts$num_days == 0)) return(NULL)

    model <- try(glm(total_deaths ~ Storm_Effect + offset(log(num_days)),
                     data = daily_counts, family = poisson()), silent = TRUE)
    if (inherits(model, "try-error")) return(NULL)

    rr <- exp(coef(model))
    tibble(
      GEOID20 = df$GEOID20[1],
      RR_Cleanup = rr["Storm_EffectCleanup"] %||% NA,
      RR_Impact  = rr["Storm_EffectImpact"] %||% NA,
      RR_Threat  = rr["Storm_EffectThreat"] %||% NA
    )
  })
```

Join RRs to ZCTA geometry
```{r}
zcta_results <- left_join(zctas_fl, models_by_zcta, by = "GEOID20")

# Make polygons Leaflet-ready (lightweight & clean)
FL_ALBERS <- 3086L
zcta_results_poly <- zcta_results |>
  dplyr::select(GEOID20, RR_Cleanup, RR_Impact, RR_Threat, geometry) |>
  st_transform(FL_ALBERS) |>
  st_simplify(dTolerance = 75, preserveTopology = TRUE) |>
  st_transform(4326)
saveRDS(zcta_results_poly, "data/outputs/Results/zcta_results.rds")

#sf::st_write(zcta_results, "data/outputs/Results/zcta_results.shp")
```

```{r}
library(rlang)

dat <- models_by_zcta
#rr_cols <- c("RR_Cleanup", "RR_Impact", "RR_Threat")
rr_cols <- c("RR_Threat")

dat_ranked <- dat %>%
  # ensure the RR columns are numeric (optional but handy)
  mutate(across(all_of(rr_cols), as.numeric)) %>%
  # overall scores
  mutate(
    rr_mean = rowMeans(across(all_of(rr_cols)), na.rm = TRUE),
    rr_geom = exp(rowMeans(log(across(all_of(rr_cols))), na.rm = TRUE)) # tie-breaker
  ) %>%
  arrange(desc(rr_mean), desc(rr_geom))

top10 <- dat_ranked %>%
  slice_head(n = 10) %>%
  select(GEOID20, all_of(rr_cols), rr_mean, rr_geom)

bottom10 <- dat_ranked %>%
  arrange(rr_mean, rr_geom) %>%
  slice_head(n = 10) %>%
  select(GEOID20, all_of(rr_cols), rr_mean, rr_geom)

top10
bottom10

top_bottom_zctas_by_threat_rates <- c(top10$GEOID20, bottom10$GEOID20)

top_bottom_dates_by_threat.df <- dates_by_threat_df %>%
  filter(GEOID20 %in% top_bottom_zctas_by_threat_rates)

dates_by_threat_df$Death_Dates[dates_by_threat_df$GEOID20 == "32750"]

```


Plot all 3 RR maps in a loop
```{r}
library(ggplot2)
library(purrr)
library(rlang)

rr_vars <- c("RR_Cleanup", "RR_Impact", "RR_Threat")
titles <- c("RR: Cleanup vs None", "RR: Impact vs None", "RR: Threat vs None")

walk2(rr_vars, titles, function(rr_var, plot_title) {
  p <- ggplot(zcta_results) +
    geom_sf(aes(fill = !!sym(rr_var)), color = "gray70", size = 0.1) +
    scale_fill_gradient2(
      low = "blue", mid = "white", high = "red",
      midpoint = 1, na.value = "gray90", name = rr_var
    ) +
    theme_minimal() +
    labs(title = plot_title)
  
  print(p)
  ggsave(paste0(rr_var, "_ZCTA_map.png"), plot = p, width = 8, height = 6)
})
```

## Hot spot analysis

Using Moran's I and k-nearest neighbors for spatial contiguity
```{r}
rr_sf <- zcta_results %>%
  filter(!is.na(RR_Threat)) %>%
  select(GEOID20, RR = RR_Threat, geometry)

rr_sf <- rr_sf %>%
  filter(st_geometry_type(geometry) %in% c("POLYGON", "MULTIPOLYGON")) %>%
  st_cast("MULTIPOLYGON") # Ensure polygon geometry
```

Create spatial weights
```{r}
centroids <- st_centroid(rr_sf)
coords <- st_coordinates(centroids)
# Choose k (e.g., 5 or 8 neighbors)
k <- 7
knn <- spdep::knearneigh(coords, k = k)
nb <- spdep::knn2nb(knn)

# Convert to spatial weights
lw <- spdep::nb2listw(nb, style = "W")
```

Run local hot spot
```{r}
# Calculate Local Moran’s I
lisa <- spdep::localmoran(rr_sf$RR, lw)

# Add results to the sf object
rr_sf <- rr_sf %>%
  mutate(
    local_i = lisa[, 1],         # Local Moran's I
    p_value = lisa[, 5],         # p-value
    hotspot_type = case_when(
      p_value < 0.15 & RR > mean(RR, na.rm = TRUE) ~ "Hot Spot",
      p_value < 0.15 & RR < mean(RR, na.rm = TRUE) ~ "Cold Spot",
      TRUE ~ "Not Significant"
    )
  )

saveRDS(rr_sf, "data/outputs/Results/hot_spot_threat.rds")
```

```{r}
dat <- rr_sf

top10 <- dat %>%
  filter(hotspot_type == "Hot Spot") %>%
  arrange(desc(RR)) %>%
  slice_head(n = 10) %>%
  select(GEOID20, RR, hotspot_type)
top10

bottom10 <- dat %>%
  filter(hotspot_type == "Cold Spot") %>%
  arrange(desc(RR)) %>%
  slice_tail(n = 10) %>%
  select(GEOID20, RR, hotspot_type)
bottom10

top10
bottom10
```

Map the results
```{r}
library(ggplot2)

ggplot(rr_sf) +
  geom_sf(aes(fill = hotspot_type), color = "gray90", size = 0.1) +
  scale_fill_manual(values = c(
    "Hot Spot" = "red",
    "Cold Spot" = "blue",
    "Not Significant" = "gray90"
  )) +
  labs(title = "Local Hot Spot Analysis (Threat)", fill = "Cluster Type") +
  theme_minimal()
```


## Attach census data

```{r}
# Packages
library(tidycensus)
library(dplyr)
library(tidyr)
library(stringr)
library(purrr)
library(sf)
library(tigris)
```

```{r}
# Make sure your Census key is set once per session
#census_api_key("xxxx", install = FALSE)
options(tigris_use_cache = TRUE)
```

```{r}
# --- Helper: get Decennial 2000 variables ---
# SF1 (population & age/sex); SF3 (income)
sf1_vars <- load_variables(2000, "sf1", cache = TRUE)
sf3_vars <- load_variables(2000, "sf3", cache = TRUE)

# Total population (= P001001 in 2000 SF1)
pop_var <- "P001001"

# Median age (total). In 2000 SF1 this is P013001 "Median age -- Total"
median_age_var <- "P013001"

# For % 65+, we’ll sum all SF1 P012 (sex by age) components whose labels contain "65 years and over"
age_table <- "P012"  # sex by age
age_vars_2000 <- sf1_vars %>%
  filter(str_starts(name, age_table)) %>%
  mutate(is_65plus = str_detect(label, "65 to|66 to|67 to|68 to|69 to|70 to|71 to|72 to|73 to|74 to|75 to|76 to|77 to|78 to|79 to|80 to|81 to|82 to|83 to|84 to|85 years")) %>%
  select(name, is_65plus)

age_vars_65plus <- age_vars_2000 %>% filter(is_65plus) %>% pull(name)

# Median household income (1999$) from 2000 SF3
# In 2000 SF3 this is P053001: "Median household income in 1999 (dollars)"
mhi_var <- "P053001"
```

```{r}
# Pull Decennial 2000 SF1: population + median age
sf1_core <- get_decennial(
  geography = "zcta",
  year = 2000,
  variables = c(total_pop = pop_var, median_age = median_age_var),
  geometry = FALSE,
  output = "wide",
  sumfile = "sf1"
) %>%
  rename(GEOID = GEOID) %>%
  mutate(ZCTA5 = GEOID)

# Pull 65+ counts and compute percent 65+
sf1_age <- get_decennial(
  geography = "zcta",
  year = 2000,
  variables = age_vars_2000$name,
  geometry = FALSE,
  output = "wide",
  sumfile = "sf1"
) %>%
  rename(ZCTA5 = GEOID)

# total population again (P001001) to be safe for % calculation
sf1_pop_only <- get_decennial(
  geography = "zcta",
  year = 2000,
  variables = pop_var,
  geometry = FALSE,
  output = "wide",
  sumfile = "sf1"
) %>%
  rename(ZCTA5 = GEOID, pop2000 = P001001)

# sum 65+ across all matching P012 components
sf1_age_65pct <- sf1_age %>%
  mutate(across(all_of(age_vars_65plus), as.numeric)) %>%
  transmute(ZCTA5,
            age65plus_2000 = rowSums(across(all_of(age_vars_65plus)), na.rm = TRUE)) %>%
  left_join(sf1_pop_only, by = "ZCTA5") %>%
  mutate(pct_65plus_2000 = if_else(pop2000 > 0, 100 * age65plus_2000 / pop2000, NA_real_)) %>%
  select(ZCTA5, age65plus_2000, pop2000, pct_65plus_2000)

# Pull Decennial 2000 SF3: median household income (1999$)
sf3_income <- get_decennial(
  geography = "zcta",
  year = 2000,
  variables = c(median_hh_income_1999 = mhi_var),
  geometry = FALSE,
  output = "wide",
  sumfile = "sf3"
) %>%
  rename(ZCTA5 = GEOID)

# 5) Land & water area for 2020 ZCTAs to match rr_sf$GEOID20
# (ALAND/AWATER are in square meters; this uses 2020 TIGER shapefiles to align with GEOID20)
zcta20 <- tigris::zctas(year = 2020, cb = TRUE, progress_bar = FALSE) %>%
  st_drop_geometry() %>%
  transmute(GEOID20 = ZCTA5CE20,
            land_area_m2_2020 = ALAND20,
            water_area_m2_2020 = AWATER20)

# 6) Assemble a tidy 2000 attributes table keyed to 5-digit ZCTA
attrs2000 <- sf1_core %>%
  transmute(ZCTA5,
            pop_2000 = total_pop,
            median_age_2000 = median_age) %>%
  left_join(sf1_age_65pct, by = "ZCTA5") %>%
  left_join(sf3_income, by = "ZCTA5") %>%
  # Rename income from wide column name
  rename(median_hh_income_1999 = median_hh_income_1999) %>%
  # Make sure ZCTA key is 5-digit character
  mutate(ZCTA5 = str_pad(ZCTA5, width = 5, pad = "0"))

# 7) Join to your rr_sf (assumes rr_sf$GEOID20 is the 5-digit ZCTA code as character)
rr_sf_final <- rr_sf %>%
  mutate(GEOID20 = as.character(GEOID20),
         ZCTA5 = GEOID20) %>%
  left_join(attrs2000, by = "ZCTA5") %>%
  left_join(zcta20, by = "GEOID20") %>%
  # Population density per km^2 using 2020 land area (best align with GEOID20)
  mutate(pop_density_2000_per_km2 = if_else(!is.na(land_area_m2_2020) & land_area_m2_2020 > 0,
                                            pop_2000 / (land_area_m2_2020 / 1e6),
                                            NA_real_))

rr <- rr_sf_final %>%
  filter(hotspot_type %in% c("Hot Spot", "Cold Spot"))
```
