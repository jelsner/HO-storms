---
title: "All-Cause Mortality & Storms"
output: html_document
editor_options:
  chunk_output_type: console
---

## Summary
Experimental setup. We assemble a multi-million–record, geocoded dataset of all-cause deaths (simple-feature points in WGS84) and tag each death as one of four “storm effect” states: None, Threat, Impact, or Cleanup from an empirical storm model. We then aggregate to daily counts by zip code areas and create zip x date storm effect panel data using only days during hurricane seasons (May-November). We fit a fixed-effects Poisson model where the dependent variable is the daily death count and the fixed effects are indicator variables including zip code, year-month, and day of the week. The zip variable controls for all time-invariant differences between zip codes (e.g., population size, socioeconomic level, health infrastructure), the year-month variable controls for month-specific factors common to all zip areas (e.g., statewide seasonality, flu/COVID waves, long-term trends) and the day of the week variable controls for weekly cycles in mortality (e.g., weekend vs. weekday effects). We include a clustering option with the zip and year-month variables to ensure the standard errors are robust to residual correlation within zip codes over time and within months across zip codes. Conceptually the model estimates how daily mortality rates differ on storm-effect days (Threat, Impact, Cleanup) relative to normal “None” days, after adjusting for: differences between zip codes, monthly/seasonal patterns and day-of-week patterns.

Results. The model shows that compared with “None” days, statewide Threat days are associated with an ~8% increase in daily deaths (95% CI ~2%–15%), Cleanup days with ~8%, and Impact days with ~7% (borderline statistical evidence). The adjusted pseudo R-squared is 0.195 and the squared correlation is 0.241 indicating reasonable explanatory power given these are daily death counts with strong fixed effects. A time-stratified model per zip code yields choropleths of these rate ratios, and the Local Moran’s I highlights pockets of elevated (“hot”) and depressed (“cold”) Threat ratios, indicating geographic clustering in storm-related mortality risk. Taken together, the analysis suggests modest but reliable short-run increases in all-cause mortality around storm exposure, with non-uniform spatial patterns across Florida.

## Analytics of deaths and storms

Read the data table to a csv file
```{r}
Deaths.dt <- data.table::fread(here::here("data", "outputs", "All_Deaths_Storm_Effects.csv"))
Deaths.df <- Deaths.dt |>
  as.data.frame()
Deaths.sf <- sf::st_read(dsn = "data/outputs/Deaths", 
                         layer = "All_Deaths_Storm_Effects")
names(Deaths.sf)[1:5] <- names(Deaths.df)[1:5] # match column names with those in the csv

# Remove rows before 1985 as the first storm since 1981 occurred in that year

library(dplyr)
Deaths.df <- Deaths.df |>
  filter(Death_Date >= as.Date("1985-01-01"))
Deaths.sf <- Deaths.sf |>
  filter(Death_Date >= as.Date("1985-01-01"))
Deaths.dt <- Deaths.dt |>
  filter(Death_Date >= as.Date("1985-01-01"))
```

Compute statewide death rates
```{r}
#Deaths.df <- Deaths.df |>
#  mutate(SH = Storm_Category > 0)

total_deaths <- nrow(Deaths.df)
exposed_total <- Deaths.dt[Storm_Effect %in% c("Cleanup", "Impact", "Threat"), .N]

# Calculate death counts and rates for specified storm effects
death_rates <- Deaths.df |>
  filter(Storm_Effect %in% c("Cleanup", "Impact", "Threat")) |>
  group_by(Storm_Effect) |>
#  group_by(Storm_Effect, SH) |>
  summarise(Deaths = n(), .groups = "drop") |>
  mutate(Death_Rate = Deaths / exposed_total)

print(death_rates)
```

This tells you, for example, that .333% of all deaths statewide occurred under an Impact storm condition.

## Death rates per day

Get statewide number of deaths per day by storm effect
```{r}
# Range of dates for which there was a storm effect
range <- Deaths.df |>
  filter(Storm_Effect != "None")
range(range$Death_Date)

# Group deaths by date and storm effect
daily_deaths <- Deaths.df |>
#  group_by(Death_Date, Storm_Effect, SH) |>
  group_by(Death_Date, Storm_Effect) |>
  summarise(deaths = n(), .groups = "drop")
```

Get number of days statewide for each storm effect. This assumes storm effect applies to the whole day — i.e., each death has a unique storm effect
```{r}
num_days <- daily_deaths |>
#  distinct(Death_Date, Storm_Effect, SH) |>
  distinct(Death_Date, Storm_Effect) |>
#  count(Storm_Effect, SH, name = "num_days")
  count(Storm_Effect, name = "num_days")

print(num_days)
```

Total statewide deaths per storm effect
```{r}
total_deaths <- daily_deaths |>
#  group_by(Storm_Effect, SH) |>
  group_by(Storm_Effect) |>
  summarise(total_deaths = sum(deaths), .groups = "drop")

print(total_deaths)
```

Calculate statewide daily death rate
```{r}
# death_rates <- left_join(total_deaths, num_days, by = c("Storm_Effect", "SH")) |>
death_rates <- left_join(total_deaths, num_days, by = "Storm_Effect") |>
  mutate(daily_death_rate = total_deaths / num_days)

print(death_rates)
```

This tells us that in this sample the statewide daily death rate is somewhat higher on non-storm-effect days. But a lot is going on: time and spatial trends, seasonality, autocorrelation. We need a model that accounts for these things

## Negative binomial generalized additive model (NOT USED)

Setup the data frame
```{r}
library(lubridate)

dat <- daily_deaths %>%
  transmute(
    date  = as.Date(Death_Date),
    y     = as.integer(deaths),
    effect = factor(Storm_Effect,
                    levels = c("None", "Threat", "Impact", "Cleanup")),
    dow   = wday(date, label = TRUE, week_start = 1), # Mon..Sun
    doy   = yday(date),                               # 1..366 (handles leap days too)
    tnum  = as.integer(date - min(date))              # days since start (well-scaled)
  ) %>%
  mutate(
    # make sure 'dow' is UNordered and with desired reference level:
    dow = factor(as.character(dow),  # drop any ordered class
                 levels = c("Mon","Tue","Wed","Thu","Fri","Sat","Sun"))
  )
```

Generalized linear model (negative binomial)
```{r}
m_glm <- MASS::glm.nb(
  y ~ effect + dow + poly(doy, 3) + poly(tnum, 3),
  data = dat, link =  log
)
summary(m_glm)

acf(residuals(m_glm))
```

This looks good. Exponentiating the Threat effect term of .0283 yields 1.0287. This says that on Threat days the number of deaths is 2.9% higher than on non-effect days and it is marginally significant (p = .015)

But there is large autocorrelation in the residuals. Let's try another model that includes autocorrelation

```{r}
library(gratia)
library(mgcv)

# mark start of series (single series here)
dat$ARstart <- c(TRUE, rep(FALSE, nrow(dat)-1))

# Cyclic smooth needs knots that wrap (0.5, 366.5) for stability
knots_list <- list(doy = c(0.5, 366.5))

m_ar1 <- bam(
  y ~ effect + dow + s(doy, bs = "cc", k = 10) + s(tnum, k = 100),
  family = nb(link = "log"),
  data = dat,
  method = "fREML",
  discrete = TRUE,
  knots = knots_list,
  AR.start = dat$ARstart,
  rho = 0.57  # set to the estimated lag-1 corr
)

summary(m_ar1)
```

Effect days are no longer statistically significant. Autocorrelation inflates z-scores. Plain (quasi/NB) GLMs assume independent residuals. Daily mortality has strong autoregression (AR) structure; ignoring it underestimates SEs, making small effects look “significant.” Adding AR(1) (or equivalently, using time-stratified controls) raises SEs → p-values go up. Also there is collinearity with seasonality/trend. Storms cluster in certain seasons & years. Flexible smooths (or fine time strata) explain a lot of the same variance; the unique contribution of storm indicators shrinks.

Multiple testing/time-of-day confounding (more relevant at ZIP scale), but with many ZIPs, some will look “significant” by chance unless we account for dependence and control false detection rate.

## Poisson model with stratum fixed effects at the state level

Setup the data (as a panel)

Get Florida zips
```{r}
library(tigris)
library(sf)
options(tigris_use_cache = TRUE)

# Get national ZCTAs and subset to those intersecting Florida
zctas_all <- zctas(cb = TRUE, starts_with = NULL, year = 2020)
florida <- states(cb = TRUE) %>% filter(STUSPS == "FL")

# Filter ZCTAs that intersect Florida
zctas_fl <- st_intersection(zctas_all, st_transform(florida, st_crs(zctas_all)))
zctas_fl <- st_make_valid(zctas_fl) # in case of invalid geometries
```

Spatial join deaths to zips
```{r}
# Inspect CRS
st_crs(zctas_fl)   # NAD83 (likely EPSG:4269)
st_crs(Deaths.sf)  # WGS84 (EPSG:4326)

# Harmonize CRS (transform points to the polygons' CRS)
Deaths_aligned <- st_transform(Deaths.sf, st_crs(zctas_fl))

# Make polygons valid (fix self-intersections, rings, etc.)
zctas_valid <- st_make_valid(zctas_fl) %>% dplyr::select(GEOID20, geometry)

sf_use_s2(TRUE)

# Point-in-polygon join: add GEOID20 from polygon to each point
#    st_within() = strictly inside (excludes points exactly on boundary)
deaths_with_zip <- st_join(
  Deaths_aligned,
  zctas_valid,
  join = st_intersects,   # st_intersects includes boundary points
  left = TRUE
)

# Any points that didn't land in a polygon? (e.g., offshore or boundary issues)
sum(is.na(deaths_with_zip$GEOID20))
# returns 1915 (~.03%) when st_within or st_intersects. Could snap unmatched points to nearest polygon

# Sanity: every point should match at most one ZCTA (ZCTAs are non-overlapping)
dup_check <- deaths_with_zip |>
  st_drop_geometry() |>
  count(Death_ID) |>
  filter(n > 1)
dup_check # <0 rows> -> every point matches at most one ZCTA

# Remove deaths that didn't land in a polygon
deaths_with_zip <- deaths_with_zip |> 
  filter(!is.na(GEOID20))
```

List death dates & storm names by zip and storm effect
```{r}
library(purrr)

df <- deaths_with_zip %>%
  st_drop_geometry()

# A compact tibble with a list-column of Death_Dates
dates_by_effect_df <- df %>%
  group_by(GEOID20, Storm_Effect) %>%
  summarise(
    Death_Dates = list(sort(unique(Death_Date))),   # the actual unique dates (could be more than 1 death)
    Storm_Names = list(unique(Storm_Name)),         # storm names
    n_records   = n(),                               # total records
    n_dates     = length(Death_Dates[[1]]),          # unique dates
    .groups = "drop"
  ) %>%
  arrange(GEOID20, Storm_Effect)

dates_by_effect_df
length(unique(dates_by_effect_df$GEOID20))
```

Daily death counts by zip
```{r}
daily_by_zip <- deaths_with_zip %>%
  st_drop_geometry() %>%
  transmute(
    zip    = as.character(GEOID20),
    date   = as.Date(Death_Date),
    effect = as.character(Storm_Effect)
  ) %>%
  group_by(zip, date, effect) %>%
  summarise(deaths = n(), .groups = "drop")

# Sanity check
daily_by_zip %>%
  count(zip, date) %>%
  filter(n > 1)  # rows here mean the same zip–date appears under multiple effects
```

Compute death totals by zip by effect with the effect as separate columns (wide format), then join to zip polygons, then make choropleth maps
```{r}
library(tidyr)

death_counts_wide <- deaths_with_zip %>%
  sf::st_drop_geometry() %>%  
  filter(Storm_Effect %in% c("Threat","Impact","Cleanup")) %>%
  count(GEOID20, Storm_Effect, name = "deaths") %>%
  tidyr::pivot_wider(
    names_from  = Storm_Effect,
    values_from = deaths,
    values_fill = 0
  )

zcta_poly <- zctas_fl %>%
  mutate(GEOID20 = as.character(GEOID20)) %>%
  { if (is.na(sf::st_crs(.)$epsg) || sf::st_crs(.)$epsg != 4326) sf::st_transform(., 4326) else . } %>%
  sf::st_make_valid()

# keep polygonal features only and cast to MULTIPOLYGON
zcta_poly <- zcta_poly[sf::st_geometry_type(zcta_poly) %in% c("POLYGON","MULTIPOLYGON"), , drop = FALSE]
zcta_poly <- zcta_poly[!sf::st_is_empty(zcta_poly), , drop = FALSE]
zcta_poly <- suppressWarnings(sf::st_cast(zcta_poly, "MULTIPOLYGON", warn = FALSE))

# left join counts (preserves polygon geometry)
death_map_sf <- zcta_poly %>%
  left_join(death_counts_wide, by = "GEOID20") %>%
  mutate(
    Threat = replace_na(Threat, 0L),
    Impact = replace_na(Impact, 0L),
    Cleanup = replace_na(Cleanup, 0L)
  ) %>%
  select(GEOID20, Threat, Impact, Cleanup, geometry)

# for story map
#saveRDS(death_map_sf, "data/outputs/Results/zcta_death_counts.rds")
```

Make maps
```{r}
library(ggplot2)
library(RColorBrewer)

bins <- c(0, 5, 10, 20, 40, 70, 100)
pal  <- brewer.pal(length(bins) - 1, "YlOrRd")

make_map <- function(data_sf, col, title = col) {
  ggplot(data_sf) +
    geom_sf(aes(fill = .data[[col]]), color = "white", size = 0.05) +
    scale_fill_stepsn(
      colors  = pal,
      breaks  = bins,
      limits  = range(bins),
      na.value = "#cccccc",
      name    = "Deaths per ZCTA"
    ) +
    labs(title = title) +
    coord_sf(datum = NA) +
    theme_void(base_size = 12) +
    theme(plot.title = element_text(face = "bold", hjust = 0.5))
}

p_threat  <- make_map(death_map_sf, "Threat",  "Deaths on threat days")
p_impact  <- make_map(death_map_sf, "Impact",  "Deaths on impact days")
p_cleanup <- make_map(death_map_sf, "Cleanup", "Deaths on cleanup days")

# Print or save
# p_threat; p_impact; p_cleanup
# ggsave("zcta_deaths_threat.png",  p_threat,  width = 7.2, height = 6, dpi = 300)
# ggsave("zcta_deaths_impact.png",  p_impact,  width = 7.2, height = 6, dpi = 300)
# ggsave("zcta_deaths_cleanup.png", p_cleanup, width = 7.2, height = 6, dpi = 300)
```

Join deaths to a full calendar. The calendar is created in `deaths-storms.Rmd` and is called `effect_calendar`
```{r}
dbz <- as.data.table(daily_by_zip)[ , .(
  zip   = as.character(zip),
  date  = as.IDate(date),
  deaths = as.integer(deaths)
)]

zip_day <- dbz[effect_calendar, on = .(zip, date)]
zip_day[is.na(deaths), deaths := 0L]  # fill missing counts with 0

# After joining deaths: zeros should appear across all effects
zip_day[, .(zeros = sum(deaths == 0L), rows = .N), by = effect][order(effect)]
```

Then we fit a Poisson with stratum fixed effects. This is equivalent to conditioning on the stratum totals and largely removes serial confounding. Use robust (clustered) SEs by stratum to protect against residual correlation within a stratum

Add strata variables
```{r}
dat <- zip_day %>%
  filter(month(date) %in% 5:11) %>%  # hurricane season only
  transmute(
    zip    = as.character(zip),
    date   = as.IDate(date),
    deaths = as.integer(deaths),
    effect = factor(effect, levels = c("None","Threat","Impact","Cleanup")),
    dow     = factor(strftime(date, "%u"), levels = as.character(1:7),
                       labels = c("Mon","Tue","Wed","Thu","Fri","Sat","Sun")),
    ym     = format(date, "%Y-%m")                      # year-month stratum
  )
```

Fit model
```{r}
library(fixest)

m_pooled <- fepois(
  deaths ~ i(effect, ref = "None") | zip + ym + dow,
  data    = dat,
  cluster = ~ zip + ym   # two-way clustered SEs (ZIP & month)
)

summary(m_pooled)

# Rate ratios (RR) with CIs
library(broom)

rr_tbl <- tidy(m_pooled, conf.int = TRUE) %>%                # returns log-scale
  filter(grepl("^effect::", term)) %>%
  transmute(
    effect = sub("^effect::", "", term),
    RR     = exp(estimate),
    LCI    = exp(conf.low),
    UCI    = exp(conf.high),
    p      = p.value
  )
rr_tbl
```

We fit a fixed-effects Poisson with: ZIP fixed effects (| zip) → controls for all time-invariant differences across ZIPs (population size, baseline mortality, socio-demographics, etc.), Year-month fixed effects (| ym) → controls for statewide shocks and seasonality at the month level (flu waves, hurricanes seasons, COVID waves, etc.). Day-of-week fixed effects (| dow) → controls for weekly pattern. So the effect:: coefficients are within-ZIP log rate ratios vs “None”, after removing month and weekday patterns. Exponentiate them to get rate ratios (RR).

What the coefficients mean: 
We fit a fixed-effects Poisson with: ZIP fixed effects (| zip) → controls for all time-invariant differences across ZIPs (population size, baseline mortality, socio-demographics, etc.)
Year-month fixed effects (| ym) → controls for statewide shocks and seasonality at the month level (flu waves, hurricanes seasons, COVID waves, etc.)
Day-of-week fixed effects (| dow) → controls for weekly pattern
So the effect::… coefficients are within-ZIP log rate ratios vs “None”, after removing month and weekday patterns. Exponentiate them to get rate ratios (RR). Convert the estimates to RRs (95% CI)

Threat: β = 0.07994 (SE 0.02950), p = 0.0067
RR = 1.083 (95% CI 1.022–1.148) → ~8.3% higher deaths vs None, adjusted for ZIP / YM / DOW.
Impact: β = 0.06389 (SE 0.03286), p = 0.0518
RR = 1.066 (95% CI 1.000–1.137) → ~6.6% higher; borderline at 0.052.
Cleanup: β = 0.07333 (SE 0.03045), p = 0.0160
RR = 1.076 (95% CI 1.014–1.142) → ~7.6% higher.
(RR = exp(β); CI = exp(β ± 1.96·SE).)

NOTE: 44/0/0 fixed-effects (357,808 observations) removed because of only 0 outcomes or singletons.
You have 3 FE groups (zip / ym / dow). The 44/0/0 means 44 ZIP levels were dropped (0 YM, 0 DOW) because, within those ZIPs, the data contributed no identifying variation (e.g., all days had zero deaths or only singleton cells after stratification). The corresponding 357,808 rows were removed from estimation. That’s expected behavior: those ZIPs can’t help identify contrasts after the FE are applied.

Log-likelihood and BIC are for the Poisson FE fit. “Adj. Pseudo R²: 0.195” and “Squared Cor.: 0.241” indicate reasonable explanatory power given this is daily death counts with strong FE.

Compared with “None” days, statewide Threat days are associated with an ~8% increase in daily deaths (95% CI ~2%–15%), Cleanup days with ~8%, and Impact days with ~7% (borderline statistical evidence).

Try negative binomial fixed effects as a sensitivity
```{r}
m_pooled2 <- fenegbin(
  deaths ~ i(effect, ref = "None") | zip + ym + dow,
  data    = dat,
  cluster = ~ zip + ym   # two-way clustered SEs (ZIP & month)
)

summary(m_pooled2)
```

Results are essentially the same

Note: When including weaker tropical storms 34-55 kt, the effect sizes are about 1/2 the effect sizes of including only storms at 55 kt or stronger.

## Time stratified Poisson fixed effect model per zip code area

Choose candidate zips worth modeling
```{r}
cands <- dat[
  , .(
      has_none = any(effect == "None"),
      has_tic  = any(effect != "None"),
      pos_y    = any(deaths > 0L)
    ),
  by = zip
][has_none & has_tic & pos_y, zip]
length(cands) # returns 999

# Optionally require a minimum number of storm-effect days
min_effect_days <- 5L
cands2 <- dat[
  , .(n_tic = sum(effect != "None")),
  by = zip
][n_tic >= min_effect_days, zip]
length(cands2) # returns 993
```

Per-zip poisson fixed effect (stratify by ym + dow)
```{r}
library(pbapply)

# Function for a single zip code
fit_one <- function(df) {
  df[, effect := factor(effect, levels = c("None","Threat","Impact","Cleanup"))]
  # Skip if baseline mean is zero (RR not defined)
  if (!any(df$effect == "None") || mean(df$deaths[df$effect == "None"]) == 0) return(NULL)
  tryCatch(
    fepois(
      deaths ~ i(effect, ref = "None") | ym + dow,
      data    = df,
      cluster = ~ ym
    ),
    error = function(e) NULL
  )
}

# sequential (shows a progress bar)
models <- pblapply(cands, function(z) fit_one(dat[zip == z]))
names(models) <- cands
```

Extract relative rates (and 95% CIs) per zip for Threat, Impact, Cleanup
```{r}
rr_by_zip <- bind_rows(lapply(names(models), function(z) {
  m <- models[[z]]
  if (is.null(m)) return(NULL)
  tidy(m, conf.int = TRUE) %>%
    filter(grepl("^effect::", term)) %>%
    transmute(
      zip    = z,
      effect = sub("^effect::", "", term),
      RR     = exp(estimate),
      LCI    = exp(conf.low),
      UCI    = exp(conf.high),
      p      = p.value
    )
}))

# Optional: keep only Threat/Impact/Cleanup rows
rr_by_zip <- rr_by_zip %>% filter(effect %in% c("Threat","Impact","Cleanup"))
```

The extreme RRs are a data-sparsity/separation artifact of per-ZIP, heavily stratified Poisson fits

Where are the extremes coming from?
```{r}
# per-ZIP support by effect
zip_support <- dat %>%      # dat = per-ZIP daily frame 
  group_by(zip, effect) %>%
  summarise(
    days   = n(),
    deaths = sum(deaths),
    mean_y = mean(deaths),
    .groups = "drop"
  )

# join support onto the RR table
rr_chk <- rr_by_zip %>%
  left_join(zip_support %>% rename(days_e = days, deaths_e = deaths, mean_e = mean_y),
            by = c("zip","effect")) %>%
  left_join(zip_support %>% filter(effect=="None") %>%
              select(zip, days_none = days, deaths_none = deaths, mean_none = mean_y),
            by = "zip")

# look at the pathological ones
rr_chk %>%
  filter(RR < 0.2 | RR > 5 | !is.finite(RR)) %>%
  arrange(RR) %>%
  select(zip, effect, RR, LCI, UCI, p, days_e, deaths_e, days_none, deaths_none)
```

Empirical Bayes shrinkage before mapping
```{r}
# Pooled FE Poisson to get a prior mean per effect (log scale)
# This was done above and saved as `m_pooled`
#pooled <- fixest::fepois(deaths ~ i(effect, "None") | zip + ym + dow, data = dat, cluster = ~ zip + ym)
pooled_eff <- broom::tidy(m_pooled, conf.int = FALSE) %>%
  filter(grepl("^effect::", term)) %>%
  transmute(effect = sub("^effect::","",term), mu0 = estimate)   # log-RR prior

# Approximate per-ZIP log-RR variance from the model (delta from CI)
log_rr <- rr_by_zip %>%
  mutate(
    logRR   = log(RR),
    se_log  = (log(UCI) - log(LCI)) / (2*1.96)
  ) %>% filter(is.finite(logRR), se_log > 0)

# Shrink: posterior mean = (logRR / se^2 + mu0 / tau^2) / (1/se^2 + 1/tau^2)
#    Use a single tau per effect (between-ZIP SD). Estimate tau via robust SD.
tau_by_eff <- log_rr %>% group_by(effect) %>%
  summarise(tau = stats::mad(logRR, constant = 1), .groups = "drop")

rr_shrunk <- log_rr %>%
  left_join(pooled_eff, by="effect") %>%
  left_join(tau_by_eff, by="effect") %>%
  mutate(
    prec_obs = 1/(se_log^2),
    prec_pri = 1/(pmax(tau, 1e-6)^2),
    post     = (logRR*prec_obs + mu0*prec_pri) / (prec_obs + prec_pri),
    RR_shr   = exp(post)
  ) %>%
  select(zip, effect, RR_shr)

# for story map (Threat only)
rr_wide <- rr_shrunk %>%
  filter(effect == "Threat") %>%
  mutate(GEOID20 = zip,
         Threat = RR_shr) %>%
  select(GEOID20, Threat)

rr_map_sf <- zcta_poly %>%
  left_join(rr_wide, by = "GEOID20") %>%
  select(GEOID20, Threat, geometry)
```

Map the Threat
```{r}
effect_to_map = "Threat"
# Bin RRs: [0,1), [1,2), [2,4), [4,∞)
rr_map_sf <- rr_map_sf %>%
  mutate(
    rr_bin = cut(
      Threat,
      breaks = c(0, 1, 2, 4, Inf),  # last break is Inf
      labels = c("0–1", "1–2", "2–4", ">4"),
      right  = TRUE,                   # (a, b] so last bin is (8, Inf]
      include.lowest = TRUE
    ),
    # lock the level order to match the palette keys
    rr_bin = fct_relevel(rr_bin, "0–1", "1–2", "2–4", ">4")
  )

# for story map
saveRDS(rr_map_sf, "data/outputs/Results/zcta_results.rds")

# Define a discrete palette (colorblind-friendly)
pal <- c(
  "0–1" = "#fbb4b9",  # light
  "1–2" = "#9ecae1",
  "2–4" = "#6baed6",
  ">4"  = "#08519c"
)

# Plot
ggplot(rr_map_sf) +
  geom_sf(aes(fill = rr_bin), color = NA) +
  scale_fill_manual(
    values   = pal,
    limits = levels(rr_map_sf$rr_bin),
    drop     = FALSE,
    na.value = "grey90",
    name     = paste0("RR vs None (", effect_to_map, ")")
  ) +
  labs(
    title = paste("Per-ZIP Rate Ratios on", effect_to_map, "days"),
    subtitle = "Bins: 0–1, 1–2, 2–4, >4",
    caption = "RRs from per-ZIP time-stratified Poisson fixed-effect"
  ) +
  theme_minimal() +
  theme(
    legend.position = "right",
    panel.grid.major = element_line(color = "white"),
    axis.title = element_blank()
  )

# Sanity check
table(rr_map_sf$rr_bin, useNA = "ifany")
sum(is.infinite(rr_map_sf$RR_shr))
```

Save per zip model results for story-map (Threat only)
```{r}
# zcta_results.rds
rr_shrunk_threat_only <- rr_shrunk %>%
  filter(effect == "Threat")
saveRDS(rr_shrunk_threat_only, "data/outputs/Results/zcta_results.rds")
```

## Hot spot analysis
```{r}
library(spdep)

# Prep: keep only rows with RR, keep polygons ---------------------------
g <- Moran_sf %>%
  filter(!is.na(RR)) %>%
  st_make_valid()

# Neighbors & weights (Queen) -------------------------------------------
# Set queen = TRUE (default). Use rook = FALSE (change to TRUE for rook)
nb <- spdep::poly2nb(g, queen = TRUE)
lw <- spdep::nb2listw(nb, style = "W", zero.policy = TRUE)  # allow islands

RR_vec <- as.numeric(log(g$RR))  # logs and make simple vector

# Local Moran's I with permutations -------------------------------------
# 999 permutations; two-sided test; handle islands
lm <- spdep::localmoran_perm(
  x = RR_vec,
  listw = lw,
  nsim = 999,
  alternative = "two.sided",
  zero.policy = TRUE
)
lm <- as.data.frame(lm)
names(lm) <- c("Ii","E.Ii","Var.Ii","Z.Ii","p.value")

# Multiple-testing correction (FDR)
lm$p.adj <- p.adjust(lm$p.value, method = "fdr")

# Spatial lag of RR (for quadrant classification)
lagRR <- spdep::lag.listw(lw, g$RR, zero.policy = TRUE)

# Cluster classification -------------------------------------------------
alpha <- 0.25
mx <- mean(g$RR, na.rm = TRUE)

cluster <- dplyr::case_when(
  g$RR >= mx & lagRR >= mx & lm$Ii > 0 & lm$p.value < alpha ~ "High-High",
  g$RR <= mx & lagRR <= mx & lm$Ii > 0 & lm$p.value < alpha ~ "Low-Low",
  g$RR >= mx & lagRR <= mx & lm$Ii < 0 & lm$p.value < alpha ~ "High-Low",
  g$RR <= mx & lagRR >= mx & lm$Ii < 0 & lm$p.value < alpha ~ "Low-High",
  TRUE ~ "Not Significant"
)

g_locI <- g %>%
  mutate(
    Ii       = lm$Ii,
    E_Ii     = lm$E.Ii,
    Var_Ii   = lm$Var.Ii,
    Z_I      = lm$Z.Ii,
    p_value  = lm$p.value,
    p_adj    = lm$p.adj,
    lag_RR   = lagRR,
    cluster  = factor(cluster,
                      levels = c("High-High","Low-Low","High-Low","Low-High","Not Significant"))
  )

# Join back to the full set (keeps rows where RR was NA) ----------------
Moran_out <- Moran_sf %>%
  left_join(
    g_locI %>%
      st_drop_geometry() %>%
      select(GEOID20, Ii, E_Ii, Var_Ii, Z_I, p_value, p_adj, lag_RR, cluster),
    by = "GEOID20"
  ) %>%
  mutate(
    cluster = ifelse(is.na(cluster), "No data", as.character(cluster)),
    cluster = factor(cluster,
      levels = c("High-High","Low-Low","High-Low","Low-High","Not Significant","No data")
    )
  )
```

Maps
```{r}
# Map local Moran's I
library(scales)

# Pick a symmetric range around 0 so colors are comparable
imax <- max(abs(g_locI$Ii), na.rm = TRUE)

ggplot(g_locI) +
  geom_sf(aes(fill = Ii), color = NA) +
  scale_fill_gradient2(
    low    = "#2b6cb0",   # blue  (negative Ii)
    mid    = "#f7f7f7",   # light
    high   = "#e53e3e",   # red   (positive Ii)
    midpoint = 0,
    limits = c(-imax, imax),
    oob = squish,
    name = "Local Moran's I"
  ) +
  coord_sf(datum = NA) +
  theme_void(base_size = 12) +
  theme(legend.position = "right")

# Map the clusters
pal <- c("High-High"="#e53e3e","Low-Low"="#2b6cb0",
         "High-Low" ="#f59e0b","Low-High"="#10b981",
         "Not Significant"="#d9d9d9","No data"="#cccccc")

ggplot(Moran_out) +
  geom_sf(aes(fill = cluster), color = NA) +
  scale_fill_manual(values = pal, drop = FALSE) +
  theme_void() + labs(fill = "Local Moran clusters")
```

Save per zip local Moran's results for story-map (Threat only)
```{r}
# zcta_clusters.rds
rr_clusters <- g_locI %>%
  select(GEOID20, Ii, geometry)

saveRDS(rr_clusters, "data/outputs/Results/zcta_clusters.rds")
```


### Figure 1: Study design and workflow

```{r}
library(ggplot2)
library(dplyr)
library(tibble)

# ------------------------------------------------------------
# Define boxes (positions tuned for a clean layout)
# ------------------------------------------------------------
boxes <- tibble(
  id = c(
    "deaths_geo",
    "storms",
    "zcta",
    "panel_all",
    "panel_restr",
    "state_model",
    "state_outputs",
    "zip_models",
    "zip_rrs",
    "zip_lisa"
  ),
  xmin = c(
    0.5,  # deaths_geo
    3.0,  # storms
    5.5,  # zcta
    2.0,  # panel_all
    2.0,  # panel_restr
    0.5,  # state_model
    0.5,  # state_outputs
    4.5,  # zip_models
    4.5,  # zip_rrs
    4.5   # zip_lisa
  ),
  xmax = c(
    2.5,  # deaths_geo
    5.0,  # storms
    7.5,  # zcta
    6.0,  # panel_all
    6.0,  # panel_restr
    3.5,  # state_model
    3.5,  # state_outputs
    7.5,  # zip_models
    7.5,  # zip_rrs
    7.5   # zip_lisa
  ),
  ymin = c(
    8.0,  # deaths_geo
    8.0,  # storms
    8.0,  # zcta
    6.5,  # panel_all
    5.0,  # panel_restr
    3.5,  # state_model
    2.0,  # state_outputs
    3.5,  # zip_models
    2.0,  # zip_rrs
    0.5   # zip_lisa
  ),
  ymax = c(
    9.0,  # deaths_geo
    9.0,  # storms
    9.0,  # zcta
    7.5,  # panel_all
    6.0,  # panel_restr
    4.5,  # state_model
    3.0,  # state_outputs
    4.5,  # zip_models
    3.0,  # zip_rrs
    1.5   # zip_lisa
  )
)

boxes <- boxes %>%
  mutate(
    x = (xmin + xmax) / 2,
    y = (ymin + ymax) / 2
  )

# Labels
label_map <- c(
  deaths_geo = "Vital records\nGeocoded deaths\n(point-level, daily)",
  storms     = "Storm data\nIBTrACS + windfields\n(Threat/Impact/Cleanup)",
  zcta       = "ZCTA/ZIP boundaries\nPopulation & covariates",
  panel_all  = "ZIP–day mortality panel\nTagged by storm effect\n(1985–2022)",
  panel_restr = "Analysis sample\n1985–2022, May–Nov\nFlorida ZIP–day deaths",
  state_model = "Statewide time-stratified\nfixed-effects Poisson\n(daily deaths ~ storm effect\n+ calendar/time strata)",
  state_outputs = "Statewide rate ratios (RRs)\nThreat / Impact / Cleanup vs None\nEvent-time profiles",
  zip_models = "ZIP-level Poisson models\n(deaths ~ storm effect\n+ time fixed effects)",
  zip_rrs = "Shrunken ZIP-level RRs\nby storm phase\n(Threat / Impact / Cleanup)",
  zip_lisa = "Spatial clustering of RRs\nLocal Moran's I\nHot and cold spot maps"
)

boxes$plot_label <- label_map[boxes$id]

# ------------------------------------------------------------
# Define arrows between boxes
# ------------------------------------------------------------
arrows <- tibble(
  from = c(
    "deaths_geo",
    "storms",
    "zcta",
    "panel_all",
    "panel_all",
    "panel_restr",
    "panel_restr",
    "state_model",
    "zip_models",
    "zip_rrs"
  ),
  to = c(
    "panel_all",
    "panel_all",
    "panel_all",
    "panel_restr",
    # If you don't want two arrows from panel_all, comment one of these
    "panel_restr",
    "state_model",
    "zip_models",
    "state_outputs",
    "zip_rrs",
    "zip_lisa"
  )
) %>%
  left_join(boxes %>% select(id, x, y), by = c("from" = "id")) %>%
  rename(xstart = x, ystart = y) %>%
  left_join(boxes %>% select(id, x, y), by = c("to" = "id")) %>%
  rename(xend = x, yend = y) %>%
  mutate(
    # optional small nudges to avoid arrow overlap from top row
    ystart = case_when(
      from == "deaths_geo" ~ ystart - 0.1,
      from == "storms"     ~ ystart,
      from == "zcta"       ~ ystart + 0.1,
      TRUE ~ ystart
    )
  )

# ------------------------------------------------------------
# Plot
# ------------------------------------------------------------
p_fig1 <- ggplot() +
  # Arrows
  geom_curve(
    data = arrows,
    aes(x = xstart, y = ystart, xend = xend, yend = yend),
    curvature = 0.1,
    arrow = arrow(length = unit(0.15, "inches")),
    linewidth = 0.4
  ) +
  # Boxes
  geom_rect(
    data = boxes,
    aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax),
    fill = "white",
    color = "black",
    linewidth = 0.4
  ) +
  # Box labels
  geom_text(
    data = boxes,
    aes(x = x, y = y, label = plot_label),
    size = 3.3,
    lineheight = 0.9
  ) +
  coord_fixed(xlim = c(0, 8), ylim = c(0, 9), expand = FALSE) +
  theme_void() +
  theme(
    plot.margin = margin(10, 10, 10, 10),
    panel.background = element_rect(fill = "white", color = NA)
  )

p_fig1
# p_fig1 + ggtitle("Figure 1. Study design and analysis workflow")
```


### Figure 2: Coefficient plot
```{r}
library(broom)
library(dplyr)
library(stringr)
library(ggplot2)

# 1. Extract log-coefficients + CIs from m_pooled
tt <- tidy(m_pooled, conf.int = TRUE)   # no exponentiation here

# 2. Keep only the storm-effect terms and exponentiate manually
rr_df <- tt %>%
  filter(str_detect(term, "^effect::")) %>%
  mutate(
    storm_phase = str_remove(term, "^effect::"),
    storm_phase = factor(storm_phase,
                         levels = c("Threat", "Impact", "Cleanup")),
    RR      = exp(estimate),
    RR_low  = exp(conf.low),
    RR_high = exp(conf.high)
  )

# Quick sanity check
rr_df[, c("term", "storm_phase", "estimate", "RR", "RR_low", "RR_high")]

# 3. Forest plot: RRs on x-axis, phases on y-axis
p_rr <- ggplot(rr_df, aes(y = storm_phase, x = RR)) +
  geom_vline(xintercept = 1, linetype = "dashed", color = "grey50") +
  geom_point(size = 3) +
  geom_errorbarh(aes(xmin = RR_low, xmax = RR_high),
                 height = 0.2, linewidth = 0.6) +
  labs(
    x = "Rate ratio (RR)",
    y = "Storm phase",
    title = "Statewide rate ratios by storm phase",
    subtitle = "Threat, Impact, and Cleanup relative to None\nFixed-effects Poisson (ZIP, year–month, DOW FE)"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    panel.grid.minor = element_blank(),
    plot.title = element_text(face = "bold", hjust = 0.5),
    plot.subtitle = element_text(hjust = 0.5)
  )

p_rr
```


### Table 1: Coefficients
```{r}
library(broom)
library(dplyr)
library(stringr)

tt <- tidy(m_pooled, conf.int = TRUE)

rr_df <- tt %>%
  filter(str_detect(term, "^effect::")) %>%
  mutate(
    phase = str_remove(term, "^effect::"),
    RR      = exp(estimate),
    RR_low  = exp(conf.low),
    RR_high = exp(conf.high),
    sig     = p.value < 0.05
  ) %>%
  transmute(
    `Storm phase` = phase,
    RR_CI = ifelse(
      sig,
      sprintf("\\textbf{%.2f (%.2f, %.2f)}", RR, RR_low, RR_high),
      sprintf("%.2f (%.2f, %.2f)", RR, RR_low, RR_high)
    ),
    `p-value` = sprintf("%.3f", p.value)
  )
```

```{r}
library(knitr)
library(kableExtra)

kable(
  rr_df,
  format = "latex",
  booktabs = TRUE,
  escape = FALSE,       # allow boldface in RR_CI column
  align = c("l","c","c"),
  col.names = c("Storm phase", "Rate ratio (95\\% CI)", "p-value")
) %>%
  kable_styling(
    latex_options = c("hold_position"),
    font_size = 9
  ) %>%
  add_header_above(
    c(" " = 1, "Effect estimate" = 2),
    bold = TRUE
  ) %>%
  footnote(
    general = "Rate ratios from fixed-effects Poisson model of daily deaths. Reference category is None days. Fixed effects for ZIP, year–month, and day-of-week. Standard errors clustered by ZIP and year–month.",
    general_title = "",
    threeparttable = TRUE
  )
```


### Figure 3: Zip-level maps

Prepare a clean ZIP-phase–RR dataset
```{r}
library(dplyr)
library(sf)
library(ggplot2)
library(RColorBrewer)
library(tidyr)

# --- merge geometry ---
zip_sf <- zctas_valid %>%
  rename(zip = GEOID20)
rr_map <- zip_sf %>%
  left_join(rr_by_zip, by = "zip") %>%
  mutate(
    # Log-scale quantities
    logRR  = log(RR),
    logLCI = log(LCI),
    logUCI = log(UCI),
    logCIwidth = abs(logUCI - logLCI),
    
    # Mask: wide CI or missing
    mask = ifelse(is.na(RR) | is.na(logRR) | logCIwidth > 0.7, TRUE, FALSE),

    # Storm phase as factor with standard order
    effect = factor(effect, levels = c("Threat", "Impact", "Cleanup"))
  )
```

Create a diverging palette symmetric around logRR = 0