---
title: "All-Cause Mortality & Storms"
output: html_document
editor_options:
  chunk_output_type: console
---

## Summary
Experimental setup. We assemble a multi-million–record, geocoded dataset of all-cause deaths (simple-feature points in WGS84) and tag each death as one of four “storm effect” states: None, Threat, Impact, or Cleanup from an empirical storm model. We then aggregate to daily counts by zip code areas and create zip x date storm effect panel data using only days during hurricane seasons (May-November). We fit a fixed-effects Poisson model where the dependent variable is the daily death count and the fixed effects are indicator variables including zip code, year-month, and day of the week. The zip variable controls for all time-invariant differences between zip codes (e.g., population size, socioeconomic level, health infrastructure), the year-month variable controls for month-specific factors common to all zip areas (e.g., statewide seasonality, flu/COVID waves, long-term trends) and the day of the week variable controls for weekly cycles in mortality (e.g., weekend vs. weekday effects). We include a clustering option with the zip and year-month variables to ensure the standard errors are robust to residual correlation within zip codes over time and within months across zip codes. Conceptually the model estimates how daily mortality rates differ on storm-effect days (Threat, Impact, Cleanup) relative to normal “None” days, after adjusting for: differences between zip codes, monthly/seasonal patterns and day-of-week patterns.

Results. The model shows that compared with “None” days, statewide Threat days are associated with an ~8% increase in daily deaths (95% CI ~2%–15%), Cleanup days with ~8%, and Impact days with ~7% (borderline statistical evidence). The adjusted pseudo R-squared is 0.195 and the squared correlation is 0.241 indicating reasonable explanatory power given these are daily death counts with strong fixed effects. A time-stratified model per zip code yields choropleths of these rate ratios, and the Local Moran’s I highlights pockets of elevated (“hot”) and depressed (“cold”) Threat ratios, indicating geographic clustering in storm-related mortality risk. Taken together, the analysis suggests modest but reliable short-run increases in all-cause mortality around storm exposure, with non-uniform spatial patterns across Florida.

## Analytics of deaths and storms

Read the data table to a csv file
```{r}
Deaths.dt <- data.table::fread(here::here("data", "outputs", "All_Deaths_Storm_Effects.csv"))
Deaths.df <- Deaths.dt |>
  as.data.frame()
Deaths.sf <- sf::st_read(dsn = "data/outputs/Deaths", 
                         layer = "All_Deaths_Storm_Effects")
names(Deaths.sf)[1:5] <- names(Deaths.df)[1:5] # match column names with those in the csv
```

Remove rows before 1985 as the first storm since 1981 occurred in that year
```{r}
library(dplyr)
Deaths.df <- Deaths.df |>
  filter(Death_Date >= as.Date("1985-01-01"))
Deaths.sf <- Deaths.sf |>
  filter(Death_Date >= as.Date("1985-01-01"))
Deaths.dt <- Deaths.dt |>
  filter(Death_Date >= as.Date("1985-01-01"))
```

Compute statewide death rates
```{r}
#Deaths.df <- Deaths.df |>
#  mutate(SH = Storm_Category > 0)

total_deaths <- nrow(Deaths.df)
exposed_total <- Deaths.dt[Storm_Effect %in% c("Cleanup", "Impact", "Threat"), .N]

# Calculate death counts and rates for specified storm effects
death_rates <- Deaths.df |>
  filter(Storm_Effect %in% c("Cleanup", "Impact", "Threat")) |>
  group_by(Storm_Effect) |>
#  group_by(Storm_Effect, SH) |>
  summarise(Deaths = n(), .groups = "drop") |>
  mutate(Death_Rate = Deaths / exposed_total)

print(death_rates)
```

This tells you, for example, that .333% of all deaths statewide occurred under an Impact storm condition.

## Death rates per day

Get statewide number of deaths per day by storm effect
```{r}
# Range of dates for which there was a storm effect
range <- Deaths.df |>
  filter(Storm_Effect != "None")
range(range$Death_Date)

# Group deaths by date and storm effect
daily_deaths <- Deaths.df |>
#  group_by(Death_Date, Storm_Effect, SH) |>
  group_by(Death_Date, Storm_Effect) |>
  summarise(deaths = n(), .groups = "drop")
```

Get number of days statewide for each storm effect. This assumes storm effect applies to the whole day — i.e., each death has a unique storm effect
```{r}
num_days <- daily_deaths |>
#  distinct(Death_Date, Storm_Effect, SH) |>
  distinct(Death_Date, Storm_Effect) |>
#  count(Storm_Effect, SH, name = "num_days")
  count(Storm_Effect, name = "num_days")

print(num_days)
```

Total statewide deaths per storm effect
```{r}
total_deaths <- daily_deaths |>
#  group_by(Storm_Effect, SH) |>
  group_by(Storm_Effect) |>
  summarise(total_deaths = sum(deaths), .groups = "drop")

print(total_deaths)
```

Calculate statewide daily death rate
```{r}
# death_rates <- left_join(total_deaths, num_days, by = c("Storm_Effect", "SH")) |>
death_rates <- left_join(total_deaths, num_days, by = "Storm_Effect") |>
  mutate(daily_death_rate = total_deaths / num_days)

print(death_rates)
```

This tells us that in this sample the statewide daily death rate is somewhat higher on non-storm-effect days. But a lot is going on: time and spatial trends, seasonality, autocorrelation. We need a model that accounts for these things

## Model using a negative binomial generalized additive model

Setup the data frame
```{r}
library(lubridate)

dat <- daily_deaths %>%
  transmute(
    date  = as.Date(Death_Date),
    y     = as.integer(deaths),
    effect = factor(Storm_Effect,
                    levels = c("None", "Threat", "Impact", "Cleanup")),
    dow   = wday(date, label = TRUE, week_start = 1), # Mon..Sun
    doy   = yday(date),                               # 1..366 (handles leap days too)
    tnum  = as.integer(date - min(date))              # days since start (well-scaled)
  ) %>%
  mutate(
    # make sure 'dow' is UNordered and with desired reference level:
    dow = factor(as.character(dow),  # drop any ordered class
                 levels = c("Mon","Tue","Wed","Thu","Fri","Sat","Sun"))
  )
```

Generalized linear model (negative binomial)
```{r}
m_glm <- MASS::glm.nb(
  y ~ effect + dow + poly(doy, 3) + poly(tnum, 3),
  data = dat, link =  log
)
summary(m_glm)

acf(residuals(m_glm))
```

This looks good. Exponentiating the Threat effect term of .0283 yields 1.0287. This says that on Threat days the number of deaths is 2.9% higher than on non-effect days and it is marginally significant (p = .015)

But there is large autocorrelation in the residuals. Let's try another model that includes autocorrelation

```{r}
library(gratia)
library(mgcv)

# mark start of series (single series here)
dat$ARstart <- c(TRUE, rep(FALSE, nrow(dat)-1))

# Cyclic smooth needs knots that wrap (0.5, 366.5) for stability
knots_list <- list(doy = c(0.5, 366.5))

m_ar1 <- bam(
  y ~ effect + dow + s(doy, bs = "cc", k = 10) + s(tnum, k = 100),
  family = nb(link = "log"),
  data = dat,
  method = "fREML",
  discrete = TRUE,
  knots = knots_list,
  AR.start = dat$ARstart,
  rho = 0.57  # set to the estimated lag-1 corr
)

summary(m_ar1)
```

Effect days are no longer statistically significant. Autocorrelation inflates z-scores. Plain (quasi/NB) GLMs assume independent residuals. Daily mortality has strong AR structure; ignoring it underestimates SEs, making small effects look “significant.” Adding AR(1) (or equivalently, using time-stratified controls) raises SEs → p-values go up. Also there is collinearity with seasonality/trend. Storms cluster in certain seasons & years. Flexible smooths (or fine time strata) explain a lot of the same variance; the unique contribution of storm indicators shrinks.

Multiple testing/time-of-day confounding (more relevant at ZIP scale), but with many ZIPs, some will look “significant” by chance unless we account for dependence and control false detection rate.

## Poisson model with stratum fixed effects at the state level

Setup the data (as a panel)

Get Florida zips
```{r}
library(tigris)
library(sf)
options(tigris_use_cache = TRUE)

# Get national ZCTAs and subset to those intersecting Florida
zctas_all <- zctas(cb = TRUE, starts_with = NULL, year = 2020)
florida <- states(cb = TRUE) %>% filter(STUSPS == "FL")

# Filter ZCTAs that intersect Florida
zctas_fl <- st_intersection(zctas_all, st_transform(florida, st_crs(zctas_all)))
zctas_fl <- st_make_valid(zctas_fl) # in case of invalid geometries
```

Spatial join deaths to zips
```{r}
# Inspect CRS
st_crs(zctas_fl)   # NAD83 (likely EPSG:4269)
st_crs(Deaths.sf)  # WGS84 (EPSG:4326)

# Harmonize CRS (transform points to the polygons' CRS)
Deaths_aligned <- st_transform(Deaths.sf, st_crs(zctas_fl))

# Make polygons valid (fix self-intersections, rings, etc.)
zctas_valid <- st_make_valid(zctas_fl) %>% dplyr::select(GEOID20, geometry)

sf_use_s2(TRUE)

# Point-in-polygon join: add GEOID20 from polygon to each point
#    st_within() = strictly inside (excludes points exactly on boundary)
deaths_with_zip <- st_join(
  Deaths_aligned,
  zctas_valid,
  join = st_intersects,   # or st_intersects if you want to include boundary points
  left = TRUE
)

# QA checks
# Any points that didn't land in a polygon? (e.g., offshore or boundary issues)
sum(is.na(deaths_with_zip$GEOID20))
# returns 1915 (~.03%) when st_within or st_intersects. Could snap unmatched points to nearest polygon

# Sanity: every point should match at most one ZCTA (ZCTAs are non-overlapping)
dup_check <- deaths_with_zip |>
  st_drop_geometry() |>
  count(Death_ID) |>
  filter(n > 1)
dup_check

# Remove those deaths
deaths_with_zip <- deaths_with_zip |> 
  filter(!is.na(GEOID20))
```

List death dates & storm names by zip and storm effect
```{r}
library(purrr)

df <- deaths_with_zip %>%
  st_drop_geometry()

# A compact tibble with a list-column of Death_Dates
dates_by_effect_df <- df %>%
  group_by(GEOID20, Storm_Effect) %>%
  summarise(
    Death_Dates = list(sort(unique(Death_Date))),   # the actual unique dates (could be more than 1 death)
    Storm_Names = list(unique(Storm_Name)),         # storm names
    n_records   = n(),                               # total records
    n_dates     = length(Death_Dates[[1]]),          # unique dates
    .groups = "drop"
  ) %>%
  arrange(GEOID20, Storm_Effect)

dates_by_effect_df
length(unique(dates_by_effect_df$GEOID20))
```

Daily death counts by zip
```{r}
daily_by_zip <- deaths_with_zip %>%
  st_drop_geometry() %>%
  transmute(
    zip    = as.character(GEOID20),
    date   = as.Date(Death_Date),
    effect = as.character(Storm_Effect)
  ) %>%
  group_by(zip, date, effect) %>%
  summarise(deaths = n(), .groups = "drop")

# Sanity check
daily_by_zip %>%
  count(zip, date) %>%
  filter(n > 1)  # rows here mean the same zip–date appears under multiple effects
```

Join deaths to the calendar. The calendar is created in `HO-storms.Rmd` and is called `effect_calendar`
```{r}
dbz <- as.data.table(daily_by_zip)[ , .(
  zip   = as.character(zip),
  date  = as.IDate(date),
  deaths = as.integer(deaths)
)]

zip_day <- dbz[effect_calendar, on = .(zip, date)]
zip_day[is.na(deaths), deaths := 0L]  # fill missing counts with 0

# After joining deaths: zeros should appear across all effects
zip_day[, .(zeros = sum(deaths == 0L), rows = .N), by = effect][order(effect)]
```

Then we fit a Poisson with stratum fixed effects. This is equivalent to conditioning on the stratum totals and largely removes serial confounding. Use robust (clustered) SEs by stratum to protect against residual correlation within a stratum

Add strata variables
```{r}
dat <- zip_day %>%
  filter(month(date) %in% 5:11) %>%  # hurricane season only
  transmute(
    zip    = as.character(zip),
    date   = as.IDate(date),
    deaths = as.integer(deaths),
    effect = factor(effect, levels = c("None","Threat","Impact","Cleanup")),
    dow     = factor(strftime(date, "%u"), levels = as.character(1:7),
                       labels = c("Mon","Tue","Wed","Thu","Fri","Sat","Sun")),
    ym     = format(date, "%Y-%m")                      # year-month stratum
  )
```

Fit model
```{r}
library(fixest)

m_pooled <- fepois(
  deaths ~ i(effect, ref = "None") | zip + ym + dow,
  data    = dat,
  cluster = ~ zip + ym   # two-way clustered SEs (ZIP & month)
)

summary(m_pooled)

# Rate ratios (RR) with CIs
library(broom)

rr_tbl <- tidy(m_pooled, conf.int = TRUE) %>%                # returns log-scale
  filter(grepl("^effect::", term)) %>%
  transmute(
    effect = sub("^effect::", "", term),
    RR     = exp(estimate),
    LCI    = exp(conf.low),
    UCI    = exp(conf.high),
    p      = p.value
  )
rr_tbl
```

We fit a fixed-effects Poisson with: ZIP fixed effects (| zip) → controls for all time-invariant differences across ZIPs (population size, baseline mortality, socio-demographics, etc.), Year-month fixed effects (| ym) → controls for statewide shocks and seasonality at the month level (flu waves, hurricanes seasons, COVID waves, etc.). Day-of-week fixed effects (| dow) → controls for weekly pattern. So the effect:: coefficients are within-ZIP log rate ratios vs “None”, after removing month and weekday patterns. Exponentiate them to get rate ratios (RR).

What the coefficients mean
We fit a fixed-effects Poisson with: ZIP fixed effects (| zip) → controls for all time-invariant differences across ZIPs (population size, baseline mortality, socio-demographics, etc.).
Year-month fixed effects (| ym) → controls for statewide shocks and seasonality at the month level (flu waves, hurricanes seasons, COVID waves, etc.).
Day-of-week fixed effects (| dow) → controls for weekly pattern.
So the effect::… coefficients are within-ZIP log rate ratios vs “None”, after removing month and weekday patterns. Exponentiate them to get rate ratios (RR). Convert the estimates to RRs (95% CI)
Threat: β = 0.07994 (SE 0.02950), p = 0.0067
RR = 1.083 (95% CI 1.022–1.148) → ~8.3% higher deaths vs None, adjusted for ZIP / YM / DOW.
Impact: β = 0.06389 (SE 0.03286), p = 0.0518
RR = 1.066 (95% CI 1.000–1.137) → ~6.6% higher; borderline at 0.052.
Cleanup: β = 0.07333 (SE 0.03045), p = 0.0160
RR = 1.076 (95% CI 1.014–1.142) → ~7.6% higher.
(RR = exp(β); CI = exp(β ± 1.96·SE).)

NOTE: 44/0/0 fixed-effects (357,808 observations) removed because of only 0 outcomes or singletons.
You have 3 FE groups (zip / ym / dow). The 44/0/0 means 44 ZIP levels were dropped (0 YM, 0 DOW) because, within those ZIPs, the data contributed no identifying variation (e.g., all days had zero deaths or only singleton cells after stratification). The corresponding 357,808 rows were removed from estimation. That’s expected behavior: those ZIPs can’t help identify contrasts after the FE are applied.

Log-likelihood and BIC are for the Poisson FE fit. “Adj. Pseudo R²: 0.195” and “Squared Cor.: 0.241” indicate reasonable explanatory power given this is daily death counts with strong FE.

Compared with “None” days, statewide Threat days are associated with an ~8% increase in daily deaths (95% CI ~2%–15%), Cleanup days with ~8%, and Impact days with ~7% (borderline statistical evidence).

Try negative binomial fixed effects as a sensitivity
```{r}
m_pooled2 <- fenegbin(
  deaths ~ i(effect, ref = "None") | zip + ym + dow,
  data    = dat,
  cluster = ~ zip + ym   # two-way clustered SEs (ZIP & month)
)

summary(m_pooled2)
```

Results are essentially the same

## Time stratified Poisson fixed effect per zip code area

Choose candidate zips worth modeling
```{r}
cands <- dat[
  , .(
      has_none = any(effect == "None"),
      has_tic  = any(effect != "None"),
      pos_y    = any(deaths > 0L)
    ),
  by = zip
][has_none & has_tic & pos_y, zip]
length(cands) # returns 999

# Optionally require a minimum number of storm-effect days
min_effect_days <- 5L
cands2 <- dat[
  , .(n_tic = sum(effect != "None")),
  by = zip
][n_tic >= min_effect_days, zip]
length(cands2) # returns 993
```

Per-zip poisson fixed effect (stratify by ym + dow)
```{r}
library(pbapply)

# Function for a single zip code
fit_one <- function(df) {
  df[, effect := factor(effect, levels = c("None","Threat","Impact","Cleanup"))]
  # Skip if baseline mean is zero (RR not defined)
  if (!any(df$effect == "None") || mean(df$deaths[df$effect == "None"]) == 0) return(NULL)
  tryCatch(
    fepois(
      deaths ~ i(effect, ref = "None") | ym + dow,
      data    = df,
      cluster = ~ ym
    ),
    error = function(e) NULL
  )
}

# sequential (shows a progress bar)
models <- pblapply(cands, function(z) fit_one(dat[zip == z]))
names(models) <- cands
```

Extract relative rates (and 95% CIs) per zip for Threat, Impact, Cleanup
```{r}
rr_by_zip <- bind_rows(lapply(names(models), function(z) {
  m <- models[[z]]
  if (is.null(m)) return(NULL)
  tidy(m, conf.int = TRUE) %>%
    filter(grepl("^effect::", term)) %>%
    transmute(
      zip    = z,
      effect = sub("^effect::", "", term),
      RR     = exp(estimate),
      LCI    = exp(conf.low),
      UCI    = exp(conf.high),
      p      = p.value
    )
}))

# Optional: keep only Threat/Impact/Cleanup rows
rr_by_zip <- rr_by_zip %>% filter(effect %in% c("Threat","Impact","Cleanup"))
```

The extreme RRs are a data-sparsity/separation artifact of per-ZIP, heavily stratified Poisson fits

Where are the extremes coming from?
```{r}
# per-ZIP support by effect
zip_support <- dat2 %>%      # dat = per-ZIP daily frame 
  group_by(zip, effect) %>%
  summarise(
    days   = n(),
    deaths = sum(deaths),
    mean_y = mean(deaths),
    .groups = "drop"
  )

# join support onto the RR table
rr_chk <- rr_by_zip %>%
  left_join(zip_support %>% rename(days_e = days, deaths_e = deaths, mean_e = mean_y),
            by = c("zip","effect")) %>%
  left_join(zip_support %>% filter(effect=="None") %>%
              select(zip, days_none = days, deaths_none = deaths, mean_none = mean_y),
            by = "zip")

# look at the pathological ones
rr_chk %>%
  filter(RR < 0.2 | RR > 5 | !is.finite(RR)) %>%
  arrange(RR) %>%
  select(zip, effect, RR, LCI, UCI, p, days_e, deaths_e, days_none, deaths_none)
```

Empirical Bayes shrinkage before mapping
```{r}
# Pooled FE Poisson to get a prior mean per effect (log scale)
# This was done above and saved as `m_pooled`
#pooled <- fixest::fepois(deaths ~ i(effect, "None") | zip + ym + dow, data = dat, cluster = ~ zip + ym)
pooled_eff <- broom::tidy(m_pooled, conf.int = FALSE) %>%
  filter(grepl("^effect::", term)) %>%
  transmute(effect = sub("^effect::","",term), mu0 = estimate)   # log-RR prior

# Approximate per-ZIP log-RR variance from your model (delta from CI)
log_rr <- rr_by_zip %>%
  mutate(
    logRR   = log(RR),
    se_log  = (log(UCI) - log(LCI)) / (2*1.96)
  ) %>% filter(is.finite(logRR), se_log > 0)

# Shrink: posterior mean = (logRR / se^2 + mu0 / tau^2) / (1/se^2 + 1/tau^2)
#    Use a single tau per effect (between-ZIP SD). Estimate tau via robust SD.
tau_by_eff <- log_rr %>% group_by(effect) %>%
  summarise(tau = stats::mad(logRR, constant = 1), .groups = "drop")

rr_shrunk <- log_rr %>%
  left_join(pooled_eff, by="effect") %>%
  left_join(tau_by_eff, by="effect") %>%
  mutate(
    prec_obs = 1/(se_log^2),
    prec_pri = 1/(pmax(tau, 1e-6)^2),
    post     = (logRR*prec_obs + mu0*prec_pri) / (prec_obs + prec_pri),
    RR_shr   = exp(post)
  ) %>%
  select(zip, effect, RR_shr)
```

Join to zip polygons and map
```{r}
library(ggplot2)
library(forcats)

# Choose which effect to map (e.g., "Impact"); swap to "Threat"/"Cleanup" as needed
effect_to_map <- "Threat"

# Build an sf for mapping (join your RR table to polygons)
zcta_sf <- zctas_fl %>% select(zip = GEOID20, geometry)

map_sf <- rr_shrunk %>%
  filter(effect == effect_to_map, is.finite(RR_shr), RR_shr > 0, RR_shr < 10) %>%  # keep sensible RRs
  right_join(zcta_sf, by = "zip") %>%                          # keep all polygons
  st_as_sf()

# Bin RRs: [0,1), [1,2), [2,4), [4,∞)
map_sf <- map_sf %>%
  mutate(
    rr_bin = cut(
      RR_shr,
      breaks = c(0, 1, 2, 4, Inf),  # last break is Inf
      labels = c("0–1", "1–2", "2–4", ">4"),
      right  = TRUE,                   # (a, b] so last bin is (8, Inf]
      include.lowest = TRUE
    ),
    # lock the level order to match the palette keys
    rr_bin = fct_relevel(rr_bin, "0–1", "1–2", "2–4", ">4")
  )

# Define a discrete palette (colorblind-friendly)
pal <- c(
  "0–1" = "#fbb4b9",  # light
  "1–2" = "#9ecae1",
  "2–4" = "#6baed6",
  ">4"  = "#08519c"
)

# Plot
ggplot(map_sf) +
  geom_sf(aes(fill = rr_bin), color = NA) +
  scale_fill_manual(
    values   = pal,
    limits = levels(map_sf$rr_bin),
    drop     = FALSE,
    na.value = "grey90",
    name     = paste0("RR vs None (", effect_to_map, ")")
  ) +
  labs(
    title = paste("Per-ZIP Rate Ratios on", effect_to_map, "days"),
    subtitle = "Bins: 0–1, 1–2, 2–4, >4",
    caption = "RRs from per-ZIP time-stratified Poisson fixed-effect"
  ) +
  theme_minimal() +
  theme(
    legend.position = "right",
    panel.grid.major = element_line(color = "white"),
    axis.title = element_blank()
  )

# Sanity check
table(map_sf$rr_bin, useNA = "ifany")
sum(is.infinite(map_sf$RR_shr))
```

Output for story-map
```{r}
saveRDS(map_sf, "data/outputs/Results/zcta_results2.rds")
```


## Hot spot analysis

Using Moran's I and k-nearest neighbors for spatial contiguity
```{r}
Moran_sf <- map_sf %>%
  filter(!is.na(RR_shr)) %>%
  select(zip, RR = RR_shr, geometry)

Moran_sf <- Moran_sf %>%
  filter(st_geometry_type(geometry) %in% c("POLYGON", "MULTIPOLYGON")) %>%
  st_cast("MULTIPOLYGON") # Ensure polygon geometry
```

Create spatial weights
```{r}
centroids <- st_centroid(Moran_sf)
coords <- st_coordinates(centroids)
# Choose k (e.g., 5 or 8 neighbors)
k <- 7
knn <- spdep::knearneigh(coords, k = k)
nb <- spdep::knn2nb(knn)

# Convert to spatial weights
lw <- spdep::nb2listw(nb, style = "W")
```

Run local hot spot
```{r}
# Calculate Local Moran’s I

# 1) Make sure 'RR' is a plain numeric vector (drop geometry to avoid tibble quirks)
x <- Moran_sf %>% st_drop_geometry() %>% pull(RR) %>% as.numeric()

# 2) Keep only finite values and subset the weights accordingly
ok <- is.finite(x)

# If your listw 'lw' was built from Moran_sf in the same row order:
lw_ok <- spdep::subset.listw(lw, ok)

# 3) Run local Moran’s I on the clean vector, allowing for isolated units if any
lisa_ok <- spdep::localmoran(x[ok], lw_ok, zero.policy = TRUE)

# Add results to the sf object
Moran_sf <- Moran_sf %>%
  mutate(
    local_i = lisa_ok[, 1],         # Local Moran's I
    p_value = lisa_ok[, 5],         # p-value
    hotspot_type = case_when(
      p_value < 0.15 & RR > mean(RR, na.rm = TRUE) ~ "Hot Spot",
      p_value < 0.15 & RR < mean(RR, na.rm = TRUE) ~ "Cold Spot",
      TRUE ~ "Not Significant"
    )
  )

# For input to story map
saveRDS(rr_sf, "data/outputs/Results/hot_spot_threat.rds")
```

```{r}
dat <- Moran_sf

top10 <- dat %>%
  filter(hotspot_type == "Hot Spot") %>%
  arrange(desc(RR)) %>%
  slice_head(n = 10) %>%
  select(zip, RR, hotspot_type)
top10

bottom10 <- dat %>%
  filter(hotspot_type == "Cold Spot") %>%
  arrange(desc(RR)) %>%
  slice_tail(n = 10) %>%
  select(zip, RR, hotspot_type)
bottom10

top10
bottom10
```

Map the results
```{r}
library(ggplot2)

ggplot(Moran_sf) +
  geom_sf(aes(fill = hotspot_type), color = "gray90", size = 0.1) +
  scale_fill_manual(values = c(
    "Hot Spot" = "red",
    "Cold Spot" = "blue",
    "Not Significant" = "gray90"
  )) +
  labs(title = "Local Hot Spot Analysis (Threat)", fill = "Cluster Type") +
  theme_minimal()
```

## Emergency room visits

Data received from David Hsu on October 16, 2025 via email
```{r}
ER_visits.dt <- data.table::fread(here::here("data", "Combined_ER_Data.csv"))
ER_visits.df <- ER_visits.dt |>
  as.data.frame()
```

Merge with daily mortality counts
```{r}
library(data.table)

setDT(zip_day)
setDT(ER_visits.df)

# Pad ZipCode to 5 chars; drop non-sensical codes (0/NA/too short/too big)
ER_visits.df[
  , ZipCode := fifelse(ZipCode > 0 & ZipCode < 1e6, sprintf("%05d", ZipCode), NA_character_)
]
ER_visits.df[, Date := as.IDate(Date)]

ER_visits_clean <- ER_visits.df[!is.na(ZipCode) & nchar(ZipCode) == 5]

# Standardize names to match zip_day; optionally collapse duplicates
setnames(ER_visits_clean, c("ZipCode","Date","Total_Visits"),
                           c("zip",    "date","er_visits"))

# If there can be multiple ER rows per (zip,date), aggregate to daily total
ER_visits_clean <- ER_visits_clean[, .(er_visits = sum(er_visits, na.rm = TRUE)),
                                   by = .(zip, date)]

# Standardize zip_day types 
zip_day[, date := as.IDate(date)]
zip_day[, zip  := as.character(zip)] # already looks like "31537", but safe

# Merge: keep all zip_day rows (left join) 
zip_day_with_visits <- merge(
  zip_day,
  ER_visits_clean,
  by = c("zip","date"),
  all.x = TRUE,
  allow.cartesian = TRUE  # only matters if duplicates remain
)

# Optional: treat missing ER as 0 instead of NA
zip_day_with_visits[is.na(er_visits), er_visits := 0L]
```

A model for daily death counts using ER visits as a covariate and test whether slopes differ by effect
```{r}
library(fixest)

dat <- copy(zip_day_with_visits)

# --- Build filters and control variables ---
dat[, year  := as.integer(format(date, "%Y"))]
dat[, month := as.integer(format(date, "%m"))]

# Keep hurricane season & target years
dat <- dat[year %between% c(2005, 2022) & month %in% 5:11]

# Seasonality: month-of-year (factor, May..Nov)
dat[, month_fac := factor(month, levels = 5:11, labels = month.abb[5:11])]

# Day-of-week (factor)
dow_levels <- c("Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday")
dat[, dow := factor(weekdays(as.Date(date)), levels = dow_levels)]

# Time trend: numeric index (scale to years for stability)
dat[, t_years := as.numeric(date) / 365.25]

# For clustering by year-month (optional but recommended)
dat[, ym := format(date, "%Y-%m")]

# Optional: handle missing ER visits (choose either 0 or keep as NA and drop)
dat[is.na(er_visits), er_visits := 0L]

# Optional: scale ER for easier interpretation (e.g., per 10 visits)
dat[, er10 := er_visits / 10]

m_er_int <- fepois(
  deaths ~ 
    er10 +                          # baseline ER slope on "None" days
    i(effect, ref = "None") +       # mean shifts for Threat/Impact/Cleanup vs None
    i(effect, var = er10, ref = "None") +  # ER slope *differences* by effect
    poly(t_years, 2) |              # smooth trend
    zip + month_fac + dow,          # fixed effects (ZIP, seasonality, DOW)
  data    = dat,
  cluster = ~ zip + ym
)

summary(m_er_int)
```

The model says that there is a 1.5% increase in daily deaths per 10 additional ER visits, but that increase is not changed by storm effect.

Censored Poison via {brms} package
```{r}
install.packages("cmdstanr",
                 repos = c("https://stan-dev.r-universe.dev", getOption("repos")))
library(cmdstanr)
cmdstanr::check_cmdstan_toolchain(fix = TRUE)    # confirms your compiler setup
cmdstanr::install_cmdstan(cores = 2)             # downloads & builds CmdStan
```

```{r}
library(brms)

dat <- copy(zip_day_with_visits)

# Restrict to hurricane season, 2005–2022
dat[, year  := as.integer(format(date, "%Y"))]
dat[, month := as.integer(format(date, "%m"))]
dat <- dat[year %between% c(2005, 2022) & month %in% 5:11]

# How many days have missing values for er_visits

# Storm effect factor with "None" as ref
dat[, effect := factor(effect, levels = c("None","Threat","Impact","Cleanup"))]

# Censoring indicator per brms: "left" for censored obs, "none" otherwise
dat[, cens_flag := ifelse(er_visits == 6L, "left", "none")]

# Controls: weekday, month FE; trend as smooth; ZIP as varying intercept (random effect)
# (Random intercepts approximate ZIP FE while keeping the model computationally feasible.)
dow_levels <- c("Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday")
dat[, dow := factor(weekdays(as.Date(date)), levels = dow_levels)]
dat[, ym  := format(date, "%Y-%m")]
dat[, t_years := as.numeric(date)/365.25]

bf_er <- bf(
  er_visits | cens(cens_flag) ~ 
    effect + dow + factor(month) + s(t_years, k = 6) + (1 | zip),
  family = poisson()
)

# Reasonable, weakly informative priors
pri <- c(
  set_prior("normal(0, 0.5)", class = "b"),       # population-level
  set_prior("exponential(1)", class = "sd"),      # group-level std dev
  set_prior("exponential(1)", class = "sds")      # smooth term
)

# Fit (uses Stan; uses cmdstanr backend)
fit_cens <- brm(
  bf_er, data = dat,
  prior = pri,
  backend = "cmdstanr", chains = 4, cores = 4, iter = 2000
)

summary(fit_cens)
```












Clean ER data: pad zips, ensure Date class
```{r}
er_clean <- ER_visits.df %>%
  mutate(
    Date    = as.Date(Date),
    ZipCode = str_pad(as.character(ZipCode), width = 5, pad = "0")
  )
```

Expand list of dates per GEOID20 × Storm_Effect, then join to ER data
```{r}
er_match_long <- dates_by_effect_df %>%
  select(GEOID20, Storm_Effect, Death_Dates, n_dates) %>%
  unnest_longer(Death_Dates, values_to = "Date") %>%  # one row per death date
  rename(Death_Date = Date) %>%
  left_join(
    er_clean,
    by = c("GEOID20" = "ZipCode", "Death_Date" = "Date")
  )
```

Compute averages per GEOID20 × Storm_Effect
```{r}
er_avg_by_effect <- er_match_long %>%
  group_by(GEOID20, Storm_Effect) %>%
  summarise(
    avg_er_visits = if (all(is.na(Total_Visits))) NA_real_ else mean(Total_Visits, na.rm = TRUE),
    n_er_rows     = sum(!is.na(Total_Visits)),
    n_dates       = first(n_dates),
    coverage      = ifelse(n_dates > 0, n_er_rows / n_dates, NA_real_),
    .groups = "drop"
  )
```

Attach the averages back to dates_by_effect_df
```{r}
dates_by_effect_with_er <- dates_by_effect_df %>%
  left_join(er_avg_by_effect, by = c("GEOID20", "Storm_Effect"))

dates_by_effect_with_er %>%
  group_by(Storm_Effect) %>%
  summarise(Averages = mean(avg_er_visits, na.rm = TRUE))
```

## Confirm (or challenge) results of the mortality rates using ER visit data

Build ER effect-day counts (per ZIP × day)
```{r}
library(dplyr); library(tidyr); library(stringr); library(lubridate)

# ER: make sure it's daily per ZIP
er <- ER_visits.df %>%
  mutate(Date = as.Date(Date),
         GEOID20 = str_pad(as.character(ZipCode), width = 5, side = "left", pad = "0")) %>%
  group_by(GEOID20, Date) %>%
  summarise(Total_Visits = sum(Total_Visits), .groups = "drop")

# Storm effect calendar: expand your list-column to a daily calendar
cal <- dates_by_effect_df %>%
  select(GEOID20, Storm_Effect, Death_Dates) %>%
  unnest_longer(Death_Dates, values_to = "Date") %>%
  mutate(flag = 1L) %>%
  pivot_wider(names_from = Storm_Effect, values_from = flag, values_fill = 0L)

# Join ER with the calendar (non-matches => None)
er_cal <- er %>%
  right_join(
    expand_grid(GEOID20 = unique(cal$GEOID20), Date = seq(min(cal$Date), max(cal$Date), by="day")),
    by = c("GEOID20","Date")
  ) %>%
  left_join(cal, by = c("GEOID20","Date")) %>%
  mutate(across(all_of(c("Impact","Threat","Cleanup","None")), ~replace_na(.x, 0L)),
         None = ifelse((Impact+Threat+Cleanup)==0, 1L, None),
         Total_Visits = coalesce(Total_Visits, 0L))   # or NA if you prefer strict coverage
```

Create long data frame of zcta_results
```{r}
mort_rr_long <- zcta_results %>%
  st_drop_geometry() %>%
  transmute(
    GEOID20 = str_pad(as.character(GEOID20), 5, side = "left", pad = "0"),
    RR_Impact  = as.numeric(RR_Impact),
    RR_Threat  = as.numeric(RR_Threat),
    RR_Cleanup = as.numeric(RR_Cleanup)
  ) %>%
  pivot_longer(
    cols = starts_with("RR_"),
    names_to = "Storm_Effect",
    names_pattern = "RR_(.*)",
    values_to = "RR_Mort"
  ) %>%
  filter(is.finite(RR_Mort)) %>%
  mutate(
    Storm_Effect = factor(Storm_Effect, levels = c("Impact","Threat","Cleanup"))
  )
```

Aggregate to weeks so most weeks exceed the threshold even if some days don’t. Model weekly totals with offsets for partial coverage and with storm-effect exposure defined as days per week.
```{r}
library(dplyr)
library(lubridate)
library(mgcv)
# weekly table, but add a compact time index
wk <- er_cal %>%
  mutate(week = floor_date(as.Date(Date), "week", week_start = 1)) %>%
  group_by(GEOID20, week) %>%
  summarise(
    visits_wk    = sum(as.numeric(Total_Visits), na.rm = TRUE),
    n_obs_days   = sum(!is.na(Total_Visits)),
    Impact_days  = sum((Impact  > 0L), na.rm = TRUE),
    Threat_days  = sum((Threat  > 0L), na.rm = TRUE),
    Cleanup_days = sum((Cleanup > 0L), na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(
    GEOID20     = factor(sprintf("%05s", as.character(GEOID20))),
    doy         = yday(week),                                   # 1..366
    tw          = as.integer(difftime(week, min(week), "weeks")),# small integer time
    offset_days = log(pmax(n_obs_days, 1L))
  )

wk_fit <- wk %>% filter(n_obs_days >= 3)

# knots for cyclic day-of-year smooth
knots_list <- list(doy = c(0.5, 366.5))

# Use bam() + discrete = TRUE (fast, memory-efficient) + fREML
m_wk <- bam(
  visits_wk ~ Impact_days + Threat_days + Cleanup_days +
    s(doy, bs = "cc", k = 12) +       # smaller basis is plenty weekly
    s(tw,  k = 30) +                  # trend on compact integer time
    s(GEOID20, bs = "re"),            # random intercept by ZIP
  family   = nb(link = "log"),
  offset   = offset_days,
  data     = wk_fit,
  method   = "fREML",
  discrete = TRUE,
  nthreads = max(1, parallel::detectCores() - 1),
  knots    = knots_list
)
summary(m_wk)
```

```{r}
sel <- c("Impact_days","Threat_days","Cleanup_days")
b   <- coef(m_wk)[sel]
V   <- vcov(m_wk)[sel, sel, drop = FALSE]
se  <- sqrt(diag(V))

out <- data.frame(
  Storm_Effect = c("Impact","Threat","Cleanup"),
  RR_week      = exp(b),
  RR_lo        = exp(b - 1.96*se),
  RR_hi        = exp(b + 1.96*se),
  row.names = NULL
)
out
```

Cleanup days show a clear increase in ER load per additional day that week (≈ +1.7% each). Threat days show a small but significant decrease (≈ −0.8% each) — plausible if people defer non-urgent care on days with active threats. Impact days show no clear aggregate effect at the weekly scale (could reflect offsetting mechanisms: acute spikes in some ZIPs/weeks vs access barriers or suppression elsewhere).

Allow for more zipcode-specific variation (do it in stages)
```{r}
library(dplyr); library(lubridate)

wk <- er_cal %>%
  mutate(week = floor_date(as.Date(Date), "week", week_start = 1)) %>%
  group_by(GEOID20, week) %>%
  summarise(
    visits_wk    = sum(as.numeric(Total_Visits), na.rm = TRUE),
    n_obs_days   = sum(!is.na(Total_Visits)),
    Impact_days  = sum(Impact  > 0L, na.rm = TRUE),
    Threat_days  = sum(Threat  > 0L, na.rm = TRUE),
    Cleanup_days = sum(Cleanup > 0L, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(
    GEOID20     = factor(sprintf("%05s", as.character(GEOID20))),
    doy         = yday(week),
    tw          = as.integer(difftime(week, min(week), "weeks")),
    offset_days = log(pmax(n_obs_days, 1L))
  )

wk_fit <- wk %>% filter(n_obs_days >= 3)

# Fourier seasonality (K = 2)
K <- 2
fourier <- purrr::map_dfc(1:K, function(k) {
  tibble::tibble(
    !!paste0("sin", k) := sin(2*pi*k*wk_fit$doy/366),
    !!paste0("cos", k) := cos(2*pi*k*wk_fit$doy/366)
  )
})
wk_fit <- dplyr::bind_cols(wk_fit, fourier)
fourier_terms <- paste(names(fourier), collapse = " + ")

library(glmmTMB)

# Model A — Random intercept only (should converge)
form_A <- as.formula(
  paste(
    "visits_wk ~ Impact_days + Threat_days + Cleanup_days +",
    fourier_terms, "+ poly(tw, 2) + (1 | GEOID20)"
  )
)

m_A <- glmmTMB(
  form_A,
  family = nbinom2,
  offset = wk_fit$offset_days,
  data   = wk_fit,
  control = glmmTMBControl(optimizer = optim, optArgs = list(method = "BFGS"))
)
summary(m_A)


# Model B - One random slope (Threat term)

library(glmmTMB)

# Keep your wk_fit with K=2 Fourier terms
form_B <- as.formula(
  paste(
    "visits_wk ~ Impact_days + Threat_days + Cleanup_days +",
    "sin1 + cos1 + sin2 + cos2 + poly(tw, 2)",
    "+ (1 | GEOID20) + (0 + Threat_days | GEOID20)"  # or Cleanup_days
  )
)

m_B <- glmmTMB(
  form_B, family = nbinom2,
  offset = wk_fit$offset_days, data = wk_fit,
  control = glmmTMBControl(optimizer = optim, optArgs = list(method = "BFGS"))
)
summary(m_B)

```



## Attach census data

```{r}
# Packages
library(tidycensus)
library(dplyr)
library(tidyr)
library(stringr)
library(purrr)
library(sf)
library(tigris)
```

```{r}
# Make sure your Census key is set once per session
#census_api_key("xxxx", install = FALSE)
options(tigris_use_cache = TRUE)
```

```{r}
# --- Helper: get Decennial 2000 variables ---
# SF1 (population & age/sex); SF3 (income)
sf1_vars <- load_variables(2000, "sf1", cache = TRUE)
sf3_vars <- load_variables(2000, "sf3", cache = TRUE)

# Total population (= P001001 in 2000 SF1)
pop_var <- "P001001"

# Median age (total). In 2000 SF1 this is P013001 "Median age -- Total"
median_age_var <- "P013001"

# For % 65+, we’ll sum all SF1 P012 (sex by age) components whose labels contain "65 years and over"
age_table <- "P012"  # sex by age
age_vars_2000 <- sf1_vars %>%
  filter(str_starts(name, age_table)) %>%
  mutate(is_65plus = str_detect(label, "65 to|66 to|67 to|68 to|69 to|70 to|71 to|72 to|73 to|74 to|75 to|76 to|77 to|78 to|79 to|80 to|81 to|82 to|83 to|84 to|85 years")) %>%
  select(name, is_65plus)

age_vars_65plus <- age_vars_2000 %>% filter(is_65plus) %>% pull(name)

# Median household income (1999$) from 2000 SF3
# In 2000 SF3 this is P053001: "Median household income in 1999 (dollars)"
mhi_var <- "P053001"
```

```{r}
# Pull Decennial 2000 SF1: population + median age
sf1_core <- get_decennial(
  geography = "zcta",
  year = 2000,
  variables = c(total_pop = pop_var, median_age = median_age_var),
  geometry = FALSE,
  output = "wide",
  sumfile = "sf1"
) %>%
  rename(GEOID = GEOID) %>%
  mutate(ZCTA5 = GEOID)

# Pull 65+ counts and compute percent 65+
sf1_age <- get_decennial(
  geography = "zcta",
  year = 2000,
  variables = age_vars_2000$name,
  geometry = FALSE,
  output = "wide",
  sumfile = "sf1"
) %>%
  rename(ZCTA5 = GEOID)

# total population again (P001001) to be safe for % calculation
sf1_pop_only <- get_decennial(
  geography = "zcta",
  year = 2000,
  variables = pop_var,
  geometry = FALSE,
  output = "wide",
  sumfile = "sf1"
) %>%
  rename(ZCTA5 = GEOID, pop2000 = P001001)

# sum 65+ across all matching P012 components
sf1_age_65pct <- sf1_age %>%
  mutate(across(all_of(age_vars_65plus), as.numeric)) %>%
  transmute(ZCTA5,
            age65plus_2000 = rowSums(across(all_of(age_vars_65plus)), na.rm = TRUE)) %>%
  left_join(sf1_pop_only, by = "ZCTA5") %>%
  mutate(pct_65plus_2000 = if_else(pop2000 > 0, 100 * age65plus_2000 / pop2000, NA_real_)) %>%
  select(ZCTA5, age65plus_2000, pop2000, pct_65plus_2000)

# Pull Decennial 2000 SF3: median household income (1999$)
sf3_income <- get_decennial(
  geography = "zcta",
  year = 2000,
  variables = c(median_hh_income_1999 = mhi_var),
  geometry = FALSE,
  output = "wide",
  sumfile = "sf3"
) %>%
  rename(ZCTA5 = GEOID)

# 5) Land & water area for 2020 ZCTAs to match rr_sf$GEOID20
# (ALAND/AWATER are in square meters; this uses 2020 TIGER shapefiles to align with GEOID20)
zcta20 <- tigris::zctas(year = 2020, cb = TRUE, progress_bar = FALSE) %>%
  st_drop_geometry() %>%
  transmute(GEOID20 = ZCTA5CE20,
            land_area_m2_2020 = ALAND20,
            water_area_m2_2020 = AWATER20)

# 6) Assemble a tidy 2000 attributes table keyed to 5-digit ZCTA
attrs2000 <- sf1_core %>%
  transmute(ZCTA5,
            pop_2000 = total_pop,
            median_age_2000 = median_age) %>%
  left_join(sf1_age_65pct, by = "ZCTA5") %>%
  left_join(sf3_income, by = "ZCTA5") %>%
  # Rename income from wide column name
  rename(median_hh_income_1999 = median_hh_income_1999) %>%
  # Make sure ZCTA key is 5-digit character
  mutate(ZCTA5 = str_pad(ZCTA5, width = 5, pad = "0"))

# 7) Join to your rr_sf (assumes rr_sf$GEOID20 is the 5-digit ZCTA code as character)
rr_sf_final <- rr_sf %>%
  mutate(GEOID20 = as.character(GEOID20),
         ZCTA5 = GEOID20) %>%
  left_join(attrs2000, by = "ZCTA5") %>%
  left_join(zcta20, by = "GEOID20") %>%
  # Population density per km^2 using 2020 land area (best align with GEOID20)
  mutate(pop_density_2000_per_km2 = if_else(!is.na(land_area_m2_2020) & land_area_m2_2020 > 0,
                                            pop_2000 / (land_area_m2_2020 / 1e6),
                                            NA_real_))

rr <- rr_sf_final %>%
  filter(hotspot_type %in% c("Hot Spot", "Cold Spot"))
```
