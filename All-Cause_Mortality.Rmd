---
title: "All-Cause Mortality & Storms"
output: html_document
editor_options:
  chunk_output_type: console
---

## Analytics of deaths and storms

Read the data table to a csv file
```{r}
Deaths.dt <- data.table::fread(here::here("data", "outputs", "All_Deaths_Storm_Effects.csv"))
Deaths.df <- Deaths.dt |>
  as.data.frame()
```

```{r}
library(dplyr)

#Deaths.df <- Deaths.df |>
#  mutate(SH = Storm_Category > 0)

total_deaths <- nrow(Deaths.df)
exposed_total <- Deaths.dt[Storm_Effect %in% c("Cleanup", "Impact", "Threat"), .N]

# Calculate death counts and rates for specified storm effects
death_rates <- Deaths.df |>
  filter(Storm_Effect %in% c("Cleanup", "Impact", "Threat")) |>
  group_by(Storm_Effect) |>
#  group_by(Storm_Effect, SH) |>
  summarise(Deaths = n(), .groups = "drop") |>
  mutate(Death_Rate = Deaths / exposed_total)

# View result
print(death_rates)
```

This tells you, for example, that .333% of all deaths occurred under an Impact storm condition.

## Death rates per day

Step 1: Get number of deaths per day by Storm_Effect
```{r}
# Group deaths by date and storm effect
daily_deaths <- Deaths.df |>
#  group_by(Death_Date, Storm_Effect, SH) |>
  group_by(Death_Date, Storm_Effect) |>
  summarise(deaths = n(), .groups = "drop")
```

Step 2: Get number of days for each storm effect
This step assumes Storm_Effect applies to the whole day — i.e., each Death_Date has a unique Storm_Effect
```{r}
num_days <- daily_deaths |>
#  distinct(Death_Date, Storm_Effect, SH) |>
  distinct(Death_Date, Storm_Effect) |>
#  count(Storm_Effect, SH, name = "num_days")
  count(Storm_Effect, name = "num_days")
```

Step 3: Total deaths per storm effect
```{r}
total_deaths <- daily_deaths |>
#  group_by(Storm_Effect, SH) |>
  group_by(Storm_Effect) |>
  summarise(total_deaths = sum(deaths), .groups = "drop")
```

Step 4: Calculate daily death rate
```{r}
# death_rates <- left_join(total_deaths, num_days, by = c("Storm_Effect", "SH")) |>
death_rates <- left_join(total_deaths, num_days, by = "Storm_Effect") |>
  mutate(daily_death_rate = total_deaths / num_days)

print(death_rates)
```

This tells us that in this sample the daily death rate is somewhat higher on storm-effect days than on non-storm-effect days

Test it statistically using a Poisson regression model with the reference level being a non-storm-effect day
```{r}
model_data <- left_join(total_deaths, num_days, by = "Storm_Effect")
model_data$Storm_Effect <- relevel(factor(model_data$Storm_Effect), ref = "None")

poisson_model <- glm(total_deaths ~ Storm_Effect + offset(log(num_days)), data = model_data, family = poisson())
summary(poisson_model)
```

This says that given the sample of data and a Poisson model, although the effect size is small mortality is significantly higher on storm-effect days compared to the reference level

Create a nice table of the model results. We exponentiate the model coefficients to express them as a rate ratio and add confidence intervals
```{r}
# Extract coefficients and 95% confidence intervals
coefs <- coef(summary(poisson_model))
confint_vals <- confint(poisson_model)  # May take a few seconds

# Build data frame of results
rate_ratios <- data.frame(
  Term = rownames(coefs),
  Estimate = coefs[, "Estimate"],
  Std_Error = coefs[, "Std. Error"],
  z_value = coefs[, "z value"],
  p_value = coefs[, "Pr(>|z|)"],
  RR = exp(coefs[, "Estimate"]),
  RR_lower = exp(confint_vals[, 1]),
  RR_upper = exp(confint_vals[, 2])
)

# Clean up names
rownames(rate_ratios) <- NULL
rate_ratios <- rate_ratios |>
  dplyr::select(Term, RR, RR_lower, RR_upper, p_value)

# Print the table
options(digits = 4)
print(rate_ratios)
```

## Spatialize results using lat/lon grids

1. Create 0.05° grid over Florida
```{r}
library(sf)
library(dplyr)
library(purrr)

# Florida boundary (sf polygon)
library(tigris)
florida_sf <- tigris::states(cb = TRUE, year = 2022) |>
    filter(STUSPS == "FL") |>
    st_transform(crs = 4326)

# Set up 0.5° grid
grid <- st_make_grid(florida_sf, cellsize = 0.05, square = TRUE) %>%
  st_sf(grid_id = 1:length(.), geometry = .) %>%
  st_intersection(florida_sf)  # Clip to Florida boundary
```

2. Join deaths to grid
```{r}
deaths_sf <- st_read(dsn = here::here("data", "outputs", "Deaths"),
                     layer = "All_Deaths_Storm_Effects")
colnames(deaths_sf)[1:5] <- colnames(Deaths.df)[1:5]

deaths_with_grid <- st_join(deaths_sf, grid)
```

3. Aggregate and fit Poisson models per grid cell
```{r}
# Re-level Storm_Effect so "None" is reference
deaths_with_grid$Storm_Effect <- relevel(factor(deaths_with_grid$Storm_Effect), ref = "None")

# Group by grid cell
models_by_cell <- deaths_with_grid %>%
  st_drop_geometry() %>%
  group_split(grid_id) %>%
  map_df(function(df) {
    if (n_distinct(df$Storm_Effect) < 2 || nrow(df) < 30) return(NULL)  # Skip sparse cells

    daily_counts <- df %>%
      group_by(Death_Date, Storm_Effect) %>%
      summarise(deaths = n(), .groups = "drop") %>%
      group_by(Storm_Effect) %>%
      summarise(total_deaths = sum(deaths),
                num_days = n_distinct(Death_Date),
                .groups = "drop")

    # Avoid division by zero
    if (any(daily_counts$num_days == 0)) return(NULL)

    model <- try(glm(total_deaths ~ Storm_Effect + offset(log(num_days)), data = daily_counts, family = poisson()), silent = TRUE)
    if (inherits(model, "try-error")) return(NULL)

    coefs <- coef(model)
    rr <- exp(coefs)
    rr_names <- names(rr)
    
    # Return all relevant RR values and grid ID
    tibble(
      grid_id = unique(df$grid_id)[1],
      RR_Cleanup = rr[grepl("Cleanup", rr_names)],
      RR_Impact = rr[grepl("Impact", rr_names)],
      RR_Threat = rr[grepl("Threat", rr_names)]
    )
  })

```

4. Join results back to grid for mapping
```{r}
grid_results <- left_join(grid, models_by_cell, by = "grid_id")
```

5. Map each rate ratio
```{r}
library(ggplot2)

ggplot(grid_results) +
  geom_sf(aes(fill = RR_Impact), linewidth = 0) +
  scale_fill_gradient2(
    low = "blue", mid = "white", high = "red", 
    midpoint = 1, 
    na.value = "gray80", 
    name = "RR: Impact"
  ) +
  theme_minimal() +
  ggtitle("Rate Ratio (Impact vs None)")
```

## Spatialize results using zip code areas ZCTAs

1. Get Florida ZCTAs
```{r}
library(tigris)
library(sf)
options(tigris_use_cache = TRUE)

# Get national ZCTAs and subset to those intersecting Florida
zctas_all <- zctas(cb = TRUE, starts_with = NULL, year = 2020)
florida <- states(cb = TRUE) %>% filter(STUSPS == "FL")

# Filter ZCTAs that intersect Florida
zctas_fl <- st_intersection(zctas_all, st_transform(florida, st_crs(zctas_all)))
zctas_fl <- st_make_valid(zctas_fl) # in case of invalid geometries
```

2. Join deaths to ZCTAs
```{r}
deaths_sf <- st_transform(deaths_sf, st_crs(zctas_fl))  # Ensure CRS matches
deaths_with_zcta <- st_join(deaths_sf, zctas_fl["GEOID20"])  # Join only GEOID20
```

3. Fit Poisson model at each ZCTA
```{r}
library(dplyr)
library(purrr)
library(rlang)

deaths_with_zcta$Storm_Effect <- relevel(factor(deaths_with_zcta$Storm_Effect), ref = "None")

models_by_zcta <- deaths_with_zcta %>%
  st_drop_geometry() %>%
  group_split(GEOID20) %>%
  map_df(function(df) {
    if (nrow(df) < 30 || n_distinct(df$Storm_Effect) < 2) return(NULL)

    daily_counts <- df %>%
      group_by(Death_Date, Storm_Effect) %>%
      summarise(deaths = n(), .groups = "drop") %>%
      group_by(Storm_Effect) %>%
      summarise(
        total_deaths = sum(deaths),
        num_days = n_distinct(Death_Date),
        .groups = "drop"
      )

    if (any(daily_counts$num_days == 0)) return(NULL)

    model <- try(glm(total_deaths ~ Storm_Effect + offset(log(num_days)),
                     data = daily_counts, family = poisson()), silent = TRUE)
    if (inherits(model, "try-error")) return(NULL)

    rr <- exp(coef(model))
    tibble(
      GEOID20 = df$GEOID20[1],
      RR_Cleanup = rr["Storm_EffectCleanup"] %||% NA,
      RR_Impact  = rr["Storm_EffectImpact"] %||% NA,
      RR_Threat  = rr["Storm_EffectThreat"] %||% NA
    )
  })

```

4. Join RRs to ZCTA geometry
```{r}
zcta_results <- left_join(zctas_fl, models_by_zcta, by = "GEOID20")
```

Plot all 3 RR maps in a loop
```{r}
library(ggplot2)
library(purrr)
library(rlang)

rr_vars <- c("RR_Cleanup", "RR_Impact", "RR_Threat")
titles <- c("RR: Cleanup vs None", "RR: Impact vs None", "RR: Threat vs None")

walk2(rr_vars, titles, function(rr_var, plot_title) {
  p <- ggplot(zcta_results) +
    geom_sf(aes(fill = !!sym(rr_var)), color = "gray70", size = 0.1) +
    scale_fill_gradient2(
      low = "blue", mid = "white", high = "red",
      midpoint = 1, na.value = "gray90", name = rr_var
    ) +
    theme_minimal() +
    labs(title = plot_title)
  
  print(p)
  ggsave(paste0(rr_var, "_ZCTA_map.png"), plot = p, width = 8, height = 6)
})
```

## Hot spot analysis

Using Moran's I and k-nearest neighbors for spatial contiguity
```{r}
rr_sf <- zcta_results %>%
  filter(!is.na(RR_Threat)) %>%
  select(GEOID20, RR = RR_Threat, geometry)

rr_sf <- rr_sf %>%
  filter(st_geometry_type(geometry) %in% c("POLYGON", "MULTIPOLYGON")) %>%
  st_cast("MULTIPOLYGON") # Ensure polygon geometry
```

Create spatial weights
```{r}
centroids <- st_centroid(rr_sf)
coords <- st_coordinates(centroids)
# Choose k (e.g., 5 or 8 neighbors)
k <- 7
knn <- spdep::knearneigh(coords, k = k)
nb <- spdep::knn2nb(knn)

# Convert to spatial weights
lw <- spdep::nb2listw(nb, style = "W")

```

Run local hotspot
```{r}
# Calculate Local Moran’s I
lisa <- spdep::localmoran(rr_sf$RR, lw)

# Add results to the sf object
rr_sf <- rr_sf %>%
  mutate(
    local_i = lisa[, 1],         # Local Moran's I
    p_value = lisa[, 5],         # p-value
    hotspot_type = case_when(
      p_value < 0.15 & RR > mean(RR, na.rm = TRUE) ~ "Hot Spot",
      p_value < 0.15 & RR < mean(RR, na.rm = TRUE) ~ "Cold Spot",
      TRUE ~ "Not Significant"
    )
  )

```

Map the results
```{r}
library(ggplot2)

ggplot(rr_sf) +
  geom_sf(aes(fill = hotspot_type), color = "gray90", size = 0.1) +
  scale_fill_manual(values = c(
    "Hot Spot" = "red",
    "Cold Spot" = "blue",
    "Not Significant" = "gray90"
  )) +
  labs(title = "Local Hot Spot Analysis (Threat)", fill = "Cluster Type") +
  theme_minimal()

```
```{r}
# --- Packages ---
library(sf)
library(dplyr)
library(spdep)
library(ggplot2)
library(rlang)
library(purrr)

# --- Parameters you can tweak ---
rr_vars <- c(
  Threat = "RR_Threat",
  Impact = "RR_Impact",
  Cleanup = "RR_Cleanup"
)
k      <- 7         # k-nearest neighbors (your code used 7)
alpha  <- 0.15      # significance threshold to match example
outdir <- "figs"    # output folder for PNGs

if (!dir.exists(outdir)) dir.create(outdir, recursive = TRUE)

# --- Helper: run LISA for one RR variable and save the map ---
analyze_one <- function(rr_var, label) {
  # Prepare per-variable sf (keep only needed rows/geometry)
  dat <- zcta_results %>%
    select(GEOID20, RR = all_of(rr_var), geometry) %>%
    filter(!is.na(RR)) %>%
    filter(st_geometry_type(geometry) %in% c("POLYGON", "MULTIPOLYGON")) %>%
    st_cast("MULTIPOLYGON")

  # Build k-NN weights on centroids (for the subset being analyzed)
  centroids <- st_centroid(dat)
  coords <- st_coordinates(centroids)
  knn <- spdep::knearneigh(coords, k = k)
  nb  <- spdep::knn2nb(knn)
  lw  <- spdep::nb2listw(nb, style = "W", zero.policy = TRUE)

  # Local Moran's I
  lisa <- spdep::localmoran(dat$RR, lw, zero.policy = TRUE)

  # Classify hot/cold spots (match your rule: p < alpha and above/below mean)
  dat <- dat %>%
    mutate(
      local_i = lisa[, 1],
      p_value = lisa[, 5],
      hotspot_type = case_when(
        p_value < alpha & RR > mean(RR, na.rm = TRUE) ~ "Hot Spot",
        p_value < alpha & RR < mean(RR, na.rm = TRUE) ~ "Cold Spot",
        TRUE ~ "Not Significant"
      ) |> factor(levels = c("Hot Spot","Cold Spot","Not Significant"))
    )

  # Plot
  p <- ggplot(dat) +
    geom_sf(aes(fill = hotspot_type), color = "gray90", size = 0.1) +
    scale_fill_manual(values = c(
      "Hot Spot" = "red",
      "Cold Spot" = "blue",
      "Not Significant" = "gray90"
    )) +
    labs(
      title = paste0("Local Hot Spot Analysis (", label, ")"),
      subtitle = paste0("Local Moran's I, k = ", k, ", α = ", alpha),
      fill = "Cluster Type"
    ) +
    theme_minimal()

  # Save PNG
  outfile <- file.path(outdir, paste0("hotspot_", tolower(label), ".png"))
  ggsave(outfile, plot = p, width = 8, height = 6, dpi = 300)

  message("Saved: ", outfile)
  invisible(p)
}

# --- Run for all three RR variables ---
iwalk(rr_vars, analyze_one)
```


## Shiny app

Compute RRs and standard errors, then save
```{r}
library(dplyr)
library(purrr)
library(rlang)
library(sf)
library(spdep)

# --- your objects assumed in memory: deaths_sf, zctas_fl ---
deaths_sf <- st_transform(deaths_sf, st_crs(zctas_fl))
deaths_with_zcta <- st_join(deaths_sf, zctas_fl["GEOID20"])

deaths_with_zcta$Storm_Effect <- relevel(factor(deaths_with_zcta$Storm_Effect), ref = "None")

models_by_zcta <- deaths_with_zcta %>%
  st_drop_geometry() %>%
  group_split(GEOID20) %>%
  map_df(function(df) {
    if (nrow(df) < 30 || n_distinct(df$Storm_Effect) < 2) return(NULL)

    daily_counts <- df %>%
      group_by(Death_Date, Storm_Effect) %>%
      summarise(deaths = n(), .groups = "drop") %>%
      group_by(Storm_Effect) %>%
      summarise(
        total_deaths = sum(deaths),
        num_days = n_distinct(Death_Date),
        .groups = "drop"
      )

    if (any(daily_counts$num_days == 0)) return(NULL)

    model <- try(glm(total_deaths ~ Storm_Effect + offset(log(num_days)),
                     data = daily_counts, family = poisson()), silent = TRUE)
    if (inherits(model, "try-error")) return(NULL)

    sm <- summary(model)$coefficients
    # Helper to extract a term safely
    get_row <- function(term) if (term %in% rownames(sm)) sm[term, ] else c(NA, NA, NA, NA)

    cleanup <- get_row("Storm_EffectCleanup")
    impact  <- get_row("Storm_EffectImpact")
    threat  <- get_row("Storm_EffectThreat")

    # beta -> RR; se_beta -> SE on log scale; SE(RR) via delta method
    rr_cleanup <- if (!is.na(cleanup[1])) exp(cleanup[1]) else NA_real_
    rr_impact  <- if (!is.na(impact[1]))  exp(impact[1])  else NA_real_
    rr_threat  <- if (!is.na(threat[1]))  exp(threat[1])  else NA_real_

    se_cleanup_log <- if (!is.na(cleanup[2])) cleanup[2] else NA_real_
    se_impact_log  <- if (!is.na(impact[2]))  impact[2]  else NA_real_
    se_threat_log  <- if (!is.na(threat[2]))  threat[2]  else NA_real_

    se_cleanup <- if (!is.na(rr_cleanup) && !is.na(se_cleanup_log)) rr_cleanup * se_cleanup_log else NA_real_
    se_impact  <- if (!is.na(rr_impact)  && !is.na(se_impact_log))  rr_impact  * se_impact_log  else NA_real_
    se_threat  <- if (!is.na(rr_threat)  && !is.na(se_threat_log))  rr_threat  * se_threat_log  else NA_real_

    tibble(
      GEOID20     = df$GEOID20[1],
      RR_Cleanup  = rr_cleanup,
      RR_Impact   = rr_impact,
      RR_Threat   = rr_threat,
      SE_Cleanup  = se_cleanup,   # SE on RR scale
      SE_Impact   = se_impact,
      SE_Threat   = se_threat,
      SElog_Cleanup = se_cleanup_log, # keep log-scale SEs for CIs
      SElog_Impact  = se_impact_log,
      SElog_Threat  = se_threat_log
    )
  })

zcta_results <- left_join(zctas_fl, models_by_zcta, by = "GEOID20")

# Save for the Shiny app to read
saveRDS(zcta_results, "storm-mortality/zcta_results.rds")
```

## Simplify map
```{r}
library(sf)
library(rmapshaper)

z <- readRDS("storm-mortality/zcta_results.rds")

# 1) Make valid; coerce odd surface/curve types if present
z_valid <- st_make_valid(z)

# 2) Keep only polygonal features, then cast to MULTIPOLYGON and drop empties
z_poly <- z_valid %>%
  mutate(geom_type = st_geometry_type(geometry)) %>%
  filter(geom_type %in% c("POLYGON", "MULTIPOLYGON")) %>%
  st_cast("MULTIPOLYGON", warn = FALSE) %>%
  filter(!st_is_empty(geometry)) %>%
  select(-geom_type)

# 3) (Leaflet-friendly) reproject to WGS84 and simplify
z_poly <- st_transform(z_poly, 4326)
z_slim <- rmapshaper::ms_simplify(z_poly, keep = 0.15, keep_shapes = TRUE)

saveRDS(z_slim, "storm-mortality2/zcta_results_simplified.rds")
```


