---
title: "All-Cause Mortality & Storms"
output: html_document
editor_options:
  chunk_output_type: console
---

## Analytics of deaths and storms. Start with non-spatial analytics

Read the data table to a csv file
```{r}
Deaths.dt <- data.table::fread(here::here("data", "outputs", "All_Deaths_Storm_Effects.csv"))
Deaths.df <- Deaths.dt |>
  as.data.frame()
```

```{r}
library(dplyr)

total_deaths <- nrow(Deaths.df)
exposed_total <- Deaths.dt[Storm_Effect %in% c("Cleanup", "Impact", "Threat"), .N]

# Calculate death counts and rates for specified storm effects
death_rates <- Deaths.df |>
  filter(Storm_Effect %in% c("Cleanup", "Impact", "Threat")) |>
  group_by(Storm_Effect) |>
  summarise(Deaths = n(), .groups = "drop") |>
  mutate(Death_Rate = Deaths / exposed_total)

# View result
print(death_rates)
```

This tells you, for example, that .304% of all deaths occurred under an Impact storm condition.

## Death rates per day

Step 1: Get number of deaths per day by Storm_Effect
```{r}
# Group deaths by date and storm effect
daily_deaths <- Deaths.df |>
  group_by(Death_Date, Storm_Effect) |>
  summarise(deaths = n(), .groups = "drop")
```

Step 2: Get number of days for each storm effect
This step assumes Storm_Effect applies to the whole day — i.e., each Death_Date has a unique Storm_Effect
```{r}
num_days <- daily_deaths |>
  distinct(Death_Date, Storm_Effect) |>
  count(Storm_Effect, name = "num_days")
```

Step 3: Total deaths per storm effect
```{r}
total_deaths <- daily_deaths |>
  group_by(Storm_Effect) |>
  summarise(total_deaths = sum(deaths), .groups = "drop")
```

Step 4: Calculate daily death rate
```{r}
death_rates <- left_join(total_deaths, num_days, by = "Storm_Effect") |>
  mutate(daily_death_rate = total_deaths / num_days)

print(death_rates)
```

Test it statistically
```{r}
model_data <- left_join(total_deaths, num_days, by = "Storm_Effect")
model_data$Storm_Effect <- relevel(factor(model_data$Storm_Effect), ref = "None")

poisson_model <- glm(total_deaths ~ Storm_Effect + offset(log(num_days)), data = model_data, family = poisson())
summary(poisson_model)
```

Create a nice table of the model results
```{r}
# Extract coefficients and 95% confidence intervals
coefs <- coef(summary(poisson_model))
confint_vals <- confint(poisson_model)  # May take a few seconds

# Build data frame of results
rate_ratios <- data.frame(
  Term = rownames(coefs),
  Estimate = coefs[, "Estimate"],
  Std_Error = coefs[, "Std. Error"],
  z_value = coefs[, "z value"],
  p_value = coefs[, "Pr(>|z|)"],
  RR = exp(coefs[, "Estimate"]),
  RR_lower = exp(confint_vals[, 1]),
  RR_upper = exp(confint_vals[, 2])
)

# Clean up names
rownames(rate_ratios) <- NULL
rate_ratios <- rate_ratios |>
  dplyr::select(Term, RR, RR_lower, RR_upper, p_value)

# Print the table
print(rate_ratios)

```

Spatialize these results using a grid

1. Create 0.5° grid over Florida
```{r}
library(sf)
library(dplyr)
library(purrr)

# Florida boundary (sf polygon)
library(tigris)
florida_sf <- tigris::states(cb = TRUE, year = 2022) |>
    filter(STUSPS == "FL") |>
    st_transform(crs = 4326)

# Set up 0.5° grid
grid <- st_make_grid(florida_sf, cellsize = 0.5, square = TRUE) %>%
  st_sf(grid_id = 1:length(.), geometry = .) %>%
  st_intersection(florida_sf)  # Clip to Florida boundary
```

2. Join deaths to grid
```{r}
deaths_sf <- st_read(dsn = here::here("data", "outputs", "Deaths"),
                     layer = "All_Deaths_Storm_Effects")
colnames(deaths_sf)[1:5] <- colnames(Deaths.df)[1:5]

deaths_with_grid <- st_join(deaths_sf, grid)
```

3. Aggregate and fit Poisson models per grid cell
```{r}
# Relevel Storm_Effect so "None" is reference
deaths_with_grid$Storm_Effect <- relevel(factor(deaths_with_grid$Storm_Effect), ref = "None")

# Group by grid cell
models_by_cell <- deaths_with_grid %>%
  st_drop_geometry() %>%
  group_split(grid_id) %>%
  map_df(function(df) {
    if (n_distinct(df$Storm_Effect) < 2 || nrow(df) < 30) return(NULL)  # Skip sparse cells

    daily_counts <- df %>%
      group_by(Death_Date, Storm_Effect) %>%
      summarise(deaths = n(), .groups = "drop") %>%
      group_by(Storm_Effect) %>%
      summarise(total_deaths = sum(deaths),
                num_days = n_distinct(Death_Date),
                .groups = "drop")

    # Avoid division by zero
    if (any(daily_counts$num_days == 0)) return(NULL)

    model <- try(glm(total_deaths ~ Storm_Effect + offset(log(num_days)), data = daily_counts, family = poisson()), silent = TRUE)
    if (inherits(model, "try-error")) return(NULL)

    coefs <- coef(model)
    rr <- exp(coefs)
    rr_names <- names(rr)
    
    # Return all relevant RR values and grid ID
    tibble(
      grid_id = unique(df$grid_id)[1],
      RR_Cleanup = rr[grepl("Cleanup", rr_names)],
      RR_Impact = rr[grepl("Impact", rr_names)],
      RR_Threat = rr[grepl("Threat", rr_names)]
    )
  })

```

4. Join results back to grid for mapping
```{r}
grid_results <- left_join(grid, models_by_cell, by = "grid_id")
```

5. Map each rate ratio
```{r}
library(ggplot2)

ggplot(grid_results) +
  geom_sf(aes(fill = RR_Threat)) +
  scale_fill_gradient2(
    low = "blue", mid = "white", high = "red", 
    midpoint = 1, 
    na.value = "gray80", 
    name = "RR: Threat"
  ) +
  theme_minimal() +
  ggtitle("Rate Ratio (Threat vs None)")
```

Repeat using counties instead of grids

1. Load Florida counties
```{r}
library(tigris)
library(sf)
options(tigris_use_cache = TRUE)

fl_counties <- counties(state = "FL", cb = TRUE, year = 2022)
```

2. Spatial join deaths to counties
```{r}
deaths_sf <- st_transform(deaths_sf, st_crs(fl_counties))  # Ensure same CRS
deaths_with_county <- st_join(deaths_sf, fl_counties["GEOID"])  # Join only GEOID
```

3. Fit Poisson models per county
```{r}
library(dplyr)
library(purrr)

# Set "None" as reference level
deaths_with_county$Storm_Effect <- relevel(factor(deaths_with_county$Storm_Effect), ref = "None")

# Group and model
models_by_county <- deaths_with_county %>%
  st_drop_geometry() %>%
  group_split(GEOID) %>%
  map_df(function(df) {
    if (nrow(df) < 30 || n_distinct(df$Storm_Effect) < 2) return(NULL)

    daily_counts <- df %>%
      group_by(Death_Date, Storm_Effect) %>%
      summarise(deaths = n(), .groups = "drop") %>%
      group_by(Storm_Effect) %>%
      summarise(total_deaths = sum(deaths),
                num_days = n_distinct(Death_Date),
                .groups = "drop")

    if (any(daily_counts$num_days == 0)) return(NULL)

    # Fit Poisson model
    model <- try(glm(total_deaths ~ Storm_Effect + offset(log(num_days)), family = poisson(), data = daily_counts), silent = TRUE)
    if (inherits(model, "try-error")) return(NULL)

    rr <- exp(coef(model))
    tibble(
      GEOID = df$GEOID[1],
      RR_Cleanup = rr["Storm_EffectCleanup"] %||% NA,
      RR_Impact  = rr["Storm_EffectImpact"] %||% NA,
      RR_Threat  = rr["Storm_EffectThreat"] %||% NA
    )
  })

```

4. Merge RRs back to county map
```{r}
library(sf)
library(ggplot2)

county_results <- left_join(fl_counties, models_by_county, by = "GEOID")

```

5. Plot each RR (diverging around 1)
```{r}
ggplot(county_results) +
  geom_sf(aes(fill = RR_Cleanup)) +
  scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 1,
                       na.value = "gray90", name = "RR: Impact") +
  theme_minimal() +
  ggtitle("Rate Ratio: Cleanup vs None (by County)")

```

Repeat for ZCTAs

1. Get Florida ZCTAs
```{r}
library(tigris)
library(sf)
options(tigris_use_cache = TRUE)

# Get national ZCTAs and subset to those intersecting Florida
zctas_all <- zctas(cb = TRUE, starts_with = NULL, year = 2020)
florida <- states(cb = TRUE) %>% filter(STUSPS == "FL")

# Filter ZCTAs that intersect Florida
zctas_fl <- st_intersection(zctas_all, st_transform(florida, st_crs(zctas_all)))

```

2. Join deaths to ZCTAs
```{r}
deaths_sf <- st_transform(deaths_sf, st_crs(zctas_fl))  # Ensure CRS matches
deaths_with_zcta <- st_join(deaths_sf, zctas_fl["GEOID20"])  # Join only GEOID20
```

3. Fit Poisson model per ZCTA
```{r}
library(dplyr)
library(purrr)
library(rlang)

deaths_with_zcta$Storm_Effect <- relevel(factor(deaths_with_zcta$Storm_Effect), ref = "None")

models_by_zcta <- deaths_with_zcta %>%
  st_drop_geometry() %>%
  group_split(GEOID20) %>%
  map_df(function(df) {
    if (nrow(df) < 30 || n_distinct(df$Storm_Effect) < 2) return(NULL)

    daily_counts <- df %>%
      group_by(Death_Date, Storm_Effect) %>%
      summarise(deaths = n(), .groups = "drop") %>%
      group_by(Storm_Effect) %>%
      summarise(
        total_deaths = sum(deaths),
        num_days = n_distinct(Death_Date),
        .groups = "drop"
      )

    if (any(daily_counts$num_days == 0)) return(NULL)

    model <- try(glm(total_deaths ~ Storm_Effect + offset(log(num_days)),
                     data = daily_counts, family = poisson()), silent = TRUE)
    if (inherits(model, "try-error")) return(NULL)

    rr <- exp(coef(model))
    tibble(
      GEOID20 = df$GEOID20[1],
      RR_Cleanup = rr["Storm_EffectCleanup"] %||% NA,
      RR_Impact  = rr["Storm_EffectImpact"] %||% NA,
      RR_Threat  = rr["Storm_EffectThreat"] %||% NA
    )
  })

```

4. Join RRs to ZCTA geometry
```{r}
zcta_results <- left_join(zctas_fl, models_by_zcta, by = "GEOID20")
```

Plot all 3 RR maps in a loop
```{r}
library(ggplot2)
library(purrr)

rr_vars <- c("RR_Cleanup", "RR_Impact", "RR_Threat")
titles <- c("RR: Cleanup vs None", "RR: Impact vs None", "RR: Threat vs None")

walk2(rr_vars, titles, function(rr_var, plot_title) {
  p <- ggplot(zcta_results) +
    geom_sf(aes_string(fill = rr_var), color = "gray70", size = 0.1) +
    scale_fill_gradient2(
      low = "blue", mid = "white", high = "red",
      midpoint = 1, na.value = "gray90", name = rr_var
    ) +
    theme_minimal() +
    labs(title = plot_title)
  
  print(p)
  ggsave(paste0(rr_var, "_ZCTA_map.png"), plot = p, width = 8, height = 6)
})

```



