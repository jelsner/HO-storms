---
title: "Daily death counts relative to storm effects"
output: html_document
editor_options:
  chunk_output_type: console
---

## Create an empirical storm effects model

```{r}
library(sf)
library(dplyr)

storm_intensity <- 34 # all tropical storms and hurricanes
begin_year <- 1981
end_year <- 2022

L <- "https://www.ncei.noaa.gov/data/international-best-track-archive-for-climate-stewardship-ibtracs/v04r00/access/shapefile/IBTrACS.NA.list.v04r00.lines.zip"
  
if(!"IBTrACS.NA.list.v04r00.lines.zip" %in% list.files(here::here("data"))) {
download.file(url = L,
              destfile = here::here("data",
                                    "IBTrACS.NA.list.v04r00.lines.zip"))
unzip(here::here("data", "IBTrACS.NA.list.v04r00.lines.zip"),
      exdir = here::here("data"))
}

Tracks.sf <- st_read(dsn = here::here("data"), 
                     layer = "IBTrACS.NA.list.v04r00.lines") |>
  st_transform(crs = 32616)

Tracks.sf <- Tracks.sf |>
  filter(year >= begin_year & year <= end_year) |>
  filter(USA_WIND >= storm_intensity) |>
  select(SID, SEASON, year, month, day, hour, min,
         NAME, SUBBASIN, ISO_TIME, USA_WIND, USA_PRES, USA_RMW, USA_EYE, USA_ROCI)

Tracks.sf |>
  st_drop_geometry() |>
  summarize(avgRMW = mean(USA_RMW, na.rm = TRUE) * 1.852,
                   avgEYE = mean(USA_EYE, na.rm = TRUE) * 1.852,
                   avgROCI = mean(USA_ROCI, na.rm = TRUE) * 1.852)

# Fill in missing RMW values. Start with pedigree, then use minimum pressure, and finish again with pedigree
Tracks.sf <- Tracks.sf |>
  group_by(SID) |>  # pedigree
  mutate(USA_RMW = ifelse(is.na(USA_RMW), mean(USA_RMW, na.rm = TRUE), USA_RMW))

Tracks.sf <- Tracks.sf |>
  group_by(USA_PRES) |> # minimum pressure
  mutate(USA_RMW = ifelse(is.na(USA_RMW), mean(USA_RMW, na.rm = TRUE), USA_RMW))

Tracks.sf <- Tracks.sf |>
  group_by(SID) |> # pedigree
  mutate(USA_RMW = ifelse(is.na(USA_RMW), mean(USA_RMW, na.rm = TRUE), USA_RMW))

#Add a buffer to the tracks to make segmented wind swaths
Swaths.sf <- Tracks.sf |>
  st_buffer(dist = Tracks.sf$USA_RMW * 1852) # 1852 converts to meters

# Wind swaths that cross Florida. `USAboundaries` package no longer maintained on CRAN
#devtools::install_github("ropensci/USAboundariesData")
#devtools::install_github("ropensci/USAboundaries", force = TRUE)
#install.packages("USAboundariesData", repos = "https://ropensci.r-universe.dev", type = "source")
Boundaries.sf <- USAboundaries::us_states(resolution = "low", states = "FL") |> 
  st_transform(crs = 32616)

X <- Swaths.sf |>
  st_intersects(Boundaries.sf, sparse = FALSE) #Does the swath intersect the state border?
Swaths.sf <- Swaths.sf[X, ]
Swaths.sf <- Swaths.sf |>
  mutate(Date = lubridate::as_date(ISO_TIME)) #Add a date column

# Extract the boundaries of storm impacts by storm ID. Dissolve the overlap borders of the individual swaths by storm to create a single storm swath. Add the storm category (Saffir-Simpson) based on wind speed
Swaths.sf <- Swaths.sf |>
  group_by(SID) |>
  summarize(Date0 = first(Date),
            NAME = first(NAME),
            Wind = max(USA_WIND),
            geometry = st_union(geometry)) |>
  mutate(Storm_Category = case_when(
    Wind >= 34 & Wind <= 63 ~ 0,
    Wind >= 64 & Wind <= 82 ~ 1,
    Wind >= 83 & Wind <= 95 ~ 2,
    Wind >= 96 & Wind <= 112 ~ 3,
    Wind >= 113 & Wind <= 136 ~ 4,
    Wind >= 137 ~ 5
  ))

# Transform the geometry to a geographic CRS (4326) and unionize the swaths
sf_use_s2(TRUE)

Swaths.sf <- Swaths.sf |>
    st_transform(crs = 4326)
union_Swaths2.sfg <- Swaths.sf |>
  st_union()
```

Expand `Swaths.sf` by adding rows based on the increment value of the attribute `Date0`. Dates prior to the impact date are threat dates and those after the impact date are cleanup dates
```{r}
library(tidyr)
library(lubridate)

deltaT <- 1   # One day increment
n_new <- 1    # Number of days before & after impact

TIC.sf <- Swaths.sf |>
    rowwise() |>
    mutate(new_features = list(tidyr::tibble(
           Date = Date0 + (-n_new:n_new) * deltaT))) |>
    unnest(new_features) |>
    ungroup() |>
    select(Date, Storm_Name = NAME, Storm_Category)

TIC <- rep(rep(c("Threat", "Impact", "Cleanup"), times = c(n_new, 1, n_new)), 
           times = nrow(Swaths.sf))
TIC.sf$Storm_Effect <- TIC

# Add month and year indicators change name of the geometry column (this is needed for spatial merges)
TIC.sf <- TIC.sf |>
  mutate(Month = lubridate::month(Date),
         Year = lubridate::year(Date))
storm_months <- unique(TIC.sf$Month)
storm_year_range <- range(TIC.sf$Year)

st_geometry(TIC.sf) <- "geom"
```

## Build a zip x date effect calendar

```{r}
library(data.table)
library(tigris)
options(tigris_use_cache = TRUE)

# Florida zips
zctas_fl <- zctas(cb = FALSE, year = 2010, state = "FL") %>%
  select(zip = ZCTA5CE10, geometry)

#Align coordinate reference systems
tic_aligned  <- st_transform(TIC.sf,  st_crs(zctas_fl))

# Spatial join: which zips intersect each TIC polygon (by date & effect)
# Keep only the columns we need from TIC
tic_keep <- tic_aligned |> 
  select(Date, Storm_Effect, geom)

# Point-in-polygon for polygons
zcta_date_effect_sf <- st_join(
  zctas_fl,
  tic_keep,
  join = st_intersects,
  left = FALSE   # keep only zips that intersect a TIC polygon on that date
)

# Drop geometry for the calendar
zcta_date_effect <- as.data.table(st_drop_geometry(zcta_date_effect_sf))
setnames(zcta_date_effect, c("zip","Date","Storm_Effect"),
         c("zip","date","effect"))

# Normalize types
zcta_date_effect[, `:=`(
  zip    = as.character(zip),
  date   = as.IDate(date),
  effect = as.character(effect)
)]

# Resolve multiple effects on the same zip-date: Choose most severe ranking (None < Cleanup < Threat < Impact)
sev <- data.table(effect = c("None","Cleanup","Threat","Impact"),
                  rank   = 0:3)
zde <- sev[zcta_date_effect, on = "effect"]

# Keep the single most-severe label per ZIP–date
zde <- zde[ , .SD[which.max(rank)], by = .(zip, date)][ , .(zip, date, effect)]

# Build the full zip x date data frame & fill missing days with "None"
all_zips  <- sort(unique(zde$zip))
all_dates <- as.IDate(seq(as.Date("2004-12-31"), as.Date("2022-12-31"), by = "day"))
grid <- CJ(zip = as.character(all_zips), date = all_dates)

# Left-join the observed effects; fill NA → "None"
effect_calendar <- zde[grid, on = .(zip, date)]
effect_calendar[is.na(effect), effect := "None"]

# Keep effect as a factor with ordered levels
effect_calendar[, effect := factor(effect, levels = c("None","Cleanup","Threat","Impact"))]

# Exactly one effect per ZIP–day?
stopifnot(effect_calendar[, .N, by = .(zip, date)][, all(N == 1)])

# Effects present?
effect_calendar[, table(effect)]
```

## Merge storms with death records

Files received from Jihoon on November 27, 2025. We run these files sequentially, merging with storms and fitting models separately for each subset. Since attributes (age, sex, etc) are at the individual level and we are modeling daily counts we can't use these as simple covariates. Use 1985-01-01 for 55+ storms. ~ 2 minutes
```{r}
start_time <- Sys.time()
load("data/all_data.Rdata")
load("data/cardio_subset_all.Rdata")
cardio_code <- unique(cardio_subset_all$ICD_CODE)

Deaths.df <- all_data %>%
  mutate(Date = as_date(DATE_OF_DEATH)) %>%
  filter(Date >= as.Date("1981-01-01")) %>%
  select(Death_ID = ID, Death_Date = Date, final_lon, final_lat, 
         age = COMPUTED_AGE, white = RACE_WHITE, sex = SEX, 
         marital_code = MARITAL_CODE, cardio_code = ICD_CODE, army = ARMY_YESNO)

Deaths.sf <- Deaths.df %>%
  st_as_sf(coords = c("final_lon", "final_lat"), crs = 4326)
rm(all_data, Deaths.df)
Sys.time() - start_time
```

Note: IDs are not unique

Filter deaths by date of death and storm effect zones. It took 2.3 hours on my Apple M4 using all deaths (1981-2022), >= 34 kt storms, and +/- 1 day on either side of impact day 6.7 hr +/- 7 days
```{r}
start_time <- Sys.time()
# work in a projected CRS for sane distances
target_crs <- 5070  # NAD83 / Conus Albers, or anything projected

TIC_proj    <- st_transform(TIC.sf, target_crs)
Deaths_proj      <- st_transform(Deaths.sf, target_crs)
sf_use_s2(FALSE)

num_obs <- nrow(Deaths_proj)
chunk_size <- 50000
num_chunks <- ceiling(num_obs / chunk_size)
print(paste("Number of chunks to process", num_chunks))

# Pre-allocate an empty list to store chunks
results_list <- vector("list", num_chunks)

for (i in seq(1, num_obs, by = chunk_size)) {
  chunk_index <- (i - 1) / chunk_size + 1
  chunk_indices <- i:min(i + chunk_size - 1, num_obs)
  
  print(paste("Processing chunk", chunk_index))

  chunk.sf <- Deaths_proj[chunk_indices, ]

deaths_storm_filtered <- chunk.sf |>
  rowwise() |>
  mutate(
    matched_storms = list(
      TIC_proj |>
        filter(Date == Death_Date))) |>
  unnest(matched_storms, keep_empty = TRUE)

# Perform spatial join to check if death location falls within the storm effect zones
deaths_with_impacts <- st_join(deaths_storm_filtered, TIC_proj, join = st_within)

# Keep only records where a death was inside a storm effect zone
deaths_with_impacts <- deaths_with_impacts |>
  filter(!is.na(Storm_Name.x)) |>
  select(Death_ID, Death_Date,
            Storm_Name = Storm_Name.x, 
            Storm_Category = Storm_Category.x,
            Storm_Effect = Storm_Effect.x, 
            age, white, sex, marital_code,
            cardio_code, army, geometry) |>
  distinct()

results_list[[chunk_index]] <- deaths_with_impacts

gc() #ensures memory from previous iterations is released
}

deaths_with_impacts <- do.call(rbind, results_list) #row bind the chunks
Sys.time() - start_time
```

Collapse `deaths_with_impacts` to one row per death, keeping the death occurring with the highest category storm irrespective of effect. For example, TS Bonnie and Cat 4 Charlie on 2004-08-13 was Cleanup on Bonnie and Impact on Charlie
```{r}
impacts_max_cat <- deaths_with_impacts %>%
  st_drop_geometry() %>%
  group_by(Death_ID, Death_Date) %>%
  # keep only the row with the largest Storm_Category
  slice_max(Storm_Category, n = 1, with_ties = FALSE) %>%
  ungroup()
```

Combine with all deaths
```{r}
impacts_keep <- impacts_max_cat %>%
  select(Death_ID, Death_Date, Storm_Name, Storm_Category, Storm_Effect)
deaths_all_effects <- Deaths.sf %>%
  left_join(impacts_keep, by = c("Death_ID", "Death_Date")) %>%
  mutate(
    Storm_Effect = replace_na(Storm_Effect, "None")
  )
```

Export (optional)
```{r}
# To CSV
st_drop_geometry(deaths_all_effects) %>%
  readr::write_csv("data/outputs/Deaths/All_Deaths_Storm_Effects_34kt_1day.csv")

# To GeoPackage
st_write(
  deaths_all_effects,              # full sf object with long names
  dsn = "data/outputs/Deaths/All_Deaths_Storm_Effects_34kt_1day.gpkg",
  layer = "All_Deaths_Storm_Effects_34kt_1day",
  delete_dsn = TRUE
)
```

## Analytics of deaths and storms

Compute statewide death rates
```{r}
library(dplyr)
library(data.table)

Deaths.sf <- deaths_all_effects 
Deaths.df <- Deaths.sf %>%
  st_drop_geometry()
Deaths.dt <- as.data.table(Deaths.df)
```

Get statewide number of deaths per day by storm effect
```{r}
# Range of dates for which there was a storm effect
range <- Deaths.df |>
  filter(Storm_Effect != "None")
range(range$Death_Date)

# Group deaths by date and storm effect
daily_deaths <- Deaths.df |>
  group_by(Death_Date, Storm_Effect) |>
  summarise(deaths = n(), .groups = "drop")
```

Get number of days statewide for each storm effect. This assumes storm effect applies to the whole day — i.e., each death has a unique storm effect
```{r}
num_days <- daily_deaths |>
  distinct(Death_Date, Storm_Effect) |>
  count(Storm_Effect, name = "num_days")

print(num_days)
```

Total statewide deaths per storm effect
```{r}
total_deaths <- daily_deaths |>
  group_by(Storm_Effect) |>
  summarise(total_deaths = sum(deaths), .groups = "drop")

print(total_deaths)
```

Calculate statewide daily death rate
```{r}
death_rates <- left_join(total_deaths, num_days, by = "Storm_Effect") |>
  mutate(daily_death_rate = total_deaths / num_days)

print(death_rates)
```

This tells us that in this sample the statewide daily death rate is somewhat higher on storm-effect days (all-cause deaths). But a lot is going on: time and spatial trends, seasonality, autocorrelation. We need a model that accounts for these things

## Add zip codes to each death

Setup the data (as a panel)

Spatial join deaths to zips
```{r}
# Inspect CRS
st_crs(zctas_fl)   # NAD83 (EPSG:4269)
st_crs(Deaths.sf)  # WGS84 (EPSG:4326)

# work in a projected CRS for sane distances
target_crs <- 5070  # NAD83 / Conus Albers, or anything projected

zctas_fl_proj    <- st_transform(zctas_fl, target_crs)
Deaths_proj      <- st_transform(Deaths.sf, target_crs)
sf_use_s2(FALSE)

begin <- Sys.time()
# Spatial join: add zip code to each death
deaths_with_zip <- st_join(
  Deaths_proj,
  zctas_fl_proj,         
  join    = st_intersects,  # includes boundary points
  left    = TRUE,
  largest = TRUE            # ensure at most one zip per death
)
Sys.time() - begin

# 1.07 hrs all deaths 55+ storms

deaths_with_zip <- st_transform(deaths_with_zip, 4326)
zctas_fl <- st_transform(zctas_fl, 4269)

# How many deaths didn't land in any zip code area?
sum(is.na(deaths_with_zip$zip))

# Remove deaths not inside a zip code (if desired)
deaths_with_zip <- deaths_with_zip %>%
  filter(!is.na(zip))
```

Make maps. Compute death totals by zip by effect with the effect as separate columns (wide format), then join to zip polygons, then make choropleth maps
```{r}
death_counts_wide <- deaths_with_zip %>%
  sf::st_drop_geometry() %>%  
  filter(Storm_Effect %in% c("Threat","Impact","Cleanup")) %>%
  count(zip, Storm_Effect, name = "deaths") %>%
  tidyr::pivot_wider(
    names_from  = Storm_Effect,
    values_from = deaths,
    values_fill = 0
  )

# left join counts (preserves polygon geometry)
death_map_sf <- zctas_fl %>%
  left_join(death_counts_wide, by = "zip") %>%
  mutate(
    Threat = replace_na(Threat, 0L),
    Impact = replace_na(Impact, 0L),
    Cleanup = replace_na(Cleanup, 0L)
  ) %>%
  select(zip, Threat, Impact, Cleanup, geometry)

# for story map
#saveRDS(death_map_sf, "data/outputs/Results/zcta_death_counts.rds")
```

```{r}
library(ggplot2)
library(RColorBrewer)

bins <- c(0, 5, 10, 25, 50, 100, 200)
#bins <- c(0, 1, 2, 4, 8, 16, 32)
pal  <- brewer.pal(length(bins) - 1, "YlOrRd")

make_map <- function(data_sf, col, title = col) {
  ggplot(data_sf) +
    geom_sf(aes(fill = .data[[col]]), color = "white", size = 0.05) +
    scale_fill_stepsn(
      colors  = pal,
      breaks  = bins,
      limits  = range(bins),
      na.value = "#cccccc",
      name    = "Deaths per ZCTA"
    ) +
    labs(title = title) +
    coord_sf(datum = NA) +
    theme_void(base_size = 12) +
    theme(plot.title = element_text(face = "bold", hjust = 0.5))
}

p_impact <- make_map(death_map_sf, "Threat",  "Deaths on threat days")
p_impact

ggsave("figs/zcta_deaths_impact.png",  p_impact,  width = 7.2, height = 6, dpi = 300)
```

List death dates & storm names by zip and storm effect
```{r}
library(purrr)

df <- deaths_with_zip %>%
  st_drop_geometry()

# A compact tibble with a list-column of Death_Dates
dates_by_effect_df <- df %>%
  group_by(zip, Storm_Effect) %>%
  summarise(
    Death_Dates = list(sort(unique(Death_Date))),   # the actual unique dates (could be more than 1 death)
    Storm_Names = list(unique(Storm_Name)),         # storm names
    n_records   = n(),                               # total records
    n_dates     = length(Death_Dates[[1]]),          # unique dates
    .groups = "drop"
  ) %>%
  arrange(zip, Storm_Effect)

dates_by_effect_df
length(unique(dates_by_effect_df$zip))
```

## Daily counts by zip

Here is where we can subset by age, sex, race, cardio

Daily death counts by zip. I don't think it makes much sense to use daily average age or daily percent male
```{r}
daily_by_zip <- deaths_with_zip %>%
  st_drop_geometry() %>%
  transmute(
    zip    = as.character(zip),
    date   = as.Date(Death_Date),
    effect = as.character(Storm_Effect),
    age = as.integer(age),
    white = white,
    sex = sex,
    cc = cardio_code
  ) %>%
#  filter(cc %in% cardio_code) %>%
#  filter(age > 65) %>%
#  filter(sex == "F") %>%
#  filter(white == "N") %>%
  group_by(zip, date, effect) %>%
  summarise(deaths = n(),
            .groups = "drop")

# Sanity check
#daily_by_zip %>%
#  count(zip, date) %>%
#  filter(n > 1)  # rows here mean the same zip–date appears under multiple effects
```

Join deaths to the full calendar (`effect_calendar`)
```{r}
dbz <- as.data.table(daily_by_zip)[ , .(
  zip   = as.character(zip),
  date  = as.IDate(date),
  deaths = as.integer(deaths)
)]

zip_day <- dbz[effect_calendar, on = .(zip, date)]
zip_day[is.na(deaths), deaths := 0L]  # fill missing counts with 0

# After joining deaths: zeros should appear across all effects
#zip_day[, .(zeros = sum(deaths == 0L), rows = .N), by = effect][order(effect)]

# Replace NA with ZIP mean age (Recommended if avg_age is meaningful where deaths > 0)
#zip_day[, avg_age := ifelse(is.na(avg_age), mean(avg_age, na.rm = TRUE), avg_age), by = zip]
```

Export (optional)
```{r}
readr::write_csv(zip_day,
                 "data/outputs/Deaths/zip_day_34kt_1day.csv")
```

## Poisson model with stratum fixed effects

Next we fit a Poisson with stratum fixed effects. This is equivalent to conditioning on the stratum totals and largely removes serial confounding. Use robust (clustered) SEs by stratum to protect against residual correlation within a stratum

Add strata variables. Note here we include only months during the hurricane season (May-November). We can't do this with a time-series model
```{r}
dat <- zip_day %>%
  filter(lubridate::month(date) %in% 5:11) %>%  # hurricane season only
  transmute(
    zip    = as.character(zip),
    date   = as.IDate(date),
    deaths = as.integer(deaths),
    effect = factor(effect, levels = c("None","Threat","Impact","Cleanup")),
    dow     = factor(strftime(date, "%u"), levels = as.character(1:7),
                       labels = c("Mon","Tue","Wed","Thu","Fri","Sat","Sun")),
    ym     = format(date, "%Y-%m")                      # year-month stratum
  )
```

Fit model
```{r}
library(fixest)

m_pooled <- fepois(
  deaths ~ i(effect, ref = "None") | zip + ym + dow,
  data    = dat,
  cluster = ~ zip + ym,   # two-way clustered SEs (ZIP & month)
  notes = TRUE
)

summary(m_pooled)

# Rate ratios (RR) with CIs
library(broom)

rr_tbl <- tidy(m_pooled, conf.int = TRUE) %>%                # returns log-scale
  filter(grepl("^effect::", term)) %>%
  transmute(
    effect = sub("^effect::", "", term),
    RR     = exp(estimate),
    LCI    = exp(conf.low),
    UCI    = exp(conf.high),
    p      = p.value
  )
rr_tbl
```

Extract ZIP fixed effects from the model
```{r}
# Extract fixed effects
fe <- fixef(m_pooled)

# ZIP fixed effects as a data.table
zip_fe <- data.table(
  zip = names(fe$zip),
  fe_zip = as.numeric(fe$zip)
)
# convert to rate ratios
zip_fe[, rr_zip := exp(fe_zip)]

zip_map <- zctas_fl %>%
  mutate(zip = as.character(zip)) %>%
  left_join(zip_fe, by = "zip")

library(ggplot2)

ggplot(zip_map) +
  geom_sf(aes(fill = fe_zip), color = NA) +
  scale_fill_gradient2(
    low = "blue",
    mid = "white",
    high = "red",
    midpoint = 0,
    name = "ZIP fixed effect\n(log scale)"
  ) +
  labs(
    title = "ZIP-Level Fixed Effects from Poisson Mortality Model",
    subtitle = "Baseline mortality differences after adjusting for storm effects, month, and day-of-week"
  ) +
  theme_minimal()

ggplot(zip_map) +
  geom_sf(aes(fill = rr_zip), color = NA) +
  scale_fill_viridis_c(
    option = "plasma",
    name = "Baseline mortality RR",
    trans = "log"
  ) +
  labs(
    title = "ZIP-Level Baseline Mortality Risk",
    subtitle = "Rate ratios from ZIP fixed effects (Poisson model)"
  ) +
  theme_minimal()
```

Notes: These are log-scale effects, They are mean-centered (sum ≈ 0)

## zip level Threat effects

Instead of one giant interaction model, we fit one small model per ZIP. This yields a ZIP-specific Threat coefficient (log rate ratio vs None), controlling for month and day-of-week within that ZIP. It’s the “Threat slope by ZIP” we want, without exploding memory.

Prepare data
```{r}
library(data.table)
library(fixest)

setDT(dat)

# Make sure effect is character (fixest will handle factors too)
dat[, effect := as.character(effect)]
dat[, ym := as.factor(ym)]
dat[, dow := as.factor(dow)]
```

Function to fit one ZIP and extract Threat effect
```{r}
fit_one_zip <- function(dz){

  # Basic requirements
  if (nrow(dz) < 10) return(NULL)
  if (all(dz$deaths == 0L)) return(NULL)              # no outcome variation
  if (!("Threat" %in% dz$effect)) return(NULL)
  if (!("None" %in% dz$effect)) return(NULL)

  # Need some variation in effect across rows
  if (length(unique(dz$effect)) < 2) return(NULL)

  # Try fitting; if fixest can't estimate, skip ZIP
  m <- tryCatch(
    fepois(deaths ~ i(effect, ref = "None") | ym + dow, data = dz),
    error = function(e) NULL
  )
  if (is.null(m)) return(NULL)

  # Extract Threat coefficient + SE if present
  cf <- coef(m)
  if (!("effect::Threat" %in% names(cf))) return(NULL)

  vc <- tryCatch(vcov(m), error = function(e) NULL)
  if (is.null(vc)) return(NULL)

  se <- sqrt(vc["effect::Threat", "effect::Threat"])

  data.table(
    zip  = dz$zip[1],
    beta = unname(cf["effect::Threat"]),
    se   = as.numeric(se)
  )
}
```

Run across all zips
```{r}
start <- Sys.time()
fixest::setFixest_notes(FALSE)
# Split dat by zip
zip_list <- split(dat, by = "zip", keep.by = TRUE)

zip_raw <- rbindlist(lapply(zip_list, fit_one_zip), fill = TRUE)
Sys.time() - start
```

Estimate between zip variances
```{r}
beta_state <- as.numeric(coef(m_pooled)["effect::Threat"])

zip_raw[, v := se^2]
zip_raw[, u := (beta - beta_state)^2]

tau2 <- max(0, mean(zip_raw$u - zip_raw$v, na.rm = TRUE))
tau2
```

Note: tau2 = 0 means there is no detectable ZIP-level heterogeneity in the Threat effect, given the identification strategy

Instead we can map which ZIPs contribute the largest absolute increase in expected deaths on Threat days (“excess deaths”)

This uses our actual dat rows (same ym/dow mix) and predicts expected deaths twice per row: once with effect="None" and once with effect="Threat".

```{r}
library(data.table)
library(fixest)

setDT(dat)

# Make sure effect is character/factor with the right levels
dat[, effect := factor(effect, levels = c("None","Threat","Impact","Cleanup"))]

# Create two copies with different counterfactual effects
dat_none   <- copy(dat)[, effect := factor("None",   levels = levels(dat$effect))]
dat_threat <- copy(dat)[, effect := factor("Threat", levels = levels(dat$effect))]

# Predicted expected deaths (includes zip/ym/dow fixed effects automatically)
mu_none   <- predict(m_pooled, newdata = dat_none,   type = "response")
mu_threat <- predict(m_pooled, newdata = dat_threat, type = "response")

# Add predictions back (no need to store both huge copies)
dat[, `:=`(mu_none = mu_none, mu_threat = mu_threat)]
dat[, mu_excess_threat := mu_threat - mu_none]   # absolute excess on Threat vs None

# Aggregate to zip level
zip_threat_contrib <- dat[, .(
  mean_mu_none   = mean(mu_none, na.rm = TRUE),
  mean_mu_threat = mean(mu_threat, na.rm = TRUE),
  mean_excess    = mean(mu_excess_threat, na.rm = TRUE),
  sum_excess     = sum(mu_excess_threat, na.rm = TRUE)   # total excess over all days in dat
), by = zip]
```

Map
```{r}
zip_map <- zctas_fl |>
  mutate(zip = as.character(zip)) |>
  left_join(zip_threat_contrib, by = "zip")

ggplot(zip_map) +
  geom_sf(aes(fill = mean_excess), color = NA) +
  scale_fill_viridis_c(name = "Mean excess\ndeaths/day") +
  labs(
    title = "Where Threat days add the most expected deaths",
    subtitle = "Computed from pooled Poisson model: E[deaths|Threat] - E[deaths|None]"
  ) +
  theme_minimal()

ggplot(zip_map) +
  geom_sf(aes(fill = sum_excess), color = NA) +
  scale_fill_viridis_c(name = "Total excess\ndeaths") +
  labs(
    title = "Total expected Threat-related excess deaths by ZIP",
    subtitle = "Sum over all days in the analysis sample"
  ) +
  theme_minimal()

# sanity checks
beta_threat <- as.numeric(coef(m_pooled)["effect::Threat"])
exp(beta_threat)  # statewide Threat RR, same for every zip in this model
```

Map results
```{r}
zip_threat_map <- zctas_fl |>
  mutate(zip = as.character(zip)) |>
  left_join(zip_threat, by = "zip")

ggplot(zip_threat_map) +
  geom_sf(aes(fill = threat_beta), color = NA) +
  scale_fill_gradient2(midpoint = 0, low = "blue", mid = "white", high = "red",
                       name = "Threat effect\n(log RR)") +
  labs(title = "ZIP-level Threat effect on daily deaths",
       subtitle = "Within-ZIP Poisson: deaths ~ effect | year-month + day-of-week") +
  theme_minimal()

ggplot(zip_threat_map) +
  geom_sf(aes(fill = threat_RR), color = NA) +
  scale_fill_viridis_c(trans = "log", name = "Threat RR") +
  labs(title = "ZIP-level Threat rate ratio (vs None)") +
  theme_minimal()

```







OLDER STUFF
------------

## Refit the model with ZIP × Threat interactions (NOT RUN--TOO BIG)

We need to explicitly allow the Threat effect to vary by ZIP. The cleanest way is to interact effect with zip only for Threat, while keeping your fixed effects

```{r}
start <- Sys.time()
m_threat_zip <- fepois(
  deaths ~ i(effect, ref = "None") +
           i(zip, effect == "Threat") |
           zip + ym + dow,
  data    = dat,
  cluster = ~ zip + ym
)
Sys.time() - start
```

For all deaths and 34+ storms the coefficients on the fixed effects are
Standard-errors: Clustered (zip & ym) 
                Estimate Std. Error z value Pr(>|z|)    
effect::Threat  0.042035   0.018982 2.21448 0.026796 *  
effect::Impact  0.030880   0.013403 2.30398 0.021224 *  
effect::Cleanup 0.042110   0.016436 2.56205 0.010406 *  

What the coefficients mean. We fit a fixed-effects Poisson with: ZIP fixed effects (| zip) → controls for all time-invariant differences across ZIPs (population size, baseline mortality, socio-demographics, etc.)
Year-month fixed effects (| ym) → controls for statewide shocks and seasonality at the month level (flu waves, hurricanes seasons, COVID waves, etc.). Day-of-week fixed effects (| dow) → controls for weekly pattern. So the effect::… coefficients are within-ZIP log rate ratios vs “None”, after removing month and weekday patterns. Exponentiate them to get rate ratios (RR). Convert the estimates to RRs (95% CI)

Threat: β = 0.042035 (SE 0.018982), p = 0.0268
RR = 1.043 → ~4.3% higher deaths vs None, adjusted for ZIP / YM / DOW
Impact: β = 0.030880 (SE 0.013403), p = 0.0212
RR = 1.031 → ~3.1% higher deaths vs None, adjusted for ZIP / YM / DOW
Cleanup: β = 0.042110 (SE 0.016436), p = 0.0104
RR = 1.043 → ~4.3% higher
RR = exp(β); CI = exp(β ± 1.96·SE)

NOTE: 44/0/0 fixed-effects (395,472 observations) removed because of only 0 outcomes or singletons.
We have 3 FE groups (zip / ym / dow). The 44/0/0 means 44 ZIP levels were dropped (0 YM, 0 DOW) because, within those ZIPs, the data contributed no identifying variation (e.g., all days had zero deaths or only singleton cells after stratification). The corresponding 395,472 rows were removed from estimation. That’s expected behavior: those ZIPs can’t help identify contrasts after the FE are applied.

Log-likelihood and BIC are for the Poisson FE fit. “Adj. Pseudo R²: 0.200” and “Squared Cor.: 0.240” indicate reasonable explanatory power given this is daily death counts with strong FE.

Compared with “None” days, statewide Threat days are associated with an ~4.3% increase in daily deaths (95% CI (.5%–8.2%), Cleanup days with ~3.1%, and Impact days with ~4.3%

Note: for all deaths and 55+ storms the coefficients on the fixed effects are larger
Standard-errors: Clustered (zip & ym) 
                Estimate Std. Error z value Pr(>|z|)    
effect::Threat  0.076102   0.030525 2.49316 0.012661 *  
effect::Impact  0.064373   0.032892 1.95707 0.050339 .  
effect::Cleanup 0.077044   0.030092 2.56027 0.010459 *  

Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Log-Likelihood: -5,684,297.6   Adj. Pseudo R2: 0.186984
           BIC: 11,388,488.4     Squared Cor.: 0.236403

.076/.042 = 1.7x larger
.064/.031 = 2.1x larger
.077/.042 = 1.8x larger
When excluding weaker tropical storms 34-55 kt, the effect sizes are about twice as large

Note: for cardio deaths and 34+ storms the coefficients on the fixed effects are
Standard-errors: Clustered (zip & ym) 
                Estimate Std. Error  z value Pr(>|z|)    
effect::Threat  0.004268   0.053075 0.080409  0.93591    
effect::Impact  0.098254   0.061101 1.608043  0.10783    
effect::Cleanup 0.109236   0.048695 2.243261  0.02488 *  

Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Log-Likelihood: -1,015,338.1   Adj. Pseudo R2: 0.122662
           BIC:  2,050,675.5     Squared Cor.: 0.045869

Try negative binomial fixed effects as a sensitivity
```{r}
m_pooled2 <- fenegbin(
  deaths ~ i(effect, ref = "None") | zip + ym + dow,
  data    = dat,
  cluster = ~ zip + ym   # two-way clustered SEs (ZIP & month)
)

summary(m_pooled2)
```

Results are essentially the same as those from the Poisson model

## Time stratified Poisson fixed effect model per zip code area

This might not work for subset data (e.g., cardio deaths only) since the number of deaths per zip day is too small (mostly zeroes)

Choose candidate zips worth modeling
```{r}
cands <- dat[
  , .(
      has_none = any(effect == "None"),
      has_tic  = any(effect != "None"),
      pos_y    = any(deaths > 0L)
    ),
  by = zip
][has_none & has_tic & pos_y, zip]
length(cands) # returns 1000

# Optionally require a minimum number of storm-effect days
min_effect_days <- 5L
cands2 <- dat[
  , .(n_tic = sum(effect != "None")),
  by = zip
][n_tic >= min_effect_days, zip]
length(cands2) # returns 1036
```

Per-zip poisson fixed effect (stratify by ym + dow)
```{r}
library(pbapply)

# Function for a single zip code
fit_one <- function(df) {
  df[, effect := factor(effect, levels = c("None","Threat","Impact","Cleanup"))]
  # Skip if baseline mean is zero (RR not defined)
  if (!any(df$effect == "None") || mean(df$deaths[df$effect == "None"]) == 0) return(NULL)
  tryCatch(
    fepois(
      deaths ~ i(effect, ref = "None") | ym + dow,
      data    = df,
      cluster = ~ ym,
      notes = FALSE
    ),
    error = function(e) NULL
  )
}

# sequential (shows a progress bar)
models <- pblapply(cands, function(z) fit_one(dat[zip == z]))
names(models) <- cands
```

Extract relative rates (and 95% CIs) per zip for Threat, Impact, Cleanup
```{r}
rr_by_zip <- bind_rows(lapply(names(models), function(z) {
  m <- models[[z]]
  if (is.null(m)) return(NULL)
  tidy(m, conf.int = TRUE) %>%
    filter(grepl("^effect::", term)) %>%
    transmute(
      zip    = z,
      effect = sub("^effect::", "", term),
      RR     = exp(estimate),
      LCI    = exp(conf.low),
      UCI    = exp(conf.high),
      p      = p.value
    )
}))

# Optional: keep only Threat/Impact/Cleanup rows
rr_by_zip <- rr_by_zip %>% filter(effect %in% c("Threat","Impact","Cleanup"))
```

The extreme RRs are a data-sparsity/separation artifact of per-ZIP, heavily stratified Poisson fits

Where are the extremes coming from?
```{r}
# per-ZIP support by effect
zip_support <- dat %>%      # dat = per-ZIP daily frame 
  group_by(zip, effect) %>%
  summarise(
    days   = n(),
    deaths = sum(deaths),
    mean_y = mean(deaths),
    .groups = "drop"
  )

# join support onto the RR table
rr_chk <- rr_by_zip %>%
  left_join(zip_support %>% rename(days_e = days, deaths_e = deaths, mean_e = mean_y),
            by = c("zip","effect")) %>%
  left_join(zip_support %>% filter(effect=="None") %>%
              select(zip, days_none = days, deaths_none = deaths, mean_none = mean_y),
            by = "zip")

# look at the pathological ones
rr_chk %>%
  filter(RR < 0.2 | RR > 5 | !is.finite(RR)) %>%
  arrange(RR) %>%
  select(zip, effect, RR, LCI, UCI, p, days_e, deaths_e, days_none, deaths_none)
```

Empirical Bayes shrinkage before mapping. Pooled fixed-effects Poisson to get a prior mean per effect (log scale). This was done above and saved as `m_pooled` pooled <- fixest::fepois(deaths ~ i(effect, "None") | zip + ym + dow, data = dat, cluster = ~ zip + ym)
```{r}
pooled_eff <- broom::tidy(m_pooled, conf.int = FALSE) %>%
  filter(grepl("^effect::", term)) %>%
  transmute(effect = sub("^effect::","",term), mu0 = estimate)   # log-RR prior

# Approximate per-ZIP log-RR variance from the model (delta from CI)
log_rr <- rr_by_zip %>%
  mutate(
    logRR   = log(RR),
    se_log  = (log(UCI) - log(LCI)) / (2*1.96)
  ) %>% filter(is.finite(logRR), se_log > 0)

# Shrink: posterior mean = (logRR / se^2 + mu0 / tau^2) / (1/se^2 + 1/tau^2). Use a single tau per effect (between-ZIP SD). Estimate tau via robust SD.
tau_by_eff <- log_rr %>% group_by(effect) %>%
  summarise(tau = stats::mad(logRR, constant = 1), .groups = "drop")

rr_shrunk <- log_rr %>%
  left_join(pooled_eff, by="effect") %>%
  left_join(tau_by_eff, by="effect") %>%
  mutate(
    prec_obs = 1/(se_log^2),
    prec_pri = 1/(pmax(tau, 1e-6)^2),
    post     = (logRR*prec_obs + mu0*prec_pri) / (prec_obs + prec_pri),
    RR_shr   = exp(post)
  ) %>%
  select(zip, effect, RR_shr)

# for story map choose the effect
rr_wide <- rr_shrunk %>%
  filter(effect == "Cleanup") %>%
  mutate(GEOID20 = zip,
         Effect = RR_shr) %>%
  select(GEOID20, Effect)

rr_map_sf <- zcta_poly %>%
  left_join(rr_wide, by = "GEOID20") %>%
  select(GEOID20, Effect, geometry)
```

Map the Threat
```{r}
# 1) Define bins with a divergence at 1
rr_map_sf <- rr_map_sf %>%
  mutate(
    Effect_bin = cut(
      Effect,
      breaks = c(0, 1, 1.2, 1.5, 2.5),
      labels = c("0–1", "1–1.2", "1.2–1.5", "1.5–2.5"),
      include.lowest = TRUE,
      right = FALSE  # interval [a, b)
    )
  )

custom_cols <- c(
  "0–1"       = "#4575b4",  # blue
  "1–1.2"     = "#fbb4b9",  # light pink
  "1.2–1.5"   = "#f768a1",  # medium pink
  "1.5–2.5"   = "#dd1c77"   # red
)

# 2.1) Plot with custom colors
ggplot(rr_map_sf) +
  geom_sf(aes(fill = Effect_bin), color = NA) +
  scale_fill_manual(
    values = custom_cols,
    drop   = FALSE,
    na.value = "grey90",
    name = "Threat RR"
  ) +
  theme_minimal() +
  theme(
    legend.position = "right",
    legend.title = element_text(size = 10),
    legend.text  = element_text(size = 9)
  )

# 2.2) Plot with a discrete diverging palette (RdBu)
ggplot(rr_map_sf) +
  geom_sf(aes(fill = Effect_bin), color = NA) +
  scale_fill_brewer(
    palette   = "RdBu",   # diverging
    direction = -1,       # blues < 1, reds > 1
    drop      = FALSE,
    na.value  = "grey90",
    name      = "Impact RR"
  ) +
  theme_minimal() +
  theme(
    legend.position = "right",
    legend.title    = element_text(size = 10),
    legend.text     = element_text(size = 9)
  )
```

Save per zip model results for story-map (Threat only)
```{r}
# zcta_results.rds
rr_shrunk_threat_only <- rr_shrunk %>%
  filter(effect == "Threat")
saveRDS(rr_shrunk_threat_only, "data/outputs/Results/zcta_results.rds")
```

## Hot spot analysis
```{r}
library(spdep)

# Prep: keep only rows with RR, keep polygons ---------------------------
g <- rr_map_sf %>%
  filter(!is.na(Effect)) %>%
  st_make_valid()

# Neighbors & weights (Queen) -------------------------------------------
# Set queen = TRUE (default). Use rook = FALSE (change to TRUE for rook)
nb <- spdep::poly2nb(g, queen = TRUE)
lw <- spdep::nb2listw(nb, style = "W", zero.policy = TRUE)  # allow islands

RR_vec <- as.numeric(log(g$Effect))  # logs and make simple vector

# Local Moran's I with permutations -------------------------------------
# 999 permutations; two-sided test; handle islands
lm <- spdep::localmoran_perm(
  x = RR_vec,
  listw = lw,
  nsim = 999,
  alternative = "two.sided",
  zero.policy = TRUE
)
lm <- as.data.frame(lm)
names(lm) <- c("Ii","E.Ii","Var.Ii","Z.Ii","p.value")

# Multiple-testing correction (fdr: false discovery rate)
lm$p.adj <- p.adjust(lm$p.value, method = "fdr")

# Spatial lag of RR (for quadrant classification)
lagRR <- spdep::lag.listw(lw, g$Effect, zero.policy = TRUE)

# Cluster classification -------------------------------------------------
alpha <- 0.35
mx <- mean(g$Effect, na.rm = TRUE)

cluster <- dplyr::case_when(
  g$Effect >= mx & lagRR >= mx & lm$Ii > 0 & lm$p.value < alpha ~ "High-High",
  g$Effect <= mx & lagRR <= mx & lm$Ii > 0 & lm$p.value < alpha ~ "Low-Low",
  g$Effect >= mx & lagRR <= mx & lm$Ii < 0 & lm$p.value < alpha ~ "High-Low",
  g$Effect <= mx & lagRR >= mx & lm$Ii < 0 & lm$p.value < alpha ~ "Low-High",
  TRUE ~ "Not Significant"
)

g_locI <- g %>%
  mutate(
    Ii       = lm$Ii,
    E_Ii     = lm$E.Ii,
    Var_Ii   = lm$Var.Ii,
    Z_I      = lm$Z.Ii,
    p_value  = lm$p.value,
    p_adj    = lm$p.adj,
    lag_RR   = lagRR,
    cluster  = factor(cluster,
                      levels = c("High-High","Low-Low","High-Low","Low-High","Not Significant"))
  )

# Join back to the full set (keeps rows where RR was NA) ----------------
Moran_out <- rr_map_sf %>%
  left_join(
    g_locI %>%
      st_drop_geometry() %>%
      select(GEOID20, Ii, E_Ii, Var_Ii, Z_I, p_value, p_adj, lag_RR, cluster),
    by = "GEOID20"
  ) %>%
  mutate(
    cluster = ifelse(is.na(cluster), "No data", as.character(cluster)),
    cluster = factor(cluster,
      levels = c("High-High","Low-Low","High-Low","Low-High","Not Significant","No data")
    )
  )
```

Maps
```{r}
# Map local Moran's I
library(scales)

# Pick a symmetric range around 0 so colors are comparable
imax <- max(abs(g_locI$Ii), na.rm = TRUE)

ggplot(g_locI) +
  geom_sf(aes(fill = Ii), color = NA) +
  scale_fill_gradient2(
    low    = "#2b6cb0",   # blue  (negative Ii)
    mid    = "#f7f7f7",   # light
    high   = "#e53e3e",   # red   (positive Ii)
    midpoint = 0,
    limits = c(-imax, imax),
    oob = squish,
    name = "Local Moran's I"
  ) +
  coord_sf(datum = NA) +
  theme_void(base_size = 12) +
  theme(legend.position = "right")

# Map the clusters
pal <- c("High-High"="#e53e3e","Low-Low"="#2b6cb0",
         "High-Low" ="#f59e0b","Low-High"="#10b981",
         "Not Significant"="#d9d9d9","No data"="#cccccc")

clusters_impact <- ggplot(Moran_out) +
  geom_sf(aes(fill = cluster), color = NA) +
  scale_fill_manual(values = pal, drop = FALSE) +
  theme_void() + labs(fill = "Local Moran clusters")

ggsave("figs/clusters_impact.png",  clusters_impact,  width = 7.2, height = 6, dpi = 300)
```

Save per zip local Moran's results for story-map (Threat only)
```{r}
# zcta_clusters.rds
rr_clusters <- g_locI %>%
  select(GEOID20, Ii, geometry)

saveRDS(rr_clusters, "data/outputs/Results/zcta_clusters.rds")
```


### Figure 1: Study design and workflow

```{r}
library(ggplot2)
library(dplyr)
library(tibble)

# ------------------------------------------------------------
# Define boxes (positions tuned for a clean layout)
# ------------------------------------------------------------
boxes <- tibble(
  id = c(
    "deaths_geo",
    "storms",
    "zcta",
    "panel_all",
    "panel_restr",
    "state_model",
    "state_outputs",
    "zip_models",
    "zip_rrs",
    "zip_lisa"
  ),
  xmin = c(
    0.5,  # deaths_geo
    3.0,  # storms
    5.5,  # zcta
    2.0,  # panel_all
    2.0,  # panel_restr
    0.5,  # state_model
    0.5,  # state_outputs
    4.5,  # zip_models
    4.5,  # zip_rrs
    4.5   # zip_lisa
  ),
  xmax = c(
    2.5,  # deaths_geo
    5.0,  # storms
    7.5,  # zcta
    6.0,  # panel_all
    6.0,  # panel_restr
    3.5,  # state_model
    3.5,  # state_outputs
    7.5,  # zip_models
    7.5,  # zip_rrs
    7.5   # zip_lisa
  ),
  ymin = c(
    8.0,  # deaths_geo
    8.0,  # storms
    8.0,  # zcta
    6.5,  # panel_all
    5.0,  # panel_restr
    3.5,  # state_model
    2.0,  # state_outputs
    3.5,  # zip_models
    2.0,  # zip_rrs
    0.5   # zip_lisa
  ),
  ymax = c(
    9.0,  # deaths_geo
    9.0,  # storms
    9.0,  # zcta
    7.5,  # panel_all
    6.0,  # panel_restr
    4.5,  # state_model
    3.0,  # state_outputs
    4.5,  # zip_models
    3.0,  # zip_rrs
    1.5   # zip_lisa
  )
)

boxes <- boxes %>%
  mutate(
    x = (xmin + xmax) / 2,
    y = (ymin + ymax) / 2
  )

# Labels
label_map <- c(
  deaths_geo = "Vital records\nGeocoded deaths\n(point-level, daily)",
  storms     = "Storm data\nIBTrACS + windfields\n(Threat/Impact/Cleanup)",
  zcta       = "ZCTA/ZIP boundaries\nPopulation & covariates",
  panel_all  = "ZIP–day mortality panel\nTagged by storm effect\n(1985–2022)",
  panel_restr = "Analysis sample\n1985–2022, May–Nov\nFlorida ZIP–day deaths",
  state_model = "Statewide time-stratified\nfixed-effects Poisson\n(daily deaths ~ storm effect\n+ calendar/time strata)",
  state_outputs = "Statewide rate ratios (RRs)\nThreat / Impact / Cleanup vs None\nEvent-time profiles",
  zip_models = "ZIP-level Poisson models\n(deaths ~ storm effect\n+ time fixed effects)",
  zip_rrs = "Shrunken ZIP-level RRs\nby storm phase\n(Threat / Impact / Cleanup)",
  zip_lisa = "Spatial clustering of RRs\nLocal Moran's I\nHot and cold spot maps"
)

boxes$plot_label <- label_map[boxes$id]

# ------------------------------------------------------------
# Define arrows between boxes
# ------------------------------------------------------------
arrows <- tibble(
  from = c(
    "deaths_geo",
    "storms",
    "zcta",
    "panel_all",
    "panel_all",
    "panel_restr",
    "panel_restr",
    "state_model",
    "zip_models",
    "zip_rrs"
  ),
  to = c(
    "panel_all",
    "panel_all",
    "panel_all",
    "panel_restr",
    # If you don't want two arrows from panel_all, comment one of these
    "panel_restr",
    "state_model",
    "zip_models",
    "state_outputs",
    "zip_rrs",
    "zip_lisa"
  )
) %>%
  left_join(boxes %>% select(id, x, y), by = c("from" = "id")) %>%
  rename(xstart = x, ystart = y) %>%
  left_join(boxes %>% select(id, x, y), by = c("to" = "id")) %>%
  rename(xend = x, yend = y) %>%
  mutate(
    # optional small nudges to avoid arrow overlap from top row
    ystart = case_when(
      from == "deaths_geo" ~ ystart - 0.1,
      from == "storms"     ~ ystart,
      from == "zcta"       ~ ystart + 0.1,
      TRUE ~ ystart
    )
  )

# ------------------------------------------------------------
# Plot
# ------------------------------------------------------------
p_fig1 <- ggplot() +
  # Arrows
  geom_curve(
    data = arrows,
    aes(x = xstart, y = ystart, xend = xend, yend = yend),
    curvature = 0.1,
    arrow = arrow(length = unit(0.15, "inches")),
    linewidth = 0.4
  ) +
  # Boxes
  geom_rect(
    data = boxes,
    aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax),
    fill = "white",
    color = "black",
    linewidth = 0.4
  ) +
  # Box labels
  geom_text(
    data = boxes,
    aes(x = x, y = y, label = plot_label),
    size = 3.3,
    lineheight = 0.9
  ) +
  coord_fixed(xlim = c(0, 8), ylim = c(0, 9), expand = FALSE) +
  theme_void() +
  theme(
    plot.margin = margin(10, 10, 10, 10),
    panel.background = element_rect(fill = "white", color = NA)
  )

p_fig1
# p_fig1 + ggtitle("Figure 1. Study design and analysis workflow")
```


### Figure 2: Coefficient plot
```{r}
library(broom)
library(dplyr)
library(stringr)
library(ggplot2)

# 1. Extract log-coefficients + CIs from m_pooled
tt <- tidy(m_pooled, conf.int = TRUE)   # no exponentiation here

# 2. Keep only the storm-effect terms and exponentiate manually
rr_df <- tt %>%
  filter(str_detect(term, "^effect::")) %>%
  mutate(
    storm_phase = str_remove(term, "^effect::"),
    storm_phase = factor(storm_phase,
                         levels = c("Threat", "Impact", "Cleanup")),
    RR      = exp(estimate),
    RR_low  = exp(conf.low),
    RR_high = exp(conf.high)
  )

# Quick sanity check
rr_df[, c("term", "storm_phase", "estimate", "RR", "RR_low", "RR_high")]

# 3. Forest plot: RRs on x-axis, phases on y-axis
p_rr <- ggplot(rr_df, aes(y = storm_phase, x = RR)) +
  geom_vline(xintercept = 1, linetype = "dashed", color = "grey50") +
  geom_point(size = 3) +
  geom_errorbarh(aes(xmin = RR_low, xmax = RR_high),
                 height = 0.2, linewidth = 0.6) +
  labs(
    x = "Rate ratio (RR)",
    y = "Storm effect",
    title = "Statewide rate ratios by storm effect",
    subtitle = "Threat, Impact, and Cleanup relative to None\nFixed-effects Poisson (ZIP, year–month, DOW FE)"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    panel.grid.minor = element_blank(),
    plot.title = element_text(face = "bold", hjust = 0.5),
    plot.subtitle = element_text(hjust = 0.5)
  )

p_rr
ggsave("figs/state_rr_by_effect.png", p_rr,  width = 7.2, height = 6, dpi = 300)
```

### Table 1: Coefficients
```{r}
library(broom)
library(dplyr)
library(stringr)

tt <- tidy(m_pooled, conf.int = TRUE)

rr_df <- tt %>%
  filter(str_detect(term, "^effect::")) %>%
  mutate(
    phase = str_remove(term, "^effect::"),
    RR      = exp(estimate),
    RR_low  = exp(conf.low),
    RR_high = exp(conf.high),
    sig     = p.value < 0.05
  ) %>%
  transmute(
    `Storm phase` = phase,
    RR_CI = ifelse(
      sig,
      sprintf("\\textbf{%.2f (%.2f, %.2f)}", RR, RR_low, RR_high),
      sprintf("%.2f (%.2f, %.2f)", RR, RR_low, RR_high)
    ),
    `p-value` = sprintf("%.3f", p.value)
  )
```

```{r}
library(knitr)
library(kableExtra)

kable(
  rr_df,
  format = "latex",
  booktabs = TRUE,
  escape = FALSE,       # allow boldface in RR_CI column
  align = c("l","c","c"),
  col.names = c("Storm phase", "Rate ratio (95\\% CI)", "p-value")
) %>%
  kable_styling(
    latex_options = c("hold_position"),
    font_size = 9
  ) %>%
  add_header_above(
    c(" " = 1, "Effect estimate" = 2),
    bold = TRUE
  ) %>%
  footnote(
    general = "Rate ratios from fixed-effects Poisson model of daily deaths. Reference category is None days. Fixed effects for ZIP, year–month, and day-of-week. Standard errors clustered by ZIP and year–month.",
    general_title = "",
    threeparttable = TRUE
  )
```


### Figure 3: Zip-level maps

Prepare a clean ZIP-phase–RR dataset
```{r}
library(dplyr)
library(sf)
library(ggplot2)
library(RColorBrewer)
library(tidyr)

# --- merge geometry ---
zip_sf <- zctas_valid %>%
  rename(zip = GEOID20)
rr_map <- zip_sf %>%
  left_join(rr_by_zip, by = "zip") %>%
  mutate(
    # Log-scale quantities
    logRR  = log(RR),
    logLCI = log(LCI),
    logUCI = log(UCI),
    logCIwidth = abs(logUCI - logLCI),
    
    # Mask: wide CI or missing
    mask = ifelse(is.na(RR) | is.na(logRR) | logCIwidth > 0.7, TRUE, FALSE),

    # Storm phase as factor with standard order
    effect = factor(effect, levels = c("Threat", "Impact", "Cleanup"))
  )
```

Create a diverging palette symmetric around logRR = 0